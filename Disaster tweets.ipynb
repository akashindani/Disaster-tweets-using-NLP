{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import re, string \n",
    "import pandas as pd   \n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=pd.read_csv(\"nlp-getting-started/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>454</td>\n",
       "      <td>armageddon</td>\n",
       "      <td>Wrigley Field</td>\n",
       "      <td>@KatieKatCubs you already know how this shit g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     keyword       location  \\\n",
       "311  454  armageddon  Wrigley Field   \n",
       "\n",
       "                                                  text  target  \n",
       "311  @KatieKatCubs you already know how this shit g...       0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df[main_df['id']==454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text=main_df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7608</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7609</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7610</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7611</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7612</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     Our Deeds are the Reason of this #earthquake M...\n",
       "1                Forest fire near La Ronge Sask. Canada\n",
       "2     All residents asked to 'shelter in place' are ...\n",
       "3     13,000 people receive #wildfires evacuation or...\n",
       "4     Just got sent this photo from Ruby #Alaska as ...\n",
       "...                                                 ...\n",
       "7608  Two giant cranes holding a bridge collapse int...\n",
       "7609  @aria_ahrary @TheTawniest The out of control w...\n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
       "7611  Police investigating after an e-bike collided ...\n",
       "7612  The Latest: More Homes Razed by Northern Calif...\n",
       "\n",
       "[7613 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "\n",
    "#This will give us stop words\n",
    "#en_default = en_core_web_md\n",
    "nlp = spacy.load('en_default') \n",
    "\n",
    "\n",
    "# Choose model accordingly for contractions function\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "# model = api.load(\"glove-twitter-100\")\n",
    "# model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exclude words from spacy stopwords list\n",
    "deselect_stop_words = ['no', 'not']\n",
    "for w in deselect_stop_words:\n",
    "    nlp.vocab[w].is_stop = False\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"remove html tags from text\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "def remove_punctuation_function(text):\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|,;'+string.punctuation+']')\n",
    "    text = REPLACE_BY_SPACE_RE.sub('', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    \"\"\"remove extra whitespaces from text\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text\n",
    "def remove_user_name(text):\n",
    "    \"\"\"Remove the twitter usernames, e.g. @usatoday,@bbcmtd \"\"\"\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove the urls, e.g. http://t.co/lHYXEOHY6C\"\"\"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def text_preprocessing(text, accented_chars=True, contractions=True, \n",
    "                       convert_num=True, extra_whitespace=True, \n",
    "                       lemmatization=True, lowercase=True, punctuations=True,\n",
    "                       remove_html=True, remove_num=True, special_chars=True, \n",
    "                       stop_words=True,remove_punctuation=True,remove_user=True,remove_url=True):\n",
    "    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n",
    "    if remove_html == True: #remove html tags\n",
    "        text = strip_html_tags(text)\n",
    "    if remove_user == True:#remove twitter user names\n",
    "        text=remove_user_name(text)\n",
    "    if remove_url == True:#remove the urls from the tweets\n",
    "        text=remove_urls(text)\n",
    "    if accented_chars == True: #remove accented characters\n",
    "        text = remove_accented_chars(text)\n",
    "    if contractions == True: #expand contractions\n",
    "        text = expand_contractions(text)\n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        text = text.lower()\n",
    "    if remove_punctuation == True: #remove punctuation\n",
    "        text=remove_punctuation_function(text)\n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        text = remove_whitespace(text)\n",
    "\n",
    "    doc = nlp(text) #tokenise text\n",
    "\n",
    "    clean_text = []\n",
    "    \n",
    "    for token in doc:\n",
    "        flag = True\n",
    "        edit = token.text\n",
    "        # remove stop words\n",
    "        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n",
    "            flag = False\n",
    "        # remove punctuations\n",
    "        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n",
    "            flag = False\n",
    "        # remove special characters\n",
    "        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n",
    "            flag = False\n",
    "        # remove numbers\n",
    "        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
    "        and flag == True:\n",
    "            flag = False\n",
    "        # convert number words to numeric numbers\n",
    "        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n",
    "            edit = w2n.word_to_num(token.text)\n",
    "        # convert tokens to base form\n",
    "        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n",
    "            edit = token.lemma_\n",
    "        # append tokens edited and not removed to list \n",
    "        if edit != \"\" and flag == True:\n",
    "            clean_text.append(edit)        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fedex', 'no', 'longer', 'transport', 'bioterror', 'germ', 'wake', 'anthrax', 'lab', 'mishap']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/c0p3SEsqWm via @usatoday\"\n",
    "clean_text=text_preprocessing(sentence) \n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:29: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:29: built Dictionary(12 unique tokens: ['across', 'an', 'building', 'emergency', 'evacuation']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:29: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:29: built Dictionary(12 unique tokens: ['across', 'an', 'building', 'emergency', 'evacuation']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:29: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:29: built Dictionary(3 unique tokens: ['up', 'has', 'what']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:46:29: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:29: built Dictionary(3 unique tokens: ['up', 'is', 'what']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:46:29: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:29: built Dictionary(20 unique tokens: ['at', 'every', 'for', 'get', 'have']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:46:29: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:29: built Dictionary(20 unique tokens: ['at', 'every', 'for', 'get', 'have']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:46:29: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:29: built Dictionary(18 unique tokens: ['&', 'a', 'accident', 'bicycle', 'cannot']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:29: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:29: built Dictionary(18 unique tokens: ['&', 'a', 'accident', 'bicycle', 'cannot']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:30: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:30: built Dictionary(16 unique tokens: ['a', 'accident', 'at', 'car', 'deadly']...) from 2 documents (total 31 corpus positions)\n",
      "INFO - 18:46:30: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:30: built Dictionary(16 unique tokens: ['a', 'accident', 'at', 'car', 'deadly']...) from 2 documents (total 31 corpus positions)\n",
      "INFO - 18:46:30: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:30: built Dictionary(16 unique tokens: ['a', 'accident', 'airplane', 'an', 'at']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:30: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:30: built Dictionary(16 unique tokens: ['a', 'accident', 'airplane', 'an', 'at']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:30: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:30: built Dictionary(18 unique tokens: ['+', 'a', 'accident', 'airplane', 'bit']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:30: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:30: built Dictionary(18 unique tokens: ['+', 'a', 'accident', 'airplane', 'bit']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:30: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:30: built Dictionary(13 unique tokens: ['a', 'an', 'at', 'been', 'halt']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:30: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:30: built Dictionary(13 unique tokens: ['a', 'an', 'at', 'been', 'halt']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:31: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(15 unique tokens: ['ambulance', 'an', 'in', 'it', 'last']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:31: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(15 unique tokens: ['ambulance', 'an', 'in', 'it', 'last']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:31: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(6 unique tokens: ['ambulance', 'an', 'dog', 'thinks', 'has']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:46:31: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(6 unique tokens: ['ambulance', 'an', 'dog', 'thinks', 'he']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:46:31: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(9 unique tokens: ['ambulance', 'body', 'in', 'number', 'or']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:31: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(9 unique tokens: ['ambulance', 'body', 'in', 'number', 'or']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:31: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(15 unique tokens: ['annihilated', 'for', 'getting', 'guess', 'have']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:31: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(15 unique tokens: ['annihilated', 'for', 'getting', 'guess', 'have']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:31: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(10 unique tokens: ['getting', 'hashtag', 'in', 'is.', 'on']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:31: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(10 unique tokens: ['getting', 'hashtag', 'in', 'is.', 'on']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:31: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(16 unique tokens: ['a', 'annihilated', 'because', 'from', 'great']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:31: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(16 unique tokens: ['a', 'annihilated', 'because', 'from', 'great']...) from 2 documents (total 30 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:31: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(18 unique tokens: ['a', 'annihilated', 'because', 'from', 'great']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:31: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:31: built Dictionary(17 unique tokens: ['a', 'annihilated', 'because', 'from', 'great']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:32: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(13 unique tokens: ['after', 'and', 'books', 'current', 'died']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:32: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(13 unique tokens: ['after', 'and', 'books', 'current', 'died']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:32: Removed 15 and 14 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(3 unique tokens: ['at', 'has', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:46:32: Removed 15 and 14 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(3 unique tokens: ['at', 'is', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:46:32: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(13 unique tokens: ['apocalypse', 'has', 'hot', 'kind', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:32: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(14 unique tokens: ['apocalypse', 'has', 'hot', 'kind', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:32: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(15 unique tokens: ['apocalypse', 'has', 'hot', 'is', 'kind']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:32: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(15 unique tokens: ['apocalypse', 'has', 'hot', 'is', 'kind']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:32: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(7 unique tokens: ['apocalypse', 'gf', 'if', 'lol', 'the']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:32: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(7 unique tokens: ['apocalypse', 'gf', 'if', 'lol', 'the']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:32: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(13 unique tokens: ['a', 'but', 'called', 'interpretation', 'is']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:32: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(12 unique tokens: ['a', 'but', 'called', 'interpretation', 'is']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:32: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(10 unique tokens: ['audience', 'because', 'lone', 'my', 'of']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:32: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(10 unique tokens: ['audience', 'because', 'lone', 'my', 'of']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:32: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(12 unique tokens: ['audience', 'because', 'is', 'it', 'lone']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:32: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(11 unique tokens: ['audience', 'because', 'is', 'it', 'lone']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:32: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(21 unique tokens: ['a', 'about', 'actually', 'and', 'apocalypse']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:32: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(21 unique tokens: ['a', 'about', 'actually', 'and', 'apocalypse']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:32: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(10 unique tokens: ['apocalypse', 'be', 'comes', 'is', 'know']...) from 2 documents (total 19 corpus positions)\n",
      "INFO - 18:46:32: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:32: built Dictionary(10 unique tokens: ['apocalypse', 'be', 'comes', 'is', 'know']...) from 2 documents (total 19 corpus positions)\n",
      "INFO - 18:46:33: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:33: built Dictionary(10 unique tokens: ['a', 'in', 'latest', 'over', 'photo']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:33: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:33: built Dictionary(10 unique tokens: ['a', 'in', 'latest', 'over', 'photo']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:33: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:33: built Dictionary(15 unique tokens: ['a', 'and', 'but', 'cannot', 'girls']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:33: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:33: built Dictionary(15 unique tokens: ['a', 'and', 'but', 'cannot', 'girls']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:33: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:33: built Dictionary(14 unique tokens: ['a', 'armageddon', 'bears', 'certainty', 'coat']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:33: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:33: built Dictionary(15 unique tokens: ['a', 'armageddon', 'bears', 'certainty', 'coat']...) from 2 documents (total 36 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:33: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:33: built Dictionary(15 unique tokens: ['a', 'armageddon', 'bears', 'certainty', 'coat']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:33: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:33: built Dictionary(15 unique tokens: ['a', 'armageddon', 'bears', 'certainty', 'coat']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:34: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:34: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:46:34: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:34: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:46:34: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:34: built Dictionary(13 unique tokens: ['about', 'can', 'guess', 'just', 'say']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:34: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:34: built Dictionary(13 unique tokens: ['about', 'can', 'guess', 'just', 'say']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:34: built Dictionary(5 unique tokens: ['cracking', 'cuz', 'whats', 'has', 'what']) from 2 documents (total 7 corpus positions)\n",
      "INFO - 18:46:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:34: built Dictionary(5 unique tokens: ['cracking', 'cuz', 'whats', 'is', 'what']) from 2 documents (total 7 corpus positions)\n",
      "INFO - 18:46:34: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:34: built Dictionary(20 unique tokens: ['attack', 'for', 'governments', 'it', 'just']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:34: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:34: built Dictionary(20 unique tokens: ['attack', 'for', 'governments', 'it', 'just']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:34: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:34: built Dictionary(21 unique tokens: ['a', 'attacked', 'be', 'been', 'believe']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:34: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:34: built Dictionary(20 unique tokens: ['a', 'attacked', 'be', 'been', 'believe']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:35: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:35: built Dictionary(13 unique tokens: ['and', 'answer', 'attacked', 'being', 'calmly']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:35: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:35: built Dictionary(13 unique tokens: ['and', 'answer', 'attacked', 'being', 'calmly']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:35: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:35: built Dictionary(13 unique tokens: ['appreciate', 'checked', 'for', 'greatly', 'if']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:46:35: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:35: built Dictionary(13 unique tokens: ['appreciate', 'checked', 'for', 'greatly', 'if']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:46:35: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:35: built Dictionary(14 unique tokens: ['and', 'cast', 'for', 'head', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:35: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:35: built Dictionary(14 unique tokens: ['and', 'cast', 'for', 'head', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:36: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:36: built Dictionary(12 unique tokens: ['bc', 'been', 'college', 'difficult', 'not']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:36: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:36: built Dictionary(12 unique tokens: ['bc', 'been', 'college', 'difficult', 'not']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:36: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:36: built Dictionary(9 unique tokens: ['another', 'due', 'in', 'my', 'she']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:36: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:36: built Dictionary(9 unique tokens: ['another', 'due', 'in', 'my', 'she']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:36: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:36: built Dictionary(16 unique tokens: ['a', 'at', 'babes', 'ball', 'be']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:36: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:36: built Dictionary(16 unique tokens: ['a', 'at', 'babes', 'ball', 'be']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:36: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:36: built Dictionary(13 unique tokens: ['beats', 'best', 'blazing', 'game', 'silenced']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:37: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(13 unique tokens: ['beats', 'best', 'blazing', 'game', 'silenced']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:37: Removed 12 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(3 unique tokens: ['because', 'has', 'she']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:46:37: Removed 12 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(3 unique tokens: ['because', 'is', 'she']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:46:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(13 unique tokens: ['am', 'and', 'blazing', 'can', 'do']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(13 unique tokens: ['am', 'and', 'blazing', 'can', 'do']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:37: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(11 unique tokens: ['a', 'and', 'at', 'chicken', 'lip']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:37: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(11 unique tokens: ['a', 'and', 'at', 'chicken', 'lip']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:37: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(11 unique tokens: ['and', 'apologize', 'bleeding', 'could', 'for']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:46:37: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(11 unique tokens: ['and', 'apologize', 'bleeding', 'could', 'for']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:46:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(13 unique tokens: ['back', 'be', 'bleeding', 'but', 'can']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(13 unique tokens: ['back', 'be', 'bleeding', 'but', 'can']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(15 unique tokens: ['are', 'at', 'bleeding', 'fine', 'if']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(15 unique tokens: ['are', 'at', 'bleeding', 'fine', 'if']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:37: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(20 unique tokens: ['a', 'about', 'also', 'and', 'broken']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:37: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(20 unique tokens: ['a', 'about', 'also', 'and', 'broken']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(11 unique tokens: ['a', 'bleeding', 'cams', 'cute', 'date']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(11 unique tokens: ['a', 'bleeding', 'cams', 'cute', 'date']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:37: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(25 unique tokens: ['a', 'all', 'already', 'and', 'bad']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:46:37: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:37: built Dictionary(24 unique tokens: ['a', 'all', 'already', 'and', 'bad']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:46:38: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(15 unique tokens: ['apartment', 'around', 'at', 'bar', 'bleeding']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:46:38: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(15 unique tokens: ['apartment', 'around', 'at', 'bar', 'bleeding']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:46:38: Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(12 unique tokens: ['#oomf', 'active', 'an', 'and', 'because']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:38: Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(13 unique tokens: ['#oomf', 'active', 'an', 'and', 'because']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:38: Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(13 unique tokens: ['#oomf', 'active', 'an', 'and', 'because']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:38: Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(12 unique tokens: ['#oomf', 'active', 'an', 'and', 'because']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:38: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(18 unique tokens: ['airhead', 'be', 'blew', 'by', 'entire']...) from 2 documents (total 35 corpus positions)\n",
      "INFO - 18:46:38: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(18 unique tokens: ['airhead', 'be', 'blew', 'by', 'entire']...) from 2 documents (total 35 corpus positions)\n",
      "INFO - 18:46:38: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(14 unique tokens: ['blew', 'check', 'encouraging', 'her', 'just']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:38: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(14 unique tokens: ['blew', 'check', 'encouraging', 'her', 'just']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:38: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(15 unique tokens: ['a', 'before', 'blew', 'clicks', 'could']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:38: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(15 unique tokens: ['a', 'before', 'blew', 'clicks', 'could']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:38: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(18 unique tokens: ['a', 'actually', 'arent', 'blight', 'dwarves']...) from 2 documents (total 41 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:38: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(18 unique tokens: ['a', 'actually', 'arent', 'blight', 'dwarves']...) from 2 documents (total 41 corpus positions)\n",
      "INFO - 18:46:38: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(18 unique tokens: ['a', 'actually', 'are', 'blight', 'dwarves']...) from 2 documents (total 43 corpus positions)\n",
      "INFO - 18:46:38: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(18 unique tokens: ['a', 'actually', 'are', 'blight', 'dwarves']...) from 2 documents (total 43 corpus positions)\n",
      "INFO - 18:46:38: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:38: built Dictionary(20 unique tokens: ['>_>', 'again', 'agree', 'be', 'because']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:39: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(20 unique tokens: ['>_>', 'again', 'agree', 'be', 'because']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:39: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(20 unique tokens: ['>_>', 'again', 'agree', 'be', 'because']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:39: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(20 unique tokens: ['>_>', 'again', 'agree', 'be', 'because']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:39: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(20 unique tokens: ['>_>', 'again', 'agree', 'be', 'because']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:39: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(22 unique tokens: ['>_>', 'again', 'agree', 'be', 'because']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:46:39: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(22 unique tokens: ['>_>', 'again', 'agree', 'be', 'because']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:46:39: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(11 unique tokens: ['blight', 'car', 'dotish', 'go', 'not']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:39: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(11 unique tokens: ['blight', 'car', 'dotish', 'go', 'not']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:39: Removed 19 and 18 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(6 unique tokens: ['going', 'to', 'we', 'will', 'has']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:46:39: Removed 19 and 18 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(6 unique tokens: ['going', 'to', 'we', 'will', 'is']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:46:39: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(14 unique tokens: ['any', 'breaks', 'get', 'hype', 'let']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:39: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(14 unique tokens: ['any', 'breaks', 'get', 'hype', 'let']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:39: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(8 unique tokens: ['and', 'controllers', 'mic', 'one', 'second']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:39: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:39: built Dictionary(8 unique tokens: ['and', 'controllers', 'mic', 'one', 'second']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:40: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(7 unique tokens: ['blood', 'hoe', 'in', 'my', 'no']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:40: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(7 unique tokens: ['blood', 'hoe', 'in', 'my', 'no']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:40: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(7 unique tokens: ['blood', 'hoe', 'in', 'my', 'no']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:40: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(7 unique tokens: ['blood', 'hoe', 'in', 'my', 'no']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:40: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(7 unique tokens: ['blood', 'hoe', 'in', 'my', 'no']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:40: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(11 unique tokens: ['-', 'a', 'book', 'chapter', 'free']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:40: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(11 unique tokens: ['-', 'a', 'book', 'chapter', 'free']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:40: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(15 unique tokens: ['and', 'blood', 'daughters', 'innocent', 'land']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:40: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(15 unique tokens: ['and', 'blood', 'daughters', 'innocent', 'land']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:40: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(15 unique tokens: ['a', 'and', 'big', 'blood', 'but']...) from 2 documents (total 35 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:40: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(15 unique tokens: ['a', 'and', 'big', 'blood', 'but']...) from 2 documents (total 35 corpus positions)\n",
      "INFO - 18:46:40: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(11 unique tokens: ['as', 'awful', 'bloody', 'from', 'have']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:40: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(11 unique tokens: ['as', 'awful', 'bloody', 'from', 'have']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:40: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(16 unique tokens: ['all', 'are', 'because', 'bloody', 'by']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:40: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(16 unique tokens: ['all', 'are', 'because', 'bloody', 'by']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(4 unique tokens: ['bloody', 'sexy', 'has', 'it']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:46:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(4 unique tokens: ['bloody', 'sexy', 'is', 'it']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:46:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(8 unique tokens: ['barking', 'bloody', 'is.', 'it', 'now']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(8 unique tokens: ['barking', 'bloody', 'is.', 'it', 'now']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(13 unique tokens: ['a', 'be', 'bloody', 'but', 'day']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:40: built Dictionary(13 unique tokens: ['a', 'be', 'bloody', 'but', 'day']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:41: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(17 unique tokens: ['a', 'all', 'and', 'any', 'be']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:41: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(18 unique tokens: ['a', 'all', 'and', 'any', 'be']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:41: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(6 unique tokens: ['blown', 'has', 'not', 'still', 'why']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:41: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(7 unique tokens: ['blown', 'has', 'not', 'still', 'why']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:41: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(13 unique tokens: ['a', 'approval', 'blown', 'did', 'get']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:41: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(12 unique tokens: ['a', 'approval', 'blown', 'did', 'get']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:41: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(24 unique tokens: ['a', 'and', 'be', 'blown', 'but']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:46:41: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(24 unique tokens: ['a', 'and', 'be', 'blown', 'but']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:46:41: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(15 unique tokens: ['am', 'at', 'bagging', 'be', 'body']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:41: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(15 unique tokens: ['am', 'at', 'bagging', 'be', 'body']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:41: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(10 unique tokens: ['?', 'bagging', 'body', 'her', 'hot']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:41: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:41: built Dictionary(9 unique tokens: ['?', 'bagging', 'body', 'her', 'hot']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(15 unique tokens: ['bagging', 'be', 'begin', 'cuffed', 'it']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(15 unique tokens: ['bagging', 'be', 'begin', 'cuffed', 'it']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(15 unique tokens: ['bagging', 'be', 'begin', 'cuffed', 'it']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(15 unique tokens: ['bagging', 'be', 'begin', 'cuffed', 'it']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(15 unique tokens: ['bagging', 'be', 'begin', 'cuffed', 'it']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(14 unique tokens: ['about', 'another', 'bagging', 'body', 'did']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(14 unique tokens: ['about', 'another', 'bagging', 'body', 'did']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(14 unique tokens: ['about', 'another', 'bagging', 'body', 'did']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(14 unique tokens: ['about', 'another', 'bagging', 'body', 'did']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(14 unique tokens: ['about', 'another', 'bagging', 'body', 'did']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(22 unique tokens: ['all', 'and', 'bagging', 'body', 'but']...) from 2 documents (total 45 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(22 unique tokens: ['all', 'and', 'bagging', 'body', 'but']...) from 2 documents (total 45 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(22 unique tokens: ['all', 'and', 'bagging', 'body', 'but']...) from 2 documents (total 45 corpus positions)\n",
      "INFO - 18:46:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(21 unique tokens: ['all', 'and', 'bagging', 'body', 'but']...) from 2 documents (total 45 corpus positions)\n",
      "INFO - 18:46:42: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(9 unique tokens: ['a', 'bags', 'body', 'in', 'no']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:42: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(9 unique tokens: ['a', 'bags', 'body', 'in', 'no']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:42: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(9 unique tokens: ['a', 'bags', 'body', 'in', 'no']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:42: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(9 unique tokens: ['a', 'bags', 'body', 'in', 'no']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:42: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(9 unique tokens: ['a', 'bags', 'body', 'in', 'no']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:42: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(11 unique tokens: ['a', 'am', 'bags', 'body', 'in']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:42: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:42: built Dictionary(11 unique tokens: ['a', 'am', 'bags', 'body', 'in']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:43: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(13 unique tokens: ['anyway', 'bomb', 'guys', 'he', 'him']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:46:43: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(13 unique tokens: ['anyway', 'bomb', 'guys', 'he', 'him']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:46:43: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(9 unique tokens: ['be', 'bomb', 'guys', 'if', 'so']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:43: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(9 unique tokens: ['be', 'bomb', 'guys', 'if', 'so']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:46:43: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(22 unique tokens: ['a', 'an', 'bath', 'bathe', 'bomb']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:46:43: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(22 unique tokens: ['a', 'an', 'bath', 'bathe', 'bomb']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:46:43: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(24 unique tokens: ['&', 'a', 'bathroom', 'bombed', 'families']...) from 2 documents (total 47 corpus positions)\n",
      "INFO - 18:46:43: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(24 unique tokens: ['&', 'a', 'bathroom', 'bombed', 'families']...) from 2 documents (total 47 corpus positions)\n",
      "INFO - 18:46:43: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(14 unique tokens: ['feels', 'get', 'grab', 'hes', 'i']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:46:43: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(14 unique tokens: ['feels', 'get', 'grab', 'hes', 'i']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:46:43: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(14 unique tokens: ['feels', 'get', 'grab', 'he', 'i']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:46:43: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:43: built Dictionary(14 unique tokens: ['feels', 'get', 'grab', 'he', 'i']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:46:44: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:44: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:44: built Dictionary(15 unique tokens: ['been', 'bombing', 'let', 'of', 'one']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:44: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:44: built Dictionary(15 unique tokens: ['been', 'bombing', 'let', 'of', 'one']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:44: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:44: built Dictionary(18 unique tokens: ['babies', 'beat', 'boxer', 'buildings', 'burning']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:44: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:44: built Dictionary(18 unique tokens: ['babies', 'beat', 'boxer', 'buildings', 'burning']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:44: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:44: built Dictionary(16 unique tokens: ['a', 'am', 'ass', 'bomb', 'buildings']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:44: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:44: built Dictionary(16 unique tokens: ['a', 'am', 'ass', 'bomb', 'buildings']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:44: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:44: built Dictionary(13 unique tokens: ['burning', 'in', 'is', 'largest', 'more']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:44: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:44: built Dictionary(12 unique tokens: ['burning', 'in', 'is', 'largest', 'more']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(12 unique tokens: ['&', 'a', 'and', 'buildings', 'burning']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(12 unique tokens: ['&', 'a', 'and', 'buildings', 'burning']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:45: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(8 unique tokens: ['and', 'buildings', 'burning', 'least', 'looting']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:45: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(8 unique tokens: ['and', 'buildings', 'burning', 'least', 'looting']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(17 unique tokens: ['am', 'around', 'booty', 'do', 'fine']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(17 unique tokens: ['am', 'around', 'booty', 'do', 'fine']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(21 unique tokens: ['and', 'being', 'couches', 'do', 'fr']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(21 unique tokens: ['and', 'being', 'couches', 'do', 'fr']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(22 unique tokens: ['&', 'burned', 'but', 'coolest', 'down']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(22 unique tokens: ['&', 'burned', 'but', 'coolest', 'down']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(22 unique tokens: ['&', 'burned', 'but', 'coolest', 'down']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(22 unique tokens: ['&', 'burned', 'but', 'coolest', 'down']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:45: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:45: built Dictionary(21 unique tokens: ['&', 'burned', 'but', 'coolest', 'down']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:46: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(15 unique tokens: ['=', 'am', 'and', 'burning', 'candle']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:46: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(15 unique tokens: ['=', 'am', 'and', 'burning', 'candle']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:46: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(17 unique tokens: ['=', 'am', 'and', 'burning', 'candle']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:46: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(16 unique tokens: ['=', 'am', 'and', 'burning', 'candle']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(15 unique tokens: ['because', 'burning', 'if', 'it', 'like']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(15 unique tokens: ['because', 'burning', 'if', 'it', 'like']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:46: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(16 unique tokens: ['because', 'burning', 'if', 'is', 'it']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:46: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(16 unique tokens: ['because', 'burning', 'if', 'is', 'it']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(6 unique tokens: ['burning', 'except', 'idk', 'really', 'has']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:46:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(6 unique tokens: ['burning', 'except', 'idk', 'really', 'is']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:46:46: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(12 unique tokens: ['and', 'are', 'kids', 'old', 'one']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:46: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(12 unique tokens: ['and', 'are', 'kids', 'old', 'one']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(7 unique tokens: ['a', 'fire', 'out', 'put', 'still']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(7 unique tokens: ['a', 'fire', 'out', 'put', 'still']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(25 unique tokens: ['a', 'accidentally', 'are', 'arm', 'be']...) from 2 documents (total 50 corpus positions)\n",
      "INFO - 18:46:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(25 unique tokens: ['a', 'accidentally', 'are', 'arm', 'be']...) from 2 documents (total 50 corpus positions)\n",
      "INFO - 18:46:46: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(10 unique tokens: ['at', 'buildings', 'burning', 'demonstrations', 'down']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:46: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(10 unique tokens: ['at', 'buildings', 'burning', 'demonstrations', 'down']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:46:46: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:46: built Dictionary(16 unique tokens: ['a', 'am', 'ass', 'bomb', 'buildings']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:47: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(16 unique tokens: ['a', 'am', 'ass', 'bomb', 'buildings']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:47: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(19 unique tokens: ['a', 'buildings', 'burning', 'deep', 'from']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:47: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(19 unique tokens: ['a', 'buildings', 'burning', 'deep', 'from']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:47: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(18 unique tokens: ['am', 'and', 'anything', 'battling', 'buildings']...) from 2 documents (total 43 corpus positions)\n",
      "INFO - 18:46:47: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(18 unique tokens: ['am', 'and', 'anything', 'battling', 'buildings']...) from 2 documents (total 43 corpus positions)\n",
      "INFO - 18:46:47: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(15 unique tokens: ['a', 'acted', 'around', 'at', 'buildings']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:47: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(15 unique tokens: ['a', 'acted', 'around', 'at', 'buildings']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:47: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(7 unique tokens: ['activity', 'by', 'casualties', 'the', 'to']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:47: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:47: built Dictionary(7 unique tokens: ['activity', 'by', 'casualties', 'the', 'to']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:46:48: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:48: built Dictionary(18 unique tokens: ['a', 'an', 'any', 'athlete', 'catastrophe']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:48: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:48: built Dictionary(18 unique tokens: ['a', 'an', 'any', 'athlete', 'catastrophe']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:48: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:48: built Dictionary(11 unique tokens: ['a', 'been', 'called', 'cool', 'on']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:46:48: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:48: built Dictionary(11 unique tokens: ['a', 'been', 'called', 'cool', 'on']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:46:48: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:48: built Dictionary(13 unique tokens: ['a', 'acute', 'care', 'catastrophic', 'follows']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:48: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:48: built Dictionary(13 unique tokens: ['a', 'acute', 'care', 'catastrophic', 'follows']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:48: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:48: built Dictionary(20 unique tokens: ['a', 'baseball', 'best', 'catastrophic', 'grab']...) from 2 documents (total 38 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:48: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:48: built Dictionary(20 unique tokens: ['a', 'baseball', 'best', 'catastrophic', 'grab']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:49: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:49: built Dictionary(11 unique tokens: ['a', 'about', 'building', 'do', 'inside']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:49: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:49: built Dictionary(11 unique tokens: ['a', 'about', 'building', 'do', 'inside']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:50: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(8 unique tokens: ['a', 'batting', 'call', 'collapse', 'what']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:50: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(8 unique tokens: ['a', 'batting', 'call', 'collapse', 'what']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:50: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(13 unique tokens: ['against', 'and', 'are', 'collapsed', 'dollar']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:50: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(14 unique tokens: ['against', 'and', 'are', 'collapsed', 'dollar']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:46:50: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(15 unique tokens: ['call', 'collapsed', 'considering', 'it', 'me']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:50: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(15 unique tokens: ['call', 'collapsed', 'considering', 'it', 'me']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:50: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(14 unique tokens: ['#love', 'collide', 'of', 'on', 'ripple']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:50: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(14 unique tokens: ['#love', 'collide', 'of', 'on', 'ripple']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:50: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(8 unique tokens: ['always', 'awkward', 'collide', 'super', 'when']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:50: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(8 unique tokens: ['always', 'awkward', 'collide', 'super', 'when']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:50: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(19 unique tokens: ['and', 'been', 'behind', 'but', 'cannot']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:50: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(19 unique tokens: ['and', 'been', 'behind', 'but', 'cannot']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:50: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:50: built Dictionary(16 unique tokens: ['and', 'as', 'collide', 'either', 'for']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:51: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:51: built Dictionary(16 unique tokens: ['and', 'as', 'collide', 'either', 'for']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:51: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:51: built Dictionary(17 unique tokens: ['after', 'as', 'back', 'collided', 'fell']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:51: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:51: built Dictionary(17 unique tokens: ['after', 'as', 'back', 'collided', 'fell']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:51: Removed 14 and 13 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:51: built Dictionary(3 unique tokens: ['the', 'has', 'it']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:46:51: Removed 14 and 13 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:51: built Dictionary(3 unique tokens: ['the', 'is', 'it']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:46:51: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:51: built Dictionary(17 unique tokens: ['a', 'car', 'collision', 'does', 'estimate']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:51: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:51: built Dictionary(17 unique tokens: ['a', 'car', 'collision', 'does', 'estimate']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:52: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(3 unique tokens: ['crash', 'it', 'shall']) from 2 documents (total 5 corpus positions)\n",
      "INFO - 18:46:52: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(3 unique tokens: ['crash', 'it', 'will']) from 2 documents (total 5 corpus positions)\n",
      "INFO - 18:46:52: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(14 unique tokens: ['&', 'biggest', 'crush', 'do', 'ever']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:52: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(14 unique tokens: ['&', 'biggest', 'crush', 'do', 'ever']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:52: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(11 unique tokens: ['a', 'crush', 'do', 'he', 'just']...) from 2 documents (total 32 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:52: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(11 unique tokens: ['a', 'crush', 'do', 'he', 'just']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:52: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(11 unique tokens: ['a', 'crush', 'do', 'he', 'just']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:52: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(11 unique tokens: ['a', 'crush', 'do', 'he', 'just']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:52: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:52: built Dictionary(11 unique tokens: ['a', 'crush', 'do', 'he', 'just']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:53: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:53: built Dictionary(14 unique tokens: ['and', 'because', 'clock', 'everyone', 'go']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:53: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:53: built Dictionary(15 unique tokens: ['and', 'because', 'clock', 'everyone', 'go']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:53: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:53: built Dictionary(10 unique tokens: ['all', 'be', 'damn', 'from', 'like']...) from 2 documents (total 19 corpus positions)\n",
      "INFO - 18:46:53: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:53: built Dictionary(10 unique tokens: ['all', 'be', 'damn', 'from', 'like']...) from 2 documents (total 19 corpus positions)\n",
      "INFO - 18:46:53: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:53: built Dictionary(17 unique tokens: ['a', 'and', 'be', 'being', 'car']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:53: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:53: built Dictionary(17 unique tokens: ['a', 'and', 'be', 'being', 'car']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:53: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:53: built Dictionary(25 unique tokens: ['always', 'am', 'and', 'at', 'back']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:46:53: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:53: built Dictionary(25 unique tokens: ['always', 'am', 'and', 'at', 'back']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:46:54: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(15 unique tokens: ['allow', 'ascend', 'but', 'damage', 'get']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:54: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(15 unique tokens: ['allow', 'ascend', 'but', 'damage', 'get']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:46:54: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(13 unique tokens: ['a', 'can', 'crazy', 'damage', 'do']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:54: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(13 unique tokens: ['a', 'can', 'crazy', 'damage', 'do']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:46:54: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(22 unique tokens: ['about', 'all', 'are', 'away', 'carful']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:54: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(22 unique tokens: ['about', 'all', 'are', 'away', 'carful']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:46:54: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(12 unique tokens: ['be', 'dangerous', 'for', 'in', 'of']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:54: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(12 unique tokens: ['be', 'dangerous', 'for', 'in', 'of']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:54: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:54: built Dictionary(25 unique tokens: ['a', 'are', 'attractive', 'beat', 'but']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:46:55: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:55: built Dictionary(25 unique tokens: ['a', 'are', 'attractive', 'beat', 'but']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:46:55: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:55: built Dictionary(13 unique tokens: ['all', 'and', 'dead', 'dies', 'else']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:55: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:55: built Dictionary(13 unique tokens: ['all', 'and', 'dead', 'dies', 'else']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:46:55: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:55: built Dictionary(8 unique tokens: ['am', 'dead', 'is', 'it', 'suing']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:55: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:55: built Dictionary(7 unique tokens: ['am', 'dead', 'is', 'it', 'suing']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:46:55: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:55: built Dictionary(6 unique tokens: ['death', 'forever', 'my', 'rep', 'the']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:46:55: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:55: built Dictionary(6 unique tokens: ['death', 'forever', 'my', 'rep', 'the']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:46:55: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:55: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:46:55: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:55: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:46:56: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(13 unique tokens: ['a', 'and', 'chrissie', 'deaths', 'finn']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:46:56: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(13 unique tokens: ['a', 'and', 'chrissie', 'deaths', 'finn']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:46:56: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(16 unique tokens: ['-', 'because', 'buying', 'deluge', 'is']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:56: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(15 unique tokens: ['-', 'because', 'buying', 'deluge', 'is']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:56: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(9 unique tokens: ['a', 'deluge', 'get', 'hour', 'in']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:56: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(9 unique tokens: ['a', 'deluge', 'get', 'hour', 'in']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:56: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(16 unique tokens: ['but', 'deluge', 'embrace', 'half-naked', 'is']...) from 2 documents (total 33 corpus positions)\n",
      "INFO - 18:46:56: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(16 unique tokens: ['but', 'deluge', 'embrace', 'half-naked', 'is']...) from 2 documents (total 33 corpus positions)\n",
      "INFO - 18:46:56: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:56: built Dictionary(18 unique tokens: ['at', 'but', 'church', 'in', 'is']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:57: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(17 unique tokens: ['at', 'but', 'church', 'in', 'is']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:57: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(6 unique tokens: ['exhibitor', 'in', 'missing', 'the', 'has']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:46:57: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(6 unique tokens: ['exhibitor', 'in', 'missing', 'the', 'is']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:46:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(17 unique tokens: ['and', 'are', 'colour', 'deluged', 'likely']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(17 unique tokens: ['and', 'are', 'colour', 'deluged', 'likely']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(18 unique tokens: ['and', 'are', 'colo', 'deluged', 'of']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(18 unique tokens: ['and', 'are', 'colo', 'deluged', 'of']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:57: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(17 unique tokens: ['and', 'are', 'colour', 'deluged', 'likely']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:57: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(17 unique tokens: ['and', 'are', 'colour', 'deluged', 'likely']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:57: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(18 unique tokens: ['abe', 'and', 'colour', 'deluged', 'likely']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:57: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(18 unique tokens: ['abe', 'and', 'colour', 'deluged', 'likely']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(16 unique tokens: ['are', 'colour', 'deluged', 'likely', 'of']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(16 unique tokens: ['are', 'colour', 'deluged', 'likely', 'of']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:57: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(18 unique tokens: ['and', 'are', 'colour', 'deluged', 'likely']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:57: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(18 unique tokens: ['and', 'are', 'colour', 'deluged', 'likely']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:57: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(18 unique tokens: ['and', 'are', 'colour', 'deluged', 'likely']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:57: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(18 unique tokens: ['and', 'are', 'colour', 'deluged', 'likely']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:57: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(20 unique tokens: ['and', 'are', 'colour', 'deluged', 'e']...) from 2 documents (total 42 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:46:57: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(20 unique tokens: ['and', 'are', 'colour', 'deluged', 'e']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:46:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(17 unique tokens: ['and', 'are', 'deluged', 'likely', 'of']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:57: built Dictionary(17 unique tokens: ['and', 'are', 'deluged', 'likely', 'of']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:58: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(17 unique tokens: ['and', 'colour', 'likely', 'of', 'or']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:58: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(17 unique tokens: ['and', 'colour', 'likely', 'of', 'or']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:46:58: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(5 unique tokens: ['demolish', 'everyone', 'having', 'had', 'we']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:46:58: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(5 unique tokens: ['demolish', 'everyone', 'having', 'we', 'would']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:46:58: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(19 unique tokens: ['.', 'a', 'and', 'become', 'build']...) from 2 documents (total 43 corpus positions)\n",
      "INFO - 18:46:58: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(19 unique tokens: ['.', 'a', 'and', 'become', 'build']...) from 2 documents (total 43 corpus positions)\n",
      "INFO - 18:46:58: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(12 unique tokens: ['a', 'and', 'bought', 'butter', 'demolished']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:58: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(12 unique tokens: ['a', 'and', 'bought', 'butter', 'demolished']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:46:58: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(9 unique tokens: ['about', 'by', 'demolished', 'home', 'homes']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:58: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(9 unique tokens: ['about', 'by', 'demolished', 'home', 'homes']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:46:58: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(12 unique tokens: ['a', 'demolition', 'even', 'if', 'in']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:46:58: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(12 unique tokens: ['a', 'demolition', 'even', 'if', 'in']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:46:58: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:58: built Dictionary(19 unique tokens: ['and', 'been', 'demolition', 'derby', 'down']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:59: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(20 unique tokens: ['and', 'been', 'demolition', 'derby', 'down']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:46:59: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(17 unique tokens: ['been', 'crocodile', 'derailed', 'do', 'gravy']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:59: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(18 unique tokens: ['been', 'crocodile', 'derailed', 'do', 'gravy']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:46:59: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(17 unique tokens: ['a', 'and', 'back', 'derailed', 'freeing']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:59: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(17 unique tokens: ['a', 'and', 'back', 'derailed', 'freeing']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:46:59: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(15 unique tokens: ['a', 'beautiful', 'degrees', 'derailed', 'in']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:59: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(16 unique tokens: ['a', 'beautiful', 'degrees', 'derailed', 'in']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:59: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(16 unique tokens: ['a', 'beautiful', 'degrees', 'derailed', 'in']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:46:59: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:46:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:46:59: built Dictionary(15 unique tokens: ['a', 'beautiful', 'degrees', 'derailed', 'in']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:00: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(15 unique tokens: ['a', 'am', 'and', 'at', 'because']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:00: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(15 unique tokens: ['a', 'am', 'and', 'at', 'because']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:00: Removed 11 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(7 unique tokens: ['-', ':', 'freak', 'freakiest', 'of']...) from 2 documents (total 13 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:00: Removed 11 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(7 unique tokens: ['-', ':', 'freak', 'freakiest', 'of']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:00: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(9 unique tokens: ['is', 'made', 'not', 'to', 'totally']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:00: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(8 unique tokens: ['is', 'made', 'not', 'to', 'totally']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:00: Removed 11 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(6 unique tokens: ['-', 'freak', 'freakiest', 'of', 'the']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:00: Removed 11 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(6 unique tokens: ['-', 'freak', 'freakiest', 'of', 'the']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:00: Removed 9 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(6 unique tokens: ['-', 'freak', 'freakiest', 'of', 'the']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:00: Removed 9 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(6 unique tokens: ['-', 'freak', 'freakiest', 'of', 'the']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:00: Removed 11 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(7 unique tokens: ['-', ':', 'freak', 'freakiest', 'of']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:00: Removed 11 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(7 unique tokens: ['-', ':', 'freak', 'freakiest', 'of']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:00: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(6 unique tokens: ['-', 'freak', 'freakiest', 'of', 'the']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:00: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(6 unique tokens: ['-', 'freak', 'freakiest', 'of', 'the']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:00: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:00: built Dictionary(23 unique tokens: ['always', 'an', 'attitude', 'be', 'broke']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:01: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(23 unique tokens: ['always', 'an', 'attitude', 'be', 'broke']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:01: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(23 unique tokens: ['always', 'an', 'attitude', 'be', 'broke']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:01: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(23 unique tokens: ['always', 'an', 'attitude', 'be', 'broke']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:01: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(23 unique tokens: ['always', 'an', 'attitude', 'be', 'broke']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:01: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(14 unique tokens: ['a', 'been', 'desolate', 'down', 'flight']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:01: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(14 unique tokens: ['a', 'been', 'desolate', 'down', 'flight']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:01: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(20 unique tokens: ['a', 'and', 'anything', 'bc', 'costly']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:01: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(19 unique tokens: ['a', 'and', 'anything', 'bc', 'costly']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:01: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(11 unique tokens: ['a', 'be', 'did', 'for', 'glad']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:01: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(11 unique tokens: ['a', 'be', 'did', 'for', 'glad']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:01: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(11 unique tokens: ['are', 'like', 'me', 'my', 'named']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:01: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:01: built Dictionary(11 unique tokens: ['are', 'like', 'me', 'my', 'named']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:02: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:02: built Dictionary(5 unique tokens: ['left', 'nothing', 'oppa', 'has', 'there']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:02: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:02: built Dictionary(5 unique tokens: ['left', 'nothing', 'oppa', 'is', 'there']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:02: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:02: built Dictionary(19 unique tokens: ['ambition', 'change', 'check', 'destroy', 'for']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:02: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:02: built Dictionary(19 unique tokens: ['ambition', 'change', 'check', 'destroy', 'for']...) from 2 documents (total 44 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:02: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:02: built Dictionary(13 unique tokens: ['be', 'destroyed', 'every', 'got', 'have']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:02: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:02: built Dictionary(13 unique tokens: ['be', 'destroyed', 'every', 'got', 'have']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:02: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:02: built Dictionary(12 unique tokens: ['#bored', 'a', 'forsee', 'in', 'loose']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:02: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:02: built Dictionary(12 unique tokens: ['#bored', 'a', 'forsee', 'in', 'loose']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:03: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:03: built Dictionary(4 unique tokens: ['destruction', 'indeed', 'has', 'what']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:03: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:03: built Dictionary(4 unique tokens: ['destruction', 'indeed', 'is', 'what']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:03: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:03: built Dictionary(7 unique tokens: ['destruction', 'road', 'the', 'to', 'ultimate']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:03: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:03: built Dictionary(7 unique tokens: ['destruction', 'road', 'the', 'to', 'ultimate']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:03: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:03: built Dictionary(12 unique tokens: ['at', 'detonate', 'era', 'found', 'grenade']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:03: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:03: built Dictionary(12 unique tokens: ['at', 'detonate', 'era', 'found', 'grenade']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:03: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:03: built Dictionary(13 unique tokens: ['a', 'anybody', 'be', 'devastated', 'ghostwriter']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:03: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:03: built Dictionary(14 unique tokens: ['a', 'anybody', 'be', 'devastated', 'ghostwriter']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:04: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(7 unique tokens: ['a', 'be', 'character', 'devastated', 'great']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:04: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(7 unique tokens: ['a', 'be', 'character', 'devastated', 'great']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:04: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(9 unique tokens: ['a', 'be', 'character', 'devastated', 'great']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:04: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(9 unique tokens: ['a', 'be', 'character', 'devastated', 'great']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:04: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(14 unique tokens: ['alternatives', 'because', 'best', 'considering', 'devastation']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:04: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(14 unique tokens: ['alternatives', 'because', 'best', 'considering', 'devastation']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:04: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(20 unique tokens: ['agree', 'angers', 'but', 'devastation', 'factor']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:04: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(20 unique tokens: ['agree', 'angers', 'but', 'devastation', 'factor']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:04: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(19 unique tokens: ['a', 'am', 'but', 'every', 'in']...) from 2 documents (total 43 corpus positions)\n",
      "INFO - 18:47:04: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:04: built Dictionary(19 unique tokens: ['a', 'am', 'but', 'every', 'in']...) from 2 documents (total 43 corpus positions)\n",
      "INFO - 18:47:05: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(19 unique tokens: ['a', 'absolutely', 'but', 'disaster', 'has']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:05: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(20 unique tokens: ['a', 'absolutely', 'but', 'disaster', 'has']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:05: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(17 unique tokens: ['about', 'aggression', 'an', 'going', 'have']...) from 2 documents (total 35 corpus positions)\n",
      "INFO - 18:47:05: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(17 unique tokens: ['about', 'aggression', 'an', 'going', 'have']...) from 2 documents (total 35 corpus positions)\n",
      "INFO - 18:47:05: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(12 unique tokens: ['but', 'different', 'for', 'hey', 'more']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:05: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:05: built Dictionary(12 unique tokens: ['but', 'different', 'for', 'hey', 'more']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:05: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(14 unique tokens: ['&', 'any', 'be', 'both', 'bud']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:05: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(14 unique tokens: ['&', 'any', 'be', 'both', 'bud']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:05: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(13 unique tokens: ['-', 'a', 'daily', 'desert', 'finding']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:05: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(13 unique tokens: ['-', 'a', 'daily', 'desert', 'finding']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:05: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(12 unique tokens: ['an', 'and', 'both', 'by', 'eerie']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:47:05: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:05: built Dictionary(12 unique tokens: ['an', 'and', 'both', 'by', 'eerie']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:47:06: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(7 unique tokens: ['drown', 'in', 'river', 'the', 'walk']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:06: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(7 unique tokens: ['drown', 'in', 'river', 'the', 'walk']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:06: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(11 unique tokens: ['an', 'drown', 'idiot', 'makes', 'me']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:06: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(11 unique tokens: ['an', 'drown', 'idiot', 'makes', 'me']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:06: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(14 unique tokens: ['am', 'do', 'drown', 'for', 'not']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:06: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(15 unique tokens: ['am', 'do', 'drown', 'for', 'not']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:06: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(15 unique tokens: ['am', 'do', 'drown', 'for', 'not']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:06: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(15 unique tokens: ['am', 'do', 'drown', 'for', 'not']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:06: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(15 unique tokens: ['am', 'do', 'drown', 'for', 'not']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:06: Removed 12 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(7 unique tokens: ['/', 'and', 'awesome', 'my', 'proxies']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:06: Removed 12 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(7 unique tokens: ['/', 'and', 'awesome', 'my', 'proxies']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:06: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(13 unique tokens: ['at', 'but', 'drowned', 'emotion', 'gets']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:06: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(13 unique tokens: ['at', 'but', 'drowned', 'emotion', 'gets']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:06: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(12 unique tokens: ['a', 'cocaine', 'dollars', 'drought', 'drowned']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:06: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:06: built Dictionary(12 unique tokens: ['a', 'cocaine', 'dollars', 'drought', 'drowned']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:07: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(12 unique tokens: ['a', 'backing', 'fondness', 'for', 'have']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:07: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(12 unique tokens: ['a', 'backing', 'fondness', 'for', 'have']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:07: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(14 unique tokens: ['am', 'and', 'drowning', 'fire', 'from']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:47:07: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(14 unique tokens: ['am', 'and', 'drowning', 'fire', 'from']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:47:07: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(11 unique tokens: ['already', 'and', 'are', 'be', 'drowning']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:07: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(11 unique tokens: ['already', 'and', 'are', 'be', 'drowning']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:07: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:07: built Dictionary(12 unique tokens: ['am', 'and', 'drowning', 'going', 'hw']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:07: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(12 unique tokens: ['am', 'and', 'drowning', 'going', 'hw']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:07: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(11 unique tokens: ['a', 'answer', 'blowing', 'friend', 'in']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:47:07: Removed 7 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(10 unique tokens: ['a', 'answer', 'blowing', 'friend', 'in']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:47:07: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(15 unique tokens: ['a', 'and', 'be', 'big', 'brewing']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:07: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:07: built Dictionary(15 unique tokens: ['a', 'and', 'be', 'big', 'brewing']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:08: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(6 unique tokens: ['and', 'could', 'electrocute', 'me', 'say']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:08: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(6 unique tokens: ['and', 'could', 'electrocute', 'me', 'say']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:08: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(17 unique tokens: ['a', 'am', 'but', 'electrocute', 'for']...) from 2 documents (total 37 corpus positions)\n",
      "INFO - 18:47:08: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(17 unique tokens: ['a', 'am', 'but', 'electrocute', 'for']...) from 2 documents (total 37 corpus positions)\n",
      "INFO - 18:47:08: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(10 unique tokens: ['do', 'electrocute', 'help', 'himself', 'let']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:08: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(10 unique tokens: ['do', 'electrocute', 'help', 'himself', 'let']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:08: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(24 unique tokens: ['am', 'as', 'babies', 'but', 'care']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:08: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(24 unique tokens: ['am', 'as', 'babies', 'but', 'care']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:08: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(9 unique tokens: ['charging', 'does', 'electrocute', 'me', 'my']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:08: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(9 unique tokens: ['charging', 'does', 'electrocute', 'me', 'my']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:08: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(11 unique tokens: ['but', 'come', 'help', 'love', 'says']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:08: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(11 unique tokens: ['but', 'come', 'help', 'love', 'says']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:08: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(13 unique tokens: ['ass', 'electrocute', 'fucking', 'hell', 'lol']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:08: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(13 unique tokens: ['ass', 'electrocute', 'fucking', 'hell', 'lol']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:08: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:08: built Dictionary(13 unique tokens: ['ass', 'electrocute', 'fucking', 'hell', 'lol']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:09: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(13 unique tokens: ['ass', 'electrocute', 'fucking', 'hell', 'lol']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:09: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(13 unique tokens: ['ass', 'electrocute', 'fucking', 'hell', 'lol']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:09: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(15 unique tokens: ['am', 'ass', 'electrocute', 'fucking', 'hell']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:09: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(15 unique tokens: ['am', 'ass', 'electrocute', 'fucking', 'hell']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:09: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(19 unique tokens: ['a', 'an', 'and', 'cables', 'death']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:09: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(19 unique tokens: ['a', 'an', 'and', 'cables', 'death']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:09: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(19 unique tokens: ['a', 'all', 'be', 'good', 'hairdryer']...) from 2 documents (total 39 corpus positions)\n",
      "INFO - 18:47:09: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(19 unique tokens: ['a', 'all', 'be', 'good', 'hairdryer']...) from 2 documents (total 39 corpus positions)\n",
      "INFO - 18:47:09: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(21 unique tokens: ['a', 'all', 'be', 'good', 'hairdryer']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:09: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(22 unique tokens: ['a', 'all', 'be', 'good', 'hairdryer']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:09: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(22 unique tokens: ['a', 'all', 'be', 'good', 'hairdryer']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:09: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(21 unique tokens: ['a', 'all', 'be', 'good', 'hairdryer']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:09: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(15 unique tokens: ['a', 'after', 'attractive', 'blow', 'dry']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:09: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(15 unique tokens: ['a', 'after', 'attractive', 'blow', 'dry']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:09: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(13 unique tokens: ['cat', 'christmas', 'end', 'from', 'getting']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:09: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:09: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:09: built Dictionary(12 unique tokens: ['cat', 'christmas', 'end', 'from', 'getting']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:10: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:10: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:10: built Dictionary(17 unique tokens: ['am', 'an', 'answer', 'call', 'emergency']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:10: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:10: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:10: built Dictionary(17 unique tokens: ['am', 'an', 'answer', 'call', 'emergency']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:10: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:10: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:10: built Dictionary(9 unique tokens: ['an', 'devise', 'end', 'is', 'of']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:10: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:10: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:10: built Dictionary(8 unique tokens: ['an', 'devise', 'end', 'is', 'of']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:10: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:10: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:10: built Dictionary(15 unique tokens: ['about', 'comparing', 'emergency', 'haha', 'own']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:10: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:10: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:10: built Dictionary(15 unique tokens: ['about', 'comparing', 'emergency', 'haha', 'own']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:11: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:11: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:11: built Dictionary(17 unique tokens: ['a', 'been', 'body', 'by', 'decide']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:11: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:11: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:11: built Dictionary(17 unique tokens: ['a', 'been', 'body', 'by', 'decide']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:11: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:11: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:11: built Dictionary(19 unique tokens: ['a', 'alarm', 'and', 'decide', 'evacuate']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:11: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:11: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:11: built Dictionary(19 unique tokens: ['a', 'alarm', 'and', 'decide', 'evacuate']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:11: Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:11: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:11: built Dictionary(10 unique tokens: ['abandon', 'building', 'dealbreaker', 'evacuate', 'it']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:11: Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:11: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:11: built Dictionary(11 unique tokens: ['abandon', 'building', 'dealbreaker', 'evacuate', 'it']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:11: Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:11: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:11: built Dictionary(11 unique tokens: ['abandon', 'building', 'dealbreaker', 'evacuate', 'it']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:11: Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:11: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:11: built Dictionary(10 unique tokens: ['abandon', 'building', 'dealbreaker', 'evacuate', 'it']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:12: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:12: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:12: built Dictionary(21 unique tokens: ['-', '/', 'am', 'and', 'back']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:12: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:12: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:12: built Dictionary(21 unique tokens: ['-', '/', 'am', 'and', 'back']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:12: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:12: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:12: built Dictionary(8 unique tokens: ['heads', 'make', 'one', 'their', 'to']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:12: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:12: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:12: built Dictionary(8 unique tokens: ['heads', 'make', 'one', 'their', 'to']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:12: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:12: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:12: built Dictionary(7 unique tokens: ['going', 'mess', 'to', 'up', 'you']...) from 2 documents (total 12 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:12: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:12: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:12: built Dictionary(7 unique tokens: ['going', 'mess', 'to', 'up', 'you']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:13: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(13 unique tokens: ['atomic', 'day.', 'exploded', 'in', 'is']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:13: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(12 unique tokens: ['atomic', 'day.', 'exploded', 'in', 'is']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:13: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(16 unique tokens: ['&', 'about', 'and', 'beginning', 'brought']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:13: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(16 unique tokens: ['&', 'about', 'and', 'beginning', 'brought']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:13: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(18 unique tokens: ['ago', 'anime', 'be', 'big', 'exploded']...) from 2 documents (total 35 corpus positions)\n",
      "INFO - 18:47:13: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(17 unique tokens: ['ago', 'anime', 'be', 'big', 'exploded']...) from 2 documents (total 35 corpus positions)\n",
      "INFO - 18:47:13: Removed 8 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(13 unique tokens: ['a', 'bomb', 'definitely', 'different', 'from']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:47:13: Removed 8 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(12 unique tokens: ['a', 'bomb', 'definitely', 'different', 'from']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:47:13: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(16 unique tokens: ['&', 'about', 'and', 'beginning', 'brought']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:13: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(16 unique tokens: ['&', 'about', 'and', 'beginning', 'brought']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:13: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(13 unique tokens: ['do', 'getting', 'going', 'great', 'hahaha']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:13: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(13 unique tokens: ['do', 'getting', 'going', 'great', 'hahaha']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:13: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(7 unique tokens: ['a', 'always', 'for', 'good', 'pretty']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:13: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:13: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:13: built Dictionary(7 unique tokens: ['a', 'always', 'for', 'good', 'pretty']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:14: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:14: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:14: built Dictionary(14 unique tokens: ['again', 'and', 'be', 'between', 'force']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:14: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:14: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:14: built Dictionary(14 unique tokens: ['again', 'and', 'be', 'between', 'force']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:14: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:14: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:14: built Dictionary(7 unique tokens: ['bitches', 'famine', 'guys', 'then', 'these']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:14: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:14: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:14: built Dictionary(7 unique tokens: ['bitches', 'famine', 'guys', 'then', 'these']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:14: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:14: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:14: built Dictionary(7 unique tokens: ['bitches', 'famine', 'guys', 'then', 'these']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:14: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:14: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:14: built Dictionary(7 unique tokens: ['bitches', 'famine', 'guys', 'then', 'these']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:14: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:14: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:14: built Dictionary(7 unique tokens: ['bitches', 'famine', 'guys', 'then', 'these']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:15: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:15: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:15: built Dictionary(15 unique tokens: ['am', 'fatal', 'is', 'kindness', 'level']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:15: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:15: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:15: built Dictionary(14 unique tokens: ['am', 'fatal', 'is', 'kindness', 'level']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:15: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:15: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:15: built Dictionary(14 unique tokens: ['and', 'been', 'for', 'fruit', 'good']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:15: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:15: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:15: built Dictionary(14 unique tokens: ['and', 'been', 'for', 'fruit', 'good']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:15: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:15: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:15: built Dictionary(14 unique tokens: ['all', 'characters', 'do', 'fatalities', 'have']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:15: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:15: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:15: built Dictionary(14 unique tokens: ['all', 'characters', 'do', 'fatalities', 'have']...) from 2 documents (total 26 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:15: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:15: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:15: built Dictionary(11 unique tokens: ['a', 'every', 'fatality', 'makes', 'me']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:16: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(11 unique tokens: ['a', 'every', 'fatality', 'makes', 'me']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:16: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(24 unique tokens: ['a', 'and', 'because', 'but', 'every']...) from 2 documents (total 56 corpus positions)\n",
      "INFO - 18:47:16: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(25 unique tokens: ['a', 'and', 'because', 'but', 'every']...) from 2 documents (total 56 corpus positions)\n",
      "INFO - 18:47:16: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(19 unique tokens: ['about', 'all', 'and', 'do', 'fear']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:16: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(19 unique tokens: ['about', 'all', 'and', 'do', 'fear']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:16: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(11 unique tokens: ['am', 'fear', 'going', 'hack', 'in']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:16: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(11 unique tokens: ['am', 'fear', 'going', 'hack', 'in']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:16: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(11 unique tokens: ['am', 'fear', 'now', 'of', 'reopening']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:16: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(11 unique tokens: ['am', 'fear', 'now', 'of', 'reopening']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:16: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(16 unique tokens: ['a', 'and', 'be', 'books', 'fear']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:16: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(16 unique tokens: ['a', 'and', 'be', 'books', 'fear']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:16: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(6 unique tokens: ['about', 'all', 'taking', 'to', 'has']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:47:16: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(6 unique tokens: ['about', 'all', 'taking', 'to', 'is']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:47:16: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(8 unique tokens: ['a', 'fire', 'out', 'put', 'still']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:16: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:16: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:16: built Dictionary(8 unique tokens: ['a', 'fire', 'out', 'put', 'still']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(13 unique tokens: ['been', 'for', 'going', 'it', 'on']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(13 unique tokens: ['been', 'for', 'going', 'it', 'on']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(13 unique tokens: ['been', 'for', 'going', 'it', 'on']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(13 unique tokens: ['been', 'for', 'going', 'it', 'on']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(13 unique tokens: ['been', 'for', 'going', 'it', 'on']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(13 unique tokens: ['a', 'do', 'does', 'fire', 'in']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(13 unique tokens: ['a', 'do', 'does', 'fire', 'in']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:17: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(8 unique tokens: ['by', 'northgate', 'on', 'taco', 'the']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:17: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(8 unique tokens: ['by', 'northgate', 'on', 'taco', 'the']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(21 unique tokens: ['&', 'a', 'ambulances', 'at', 'fire']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(22 unique tokens: ['&', 'a', 'ambulances', 'at', 'fire']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(22 unique tokens: ['&', 'a', 'ambulances', 'at', 'fire']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 4 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(21 unique tokens: ['&', 'a', 'ambulances', 'at', 'fire']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:47:17: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(23 unique tokens: ['&', 'a', 'ambulances', 'at', 'fire']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:47:17: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(23 unique tokens: ['&', 'a', 'ambulances', 'at', 'fire']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(16 unique tokens: ['across', 'fire', 'hi', 'in', 'me']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:17: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:17: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:17: built Dictionary(16 unique tokens: ['across', 'fire', 'hi', 'in', 'me']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(10 unique tokens: ['a', 'fire', 'in', 'lot', 'parking']...) from 2 documents (total 17 corpus positions)\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(10 unique tokens: ['a', 'fire', 'in', 'lot', 'parking']...) from 2 documents (total 17 corpus positions)\n",
      "INFO - 18:47:18: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(22 unique tokens: ['a', 'are', 'becoming', 'call', 'cool']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:18: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(21 unique tokens: ['a', 'are', 'becoming', 'call', 'cool']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:18: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(15 unique tokens: ['a', 'and', 'around', 'do', 'first']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:18: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(15 unique tokens: ['a', 'and', 'around', 'do', 'first']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:18: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(17 unique tokens: ['annual', 'around', 'at', 'best', 'ever.']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:18: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(17 unique tokens: ['annual', 'around', 'at', 'best', 'ever.']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:18: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(7 unique tokens: ['cry', 'flames', 'in', 'my', 'pity']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:18: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(7 unique tokens: ['cry', 'flames', 'in', 'my', 'pity']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:18: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(3 unique tokens: ['&', 'has', 'that']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:18: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(3 unique tokens: ['&', 'is', 'that']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:18: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(16 unique tokens: ['be', 'can', 'doubt', 'feel', 'games']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:18: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(16 unique tokens: ['be', 'can', 'doubt', 'feel', 'games']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:18: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:18: built Dictionary(18 unique tokens: ['be', 'can', 'doubt', 'feel', 'games']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:47:19: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(17 unique tokens: ['be', 'can', 'doubt', 'feel', 'games']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:47:19: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(13 unique tokens: ['a', 'ball', 'flattened', 'hide', 'kids']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:19: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(13 unique tokens: ['a', 'ball', 'flattened', 'hide', 'kids']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:19: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(14 unique tokens: ['be', 'fallacy', 'flattened', 'is', 'it']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:47:19: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(13 unique tokens: ['be', 'fallacy', 'flattened', 'is', 'it']...) from 2 documents (total 38 corpus positions)\n",
      "INFO - 18:47:19: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(20 unique tokens: ['and', 'as', 'by', 'came', 'dreamy']...) from 2 documents (total 41 corpus positions)\n",
      "INFO - 18:47:19: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(20 unique tokens: ['and', 'as', 'by', 'came', 'dreamy']...) from 2 documents (total 41 corpus positions)\n",
      "INFO - 18:47:19: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(21 unique tokens: ['a', 'always', 'am', 'been', 'drowning']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:19: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(21 unique tokens: ['a', 'always', 'am', 'been', 'drowning']...) from 2 documents (total 44 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:19: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(10 unique tokens: ['do', 'flood', 'his', 'in', 'not']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:19: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:19: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:19: built Dictionary(10 unique tokens: ['do', 'flood', 'his', 'in', 'not']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:20: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:20: built Dictionary(20 unique tokens: ['can', 'charging', 'come', 'dilute', 'flooding']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:20: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:20: built Dictionary(19 unique tokens: ['can', 'charging', 'come', 'dilute', 'flooding']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:20: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:20: built Dictionary(17 unique tokens: ['anymore', 'are', 'at', 'cannot', 'find']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:20: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:20: built Dictionary(17 unique tokens: ['anymore', 'are', 'at', 'cannot', 'find']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:20: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:20: built Dictionary(20 unique tokens: ['a', 'after', 'be', 'degree', 'flash']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:20: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:20: built Dictionary(20 unique tokens: ['a', 'after', 'be', 'degree', 'flash']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:20: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:20: built Dictionary(25 unique tokens: ['&', 'a', 'are', 'brain', 'causing']...) from 2 documents (total 54 corpus positions)\n",
      "INFO - 18:47:20: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:20: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:20: built Dictionary(25 unique tokens: ['&', 'a', 'are', 'brain', 'causing']...) from 2 documents (total 54 corpus positions)\n",
      "INFO - 18:47:21: Removed 10 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:21: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:21: built Dictionary(19 unique tokens: ['a', 'by', 'fire', 'forest', 'has']...) from 2 documents (total 37 corpus positions)\n",
      "INFO - 18:47:21: Removed 10 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:21: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:21: built Dictionary(19 unique tokens: ['a', 'by', 'fire', 'forest', 'has']...) from 2 documents (total 37 corpus positions)\n",
      "INFO - 18:47:21: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:21: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:21: built Dictionary(12 unique tokens: ['about', 'be', 'concerned', 'fires', 'forest']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:47:21: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:21: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:21: built Dictionary(12 unique tokens: ['about', 'be', 'concerned', 'fires', 'forest']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:47:21: Removed 13 and 13 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:21: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:21: built Dictionary(2 unique tokens: ['hailing', 'has']) from 2 documents (total 3 corpus positions)\n",
      "INFO - 18:47:22: Removed 13 and 13 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(2 unique tokens: ['hailing', 'is']) from 2 documents (total 3 corpus positions)\n",
      "INFO - 18:47:22: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(17 unique tokens: ['anyone', 'cupcake', 'down', 'earth', 'end']...) from 2 documents (total 37 corpus positions)\n",
      "INFO - 18:47:22: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(17 unique tokens: ['anyone', 'cupcake', 'down', 'earth', 'end']...) from 2 documents (total 37 corpus positions)\n",
      "INFO - 18:47:22: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(19 unique tokens: ['actually', 'annoyed', 'are', 'by', 'hung']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:22: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(19 unique tokens: ['actually', 'annoyed', 'are', 'by', 'hung']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:22: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:47:22: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:47:22: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(6 unique tokens: ['are', 'collectibles', 'the', 'they', 'has']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:47:22: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(6 unique tokens: ['are', 'collectibles', 'the', 'they', 'is']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:47:22: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(11 unique tokens: ['&', 'allegations', 'are', 'attack', 'false']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:22: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(12 unique tokens: ['&', 'allegations', 'are', 'attack', 'false']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:22: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(14 unique tokens: ['better', 'by', 'do', 'either', 'elite']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:22: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(13 unique tokens: ['better', 'by', 'do', 'either', 'elite']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:22: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:22: built Dictionary(8 unique tokens: ['before', 'die', 'do', 'get', 'i']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:22: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:22: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:22: built Dictionary(8 unique tokens: ['before', 'die', 'do', 'get', 'i']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:23: Removed 9 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(16 unique tokens: ['#mufc', 'back', 'better', 'dear', 'has']...) from 2 documents (total 31 corpus positions)\n",
      "INFO - 18:47:23: Removed 9 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(16 unique tokens: ['#mufc', 'back', 'better', 'dear', 'has']...) from 2 documents (total 31 corpus positions)\n",
      "INFO - 18:47:23: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(17 unique tokens: ['a', 'aguero', 'bad', 'berahino', 'better']...) from 2 documents (total 33 corpus positions)\n",
      "INFO - 18:47:23: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(17 unique tokens: ['a', 'aguero', 'bad', 'berahino', 'better']...) from 2 documents (total 33 corpus positions)\n",
      "INFO - 18:47:23: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(11 unique tokens: ['a', 'be', 'fire', 'hazard', 'if']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:23: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(11 unique tokens: ['a', 'be', 'fire', 'hazard', 'if']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:23: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(8 unique tokens: ['alright', 'be', 'but', 'going', 'shifted']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:23: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(8 unique tokens: ['alright', 'be', 'but', 'going', 'shifted']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:23: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(8 unique tokens: ['alright', 'be', 'but', 'going', 'shifted']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:23: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(8 unique tokens: ['alright', 'be', 'but', 'going', 'shifted']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:23: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(8 unique tokens: ['alright', 'be', 'but', 'going', 'shifted']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:23: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(10 unique tokens: ['alright', 'am', 'be', 'but', 'going']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:23: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(10 unique tokens: ['alright', 'am', 'be', 'but', 'going']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:23: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(7 unique tokens: ['&', 'a', 'previews', 'tipster', 'v']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:23: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(7 unique tokens: ['&', 'a', 'previews', 'tipster', 'v']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:23: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(9 unique tokens: ['crystals', 'dangling', 'hazardous', 'piercing', 'potentially']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:23: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(9 unique tokens: ['crystals', 'dangling', 'hazardous', 'piercing', 'potentially']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:23: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(9 unique tokens: ['be', 'getting', 'hazardous', 'into', 'this']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:23: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:23: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:23: built Dictionary(9 unique tokens: ['be', 'getting', 'hazardous', 'into', 'this']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:24: Removed 10 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(11 unique tokens: ['a', 'had', 'have', 'heat', 'it']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:47:24: Removed 10 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(12 unique tokens: ['a', 'had', 'have', 'heat', 'it']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:47:24: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(14 unique tokens: ['a', 'had', 'have', 'heat', 'it']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:24: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(14 unique tokens: ['a', 'had', 'have', 'heat', 'it']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:24: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(4 unique tokens: ['a', 'heat', 'has', 'it']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:24: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(4 unique tokens: ['a', 'heat', 'is', 'it']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:24: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(11 unique tokens: ['cool', 'finally', 'heat', 'kind', 'kulli']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:24: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:24: built Dictionary(11 unique tokens: ['cool', 'finally', 'heat', 'kind', 'kulli']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:24: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(9 unique tokens: ['hotter', 'than', 'the', 'there', 'this']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:24: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(9 unique tokens: ['hotter', 'than', 'the', 'there', 'this']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:24: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(17 unique tokens: ['bcuz', 'cooler', 'dances', 'feeling', 'for']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:24: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(17 unique tokens: ['bcuz', 'cooler', 'dances', 'feeling', 'for']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:24: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(20 unique tokens: ['a', 'an', 'based', 'got', 'in']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:24: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:24: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:24: built Dictionary(20 unique tokens: ['a', 'an', 'based', 'got', 'in']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:25: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:25: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:25: built Dictionary(10 unique tokens: ['and', 'check', 'got', 'it', 'just']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:47:25: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:25: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:25: built Dictionary(10 unique tokens: ['and', 'check', 'got', 'it', 'just']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:47:25: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:25: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:25: built Dictionary(13 unique tokens: ['click', 'hashtag', 'hijack', 'lots', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:25: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:25: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:25: built Dictionary(13 unique tokens: ['click', 'hashtag', 'hijack', 'lots', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:25: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:25: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:25: built Dictionary(10 unique tokens: ['hijacker', 'kick', 'link', 'off.', 'password']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:25: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:25: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:25: built Dictionary(10 unique tokens: ['hijacker', 'kick', 'link', 'off.', 'password']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:25: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:25: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:25: built Dictionary(14 unique tokens: ['a', 'even', 'example', 'in', 'is']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:25: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:25: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:25: built Dictionary(13 unique tokens: ['a', 'even', 'example', 'in', 'is']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:26: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(13 unique tokens: ['(', 'a', 'always', 'at', 'hostage']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:26: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(13 unique tokens: ['(', 'a', 'always', 'at', 'hostage']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:26: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(3 unique tokens: ['have', 'has', 'who']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:26: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(3 unique tokens: ['have', 'is', 'who']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:26: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(11 unique tokens: ['-', 'a', 'and', 'being', 'horrific']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:26: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(11 unique tokens: ['-', 'a', 'and', 'being', 'horrific']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:26: Removed 8 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(13 unique tokens: ['damsels', 'guess', 'hostages', 'in', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:26: Removed 8 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(14 unique tokens: ['damsels', 'guess', 'hostages', 'in', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:26: Removed 8 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(14 unique tokens: ['damsels', 'guess', 'hostages', 'in', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:26: Removed 8 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:26: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:26: built Dictionary(13 unique tokens: ['damsels', 'guess', 'hostages', 'in', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:27: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(11 unique tokens: ['be', 'black', 'hurricane', 'hurricanes', 'name']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:47:27: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(11 unique tokens: ['be', 'black', 'hurricane', 'hurricanes', 'name']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:47:27: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(8 unique tokens: ['every', 'injured', 'why', 'wonder', 'year']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:27: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:27: built Dictionary(8 unique tokens: ['every', 'injured', 'why', 'wonder', 'year']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:27: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(10 unique tokens: ['does', 'every', 'get', 'his', 'injured']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:27: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(10 unique tokens: ['does', 'every', 'get', 'his', 'injured']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:27: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(7 unique tokens: ['is', 'only', 'surprise', 'that', 'they']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:27: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(7 unique tokens: ['is', 'only', 'surprise', 'that', 'they']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:27: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(20 unique tokens: ['a', 'believe', 'cannot', 'expected', 'falling']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:27: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:27: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:27: built Dictionary(20 unique tokens: ['a', 'believe', 'cannot', 'expected', 'falling']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:28: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(13 unique tokens: ['ankles', 'both', 'but', 'had', 'injuries']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:28: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(13 unique tokens: ['ankles', 'both', 'but', 'had', 'injuries']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:28: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(9 unique tokens: ['a', 'are', 'his', 'injuries', 'to']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:28: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(9 unique tokens: ['a', 'are', 'his', 'injuries', 'to']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:28: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(9 unique tokens: ['again', 'fucking', 'go', 'injuries', 'the']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:28: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(9 unique tokens: ['again', 'fucking', 'go', 'injuries', 'the']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:47:28: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(16 unique tokens: ['ass', 'baby', 'beautiful', 'big', 'climbed']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:28: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(16 unique tokens: ['ass', 'baby', 'beautiful', 'big', 'climbed']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:28: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(13 unique tokens: ['and', 'bypass', 'inundated', 'is', 'my']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:28: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:28: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:28: built Dictionary(12 unique tokens: ['and', 'bypass', 'inundated', 'is', 'my']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:29: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(16 unique tokens: ['be', 'beach', 'ever', 'inundated', 'is']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:29: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(15 unique tokens: ['be', 'beach', 'ever', 'inundated', 'is']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:29: Removed 8 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(14 unique tokens: ['being', 'hit', 'inundated', 'like', 'lol']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:47:29: Removed 8 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(14 unique tokens: ['being', 'hit', 'inundated', 'like', 'lol']...) from 2 documents (total 27 corpus positions)\n",
      "INFO - 18:47:29: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(11 unique tokens: ['banana', 'be', 'gifts', 'inundated', 'loaf']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:29: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(11 unique tokens: ['banana', 'be', 'gifts', 'inundated', 'loaf']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:29: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(14 unique tokens: ['all', 'being', 'but', 'coming', 'inundated']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:29: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(14 unique tokens: ['all', 'being', 'but', 'coming', 'inundated']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:29: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(22 unique tokens: ['&', 'able', 'are', 'as', 'be']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:29: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(22 unique tokens: ['&', 'able', 'are', 'as', 'be']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:29: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(22 unique tokens: ['about', 'articles', 'be', 'but', 'by']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:29: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:29: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:29: built Dictionary(22 unique tokens: ['about', 'articles', 'be', 'but', 'by']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:30: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(15 unique tokens: ['a', 'are', 'be', 'caught', 'for']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:30: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(16 unique tokens: ['a', 'are', 'be', 'caught', 'for']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:30: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(16 unique tokens: ['a', 'are', 'be', 'caught', 'for']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:30: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(15 unique tokens: ['a', 'are', 'be', 'caught', 'for']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:30: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(16 unique tokens: ['a', 'assholes', 'by', 'landslide', 'love']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:30: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(16 unique tokens: ['a', 'assholes', 'by', 'landslide', 'love']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:30: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(11 unique tokens: ['are', 'but', 'cold', 'feet', 'hands']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:30: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(11 unique tokens: ['are', 'but', 'cold', 'feet', 'hands']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:30: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(16 unique tokens: ['#baby', 'a', 'are', 'class', 'cries']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:30: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(15 unique tokens: ['#baby', 'a', 'are', 'class', 'cries']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:30: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(15 unique tokens: ['a', 'across', 'assumes', 'deadly', 'lightning']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:30: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:30: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:30: built Dictionary(15 unique tokens: ['a', 'across', 'assumes', 'deadly', 'lightning']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:31: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(16 unique tokens: ['a', 'about', 'accidentally', 'broken', 'downstairs']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:31: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(17 unique tokens: ['a', 'about', 'accidentally', 'broken', 'downstairs']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:31: Removed 14 and 13 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(15 unique tokens: ['.', 'and', 'been', 'by', 'have']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:31: Removed 14 and 13 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(15 unique tokens: ['.', 'and', 'been', 'by', 'have']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:31: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(10 unique tokens: ['an', 'and', 'does', 'end', 'imply']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:31: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(10 unique tokens: ['an', 'and', 'does', 'end', 'imply']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:31: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(17 unique tokens: ['act', 'an', 'and', 'been', 'cannot']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:31: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(17 unique tokens: ['act', 'an', 'and', 'been', 'cannot']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:31: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(11 unique tokens: ['and', 'camps', 'come', 'concentration', 'in.']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:31: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(11 unique tokens: ['and', 'camps', 'come', 'concentration', 'in.']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:31: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(11 unique tokens: ['and', 'day', 'domination', 'murder', 'over']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:31: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:31: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:31: built Dictionary(11 unique tokens: ['and', 'day', 'domination', 'murder', 'over']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:32: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(11 unique tokens: ['and', 'day', 'domination', 'murder', 'over']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:32: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(21 unique tokens: ['apparent', 'because', 'blah', 'for', 'guess']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:32: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(21 unique tokens: ['apparent', 'because', 'blah', 'for', 'guess']...) from 2 documents (total 44 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:32: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(18 unique tokens: ['a', 'become', 'daily', 'deal', 'dickheads']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:32: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(18 unique tokens: ['a', 'become', 'daily', 'deal', 'dickheads']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:32: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(21 unique tokens: ['a', 'and', 'aspect', 'btw', 'do']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:32: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(21 unique tokens: ['a', 'and', 'aspect', 'btw', 'do']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:32: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(14 unique tokens: ['a', 'become', 'friends', 'of', 'or']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:32: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(14 unique tokens: ['a', 'become', 'friends', 'of', 'or']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:32: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(10 unique tokens: ['a', 'class', 'hope', 'let', 'mass']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:32: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:32: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:32: built Dictionary(10 unique tokens: ['a', 'class', 'hope', 'let', 'mass']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:33: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(14 unique tokens: ['a', 'avoided', 'been', 'could', 'fact']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:33: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(14 unique tokens: ['a', 'avoided', 'been', 'could', 'fact']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:33: Removed 16 and 15 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(3 unique tokens: ['@', 'has', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:33: Removed 16 and 15 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(3 unique tokens: ['@', 'is', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:33: Removed 16 and 15 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(3 unique tokens: ['@', 'has', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:33: Removed 16 and 15 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(3 unique tokens: ['@', 'is', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:33: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(14 unique tokens: ['about', 'ask', 'asked', 'but', 'for']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:33: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(14 unique tokens: ['about', 'ask', 'asked', 'but', 'for']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:33: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(3 unique tokens: ['via', 'has', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:33: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(3 unique tokens: ['via', 'is', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:33: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(23 unique tokens: ['a', 'and', 'are', 'but', 'conclusions']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:33: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(22 unique tokens: ['a', 'and', 'are', 'but', 'conclusions']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:33: Removed 23 and 22 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(3 unique tokens: ['&', 'has', 'what']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:33: Removed 23 and 22 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:33: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:33: built Dictionary(3 unique tokens: ['&', 'is', 'what']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:34: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(18 unique tokens: ['&', 'a', 'base', 'call', 'conservative']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:34: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(18 unique tokens: ['&', 'a', 'base', 'call', 'conservative']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:34: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(6 unique tokens: ['a', 'chewing', 'like', 'looks', 'on']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:34: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(6 unique tokens: ['a', 'chewing', 'like', 'looks', 'on']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:34: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(4 unique tokens: ['not', 'pretty', 'has', 'it']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:34: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(4 unique tokens: ['not', 'pretty', 'is', 'it']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:34: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(15 unique tokens: ['a', 'and', 'between', 'dears', 'difference']...) from 2 documents (total 30 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:34: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(15 unique tokens: ['a', 'and', 'between', 'dears', 'difference']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:34: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:47:34: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:47:34: Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(10 unique tokens: ['a', 'am', 'chewing', 'like', 'on']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:34: Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(11 unique tokens: ['a', 'am', 'chewing', 'like', 'on']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:34: Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(11 unique tokens: ['a', 'am', 'chewing', 'like', 'on']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:34: Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(10 unique tokens: ['a', 'am', 'chewing', 'like', 'on']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:34: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(9 unique tokens: ['a', 'disaster', 'girls', 'last', 'natural']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:34: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:34: built Dictionary(10 unique tokens: ['a', 'disaster', 'girls', 'last', 'natural']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:35: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:35: built Dictionary(10 unique tokens: ['a', 'disaster', 'girls', 'last', 'natural']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:35: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:35: built Dictionary(9 unique tokens: ['a', 'disaster', 'girls', 'last', 'natural']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:35: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:35: built Dictionary(19 unique tokens: ['a', 'addition', 'am', 'anyway', 'but']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:35: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:35: built Dictionary(19 unique tokens: ['a', 'addition', 'am', 'anyway', 'but']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:35: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:35: built Dictionary(14 unique tokens: ['and', 'disaster', 'dying', 'from', 'in']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:35: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:35: built Dictionary(14 unique tokens: ['and', 'disaster', 'dying', 'from', 'in']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:35: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:35: built Dictionary(16 unique tokens: ['-', 'disaster', 'do', 'encyclopedia', 'free']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:35: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:35: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:35: built Dictionary(16 unique tokens: ['-', 'disaster', 'do', 'encyclopedia', 'free']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:36: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:36: built Dictionary(21 unique tokens: ['a', 'and', 'battleship', 'but', 'cross']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:36: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:36: built Dictionary(21 unique tokens: ['a', 'and', 'battleship', 'but', 'cross']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:36: Removed 8 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:36: built Dictionary(16 unique tokens: ['and', 'another', 'any', 'at', 'election']...) from 2 documents (total 31 corpus positions)\n",
      "INFO - 18:47:36: Removed 8 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:36: built Dictionary(16 unique tokens: ['and', 'another', 'any', 'at', 'election']...) from 2 documents (total 31 corpus positions)\n",
      "INFO - 18:47:36: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:36: built Dictionary(12 unique tokens: ['centre', 'closest', 'completely', 'hypo', 'not']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:36: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:36: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:36: built Dictionary(12 unique tokens: ['centre', 'closest', 'completely', 'hypo', 'not']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:37: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(5 unique tokens: ['get', 'obliterated', 'think', 'tonight', 'shall']) from 2 documents (total 9 corpus positions)\n",
      "INFO - 18:47:37: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(5 unique tokens: ['get', 'obliterated', 'think', 'tonight', 'will']) from 2 documents (total 9 corpus positions)\n",
      "INFO - 18:47:37: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(24 unique tokens: ['&', 'a', 'all', 'an', 'arcade']...) from 2 documents (total 47 corpus positions)\n",
      "INFO - 18:47:37: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(24 unique tokens: ['&', 'a', 'all', 'an', 'arcade']...) from 2 documents (total 47 corpus positions)\n",
      "INFO - 18:47:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(18 unique tokens: ['and', 'far', 'fixing', 'govt', 'if']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:37: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:37: built Dictionary(17 unique tokens: ['and', 'far', 'fixing', 'govt', 'if']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:37: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(11 unique tokens: ['always', 'are', 'but', 'like', 'many']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:37: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(12 unique tokens: ['always', 'are', 'but', 'like', 'many']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:37: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(8 unique tokens: ['an', 'how', 'oil', 'respond', 'tell']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:37: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(8 unique tokens: ['an', 'how', 'oil', 'respond', 'tell']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:37: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:37: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:37: built Dictionary(3 unique tokens: ['to', 'has', 'what']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:38: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(3 unique tokens: ['to', 'is', 'what']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:38: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(3 unique tokens: ['to', 'has', 'what']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:38: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(3 unique tokens: ['to', 'is', 'what']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:38: Removed 12 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(5 unique tokens: ['of', 'outbreak', 'to', 'has', 'what']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:38: Removed 12 and 11 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(5 unique tokens: ['of', 'outbreak', 'to', 'is', 'what']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:38: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(4 unique tokens: ['at', 'be', 'very', 'shall']) from 2 documents (total 7 corpus positions)\n",
      "INFO - 18:47:38: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(4 unique tokens: ['at', 'be', 'very', 'will']) from 2 documents (total 7 corpus positions)\n",
      "INFO - 18:47:38: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(12 unique tokens: ['another', 'have', 'mate', 'pandemonium', 'posted']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:38: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(12 unique tokens: ['another', 'have', 'mate', 'pandemonium', 'posted']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:38: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:38: built Dictionary(5 unique tokens: ['do', 'not', 'panic', 'has', 'it']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:38: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:38: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(5 unique tokens: ['do', 'not', 'panic', 'is', 'it']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:39: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(20 unique tokens: ['-', 'actually', 'and', 'bad', 'could']...) from 2 documents (total 39 corpus positions)\n",
      "INFO - 18:47:39: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(20 unique tokens: ['-', 'actually', 'and', 'bad', 'could']...) from 2 documents (total 39 corpus positions)\n",
      "INFO - 18:47:39: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(11 unique tokens: ['am', 'and', 'anxiety', 'attacks', 'crippling']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:47:39: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(11 unique tokens: ['am', 'and', 'anxiety', 'attacks', 'crippling']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:47:39: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(27 unique tokens: ['&', 'a', 'attack', 'but', 'caused']...) from 2 documents (total 57 corpus positions)\n",
      "INFO - 18:47:39: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(27 unique tokens: ['&', 'a', 'attack', 'but', 'caused']...) from 2 documents (total 57 corpus positions)\n",
      "INFO - 18:47:39: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(15 unique tokens: ['as', 'be', 'come', 'down', 'market']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:39: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(15 unique tokens: ['as', 'be', 'come', 'down', 'market']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:39: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(13 unique tokens: ['are', 'basic', 'but', 'faints', 'if']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:47:39: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(13 unique tokens: ['are', 'basic', 'but', 'faints', 'if']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:47:39: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(10 unique tokens: ['already', 'am', 'and', 'bc', 'crying']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:39: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(10 unique tokens: ['already', 'am', 'and', 'bc', 'crying']...) from 2 documents (total 22 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:39: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(13 unique tokens: ['bad', 'been', 'haha.', 'have', 'not']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:39: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(13 unique tokens: ['bad', 'been', 'haha.', 'have', 'not']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:39: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(20 unique tokens: ['brothers', 'debbie', 'found', 'had', 'his']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:39: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(20 unique tokens: ['brothers', 'debbie', 'found', 'had', 'his']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:39: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:39: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:39: built Dictionary(20 unique tokens: ['brothers', 'debbie', 'found', 'had', 'his']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:40: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:40: built Dictionary(19 unique tokens: ['brothers', 'debbie', 'found', 'had', 'his']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:40: built Dictionary(11 unique tokens: ['cannot', 'fuck', 'fucking', 'me', 'niggas']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:40: built Dictionary(11 unique tokens: ['cannot', 'fuck', 'fucking', 'me', 'niggas']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:40: built Dictionary(11 unique tokens: ['cannot', 'fuck', 'fucking', 'me', 'niggas']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:40: built Dictionary(11 unique tokens: ['cannot', 'fuck', 'fucking', 'me', 'niggas']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:40: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:40: built Dictionary(11 unique tokens: ['cannot', 'fuck', 'fucking', 'me', 'niggas']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:40: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:40: built Dictionary(7 unique tokens: ['bound', 'closed', 'from', 'of', 'to']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:40: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:40: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:40: built Dictionary(7 unique tokens: ['bound', 'closed', 'from', 'of', 'to']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:41: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:41: built Dictionary(22 unique tokens: ['am', 'and', 'bad', 'be', 'changes']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:41: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:41: built Dictionary(22 unique tokens: ['am', 'and', 'bad', 'be', 'changes']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:41: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:41: built Dictionary(20 unique tokens: ['a', 'all', 'as', 'cannot', 'computer']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:41: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:41: built Dictionary(20 unique tokens: ['a', 'all', 'as', 'cannot', 'computer']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:41: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:41: built Dictionary(15 unique tokens: ['a', 'commute', 'evening', 'impacting', 'in']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:41: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:41: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:41: built Dictionary(15 unique tokens: ['a', 'commute', 'evening', 'impacting', 'in']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:42: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:42: built Dictionary(20 unique tokens: ['am', 'and', 'animal', 'before', 'blood']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:42: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:42: built Dictionary(20 unique tokens: ['am', 'and', 'animal', 'before', 'blood']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:42: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:42: built Dictionary(10 unique tokens: ['a', 'been', 'crew', 'for', 'is']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:42: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:42: built Dictionary(9 unique tokens: ['a', 'been', 'crew', 'for', 'is']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:42: built Dictionary(15 unique tokens: ['by', 'die', 'dopey', 'get', 'going']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:42: built Dictionary(14 unique tokens: ['by', 'die', 'dopey', 'get', 'going']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:47:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:42: built Dictionary(15 unique tokens: ['ago', 'as', 'at', 'but', 'dog']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:42: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:42: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:43: built Dictionary(15 unique tokens: ['ago', 'as', 'at', 'but', 'dog']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:43: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:43: built Dictionary(12 unique tokens: ['a', 'ass', 'big', 'jail', 'me']...) from 2 documents (total 23 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:43: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:43: built Dictionary(12 unique tokens: ['a', 'ass', 'big', 'jail', 'me']...) from 2 documents (total 23 corpus positions)\n",
      "INFO - 18:47:43: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:43: built Dictionary(17 unique tokens: ['ally', 'an', 'because', 'cannot', 'did']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:43: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:43: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:43: built Dictionary(17 unique tokens: ['ally', 'an', 'because', 'cannot', 'did']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:44: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(7 unique tokens: ['already', 'been', 'hell', 'to', 'upgraded']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:44: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(7 unique tokens: ['already', 'been', 'hell', 'to', 'upgraded']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:44: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(14 unique tokens: ['and', 'because', 'been', 'behind', 'has']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:44: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(14 unique tokens: ['and', 'because', 'been', 'behind', 'has']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:44: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(15 unique tokens: ['a', 'be', 'but', 'demonstration', 'how']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:44: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(15 unique tokens: ['a', 'be', 'but', 'demonstration', 'how']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:44: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(9 unique tokens: ['and', 'be', 'everywhere', 'of', 'one']...) from 2 documents (total 17 corpus positions)\n",
      "INFO - 18:47:44: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(9 unique tokens: ['and', 'be', 'everywhere', 'of', 'one']...) from 2 documents (total 17 corpus positions)\n",
      "INFO - 18:47:44: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(20 unique tokens: ['but', 'do', 'friends', 'give', 'guy']...) from 2 documents (total 45 corpus positions)\n",
      "INFO - 18:47:44: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(20 unique tokens: ['but', 'do', 'friends', 'give', 'guy']...) from 2 documents (total 45 corpus positions)\n",
      "INFO - 18:47:44: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:44: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:44: built Dictionary(7 unique tokens: ['a', 'holidays', 'in', 'ruin', 'the']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:45: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(7 unique tokens: ['a', 'holidays', 'in', 'ruin', 'the']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:45: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(12 unique tokens: ['a', 'and', 'are', 'ask', 'do']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:45: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(12 unique tokens: ['a', 'and', 'are', 'ask', 'do']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:45: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(17 unique tokens: ['a', 'are', 'cervix', 'definitely', 'few']...) from 2 documents (total 39 corpus positions)\n",
      "INFO - 18:47:45: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(17 unique tokens: ['a', 'are', 'cervix', 'definitely', 'few']...) from 2 documents (total 39 corpus positions)\n",
      "INFO - 18:47:45: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(7 unique tokens: ['have', 'if', 'life', 'my', 'ruin']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:45: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(7 unique tokens: ['have', 'if', 'life', 'my', 'ruin']...) from 2 documents (total 13 corpus positions)\n",
      "INFO - 18:47:45: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(7 unique tokens: ['be', 'because', 'listening', 'must', 'to']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:45: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(7 unique tokens: ['be', 'because', 'listening', 'must', 'to']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:45: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(8 unique tokens: ['-', 'and', 'booze', 'friends', 'need']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:45: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:45: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:45: built Dictionary(8 unique tokens: ['-', 'and', 'booze', 'friends', 'need']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:46: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(20 unique tokens: ['a', 'and', 'at', 'believing', 'bit']...) from 2 documents (total 39 corpus positions)\n",
      "INFO - 18:47:46: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(20 unique tokens: ['a', 'and', 'at', 'believing', 'bit']...) from 2 documents (total 39 corpus positions)\n",
      "INFO - 18:47:46: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:47:46: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:46: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:47:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(7 unique tokens: ['&', 'am', 'can', 'he', 'hot']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(7 unique tokens: ['&', 'am', 'can', 'he', 'hot']...) from 2 documents (total 14 corpus positions)\n",
      "INFO - 18:47:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(10 unique tokens: ['/', 'at', 'baby', 'fine', 'screaming']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:46: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(10 unique tokens: ['/', 'at', 'baby', 'fine', 'screaming']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:46: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(3 unique tokens: ['am', 'has', 'he']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:46: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(3 unique tokens: ['am', 'he', 'is']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:46: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(20 unique tokens: ['am', 'and', 'ass', 'face', 'going']...) from 2 documents (total 47 corpus positions)\n",
      "INFO - 18:47:46: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:46: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:46: built Dictionary(20 unique tokens: ['am', 'and', 'ass', 'face', 'going']...) from 2 documents (total 47 corpus positions)\n",
      "INFO - 18:47:47: Removed 13 and 12 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(4 unique tokens: ['not', 'will', 'has', 'what']) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:47:47: Removed 13 and 12 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(4 unique tokens: ['not', 'will', 'is', 'what']) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:47:47: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(4 unique tokens: ['not', 'will', 'has', 'what']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:47: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(4 unique tokens: ['not', 'will', 'is', 'what']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:47: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(15 unique tokens: ['a', 'and', 'be', 'blamed', 'by']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:47: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(15 unique tokens: ['a', 'and', 'be', 'blamed', 'by']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:47: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(5 unique tokens: ['#funny', 'not', 'will', 'has', 'what']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:47: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(5 unique tokens: ['#funny', 'not', 'will', 'is', 'what']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:47: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(15 unique tokens: ['am', 'and', 'going', 'happens', 'insane']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:47: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:47: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:47: built Dictionary(15 unique tokens: ['am', 'and', 'going', 'happens', 'insane']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:48: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(14 unique tokens: ['&', 'a', 'delays', 'developed', 'downtown']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:48: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(14 unique tokens: ['&', 'a', 'delays', 'developed', 'downtown']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:47:48: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(5 unique tokens: ['a', 'in', 'sinkhole', 'has', 'there']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:48: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(5 unique tokens: ['a', 'in', 'sinkhole', 'is', 'there']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:48: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(12 unique tokens: ['an', 'delete', 'di', 'nor', 'not']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:48: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(12 unique tokens: ['an', 'delete', 'di', 'nor', 'not']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:48: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(20 unique tokens: ['a', 'appease', 'but', 'cannot', 'chance']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:48: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(19 unique tokens: ['a', 'appease', 'but', 'cannot', 'chance']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:48: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(20 unique tokens: ['am', 'around', 'away', 'can', 'do']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:48: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(20 unique tokens: ['am', 'around', 'away', 'can', 'do']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:48: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(13 unique tokens: ['a', 'been', 'everyday', 'for', 'it']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:48: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:48: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:48: built Dictionary(13 unique tokens: ['a', 'been', 'everyday', 'for', 'it']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:49: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(20 unique tokens: ['a', 'announce', 'are', 'bc', 'dull']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:49: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(19 unique tokens: ['a', 'announce', 'are', 'bc', 'dull']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:49: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(17 unique tokens: ['a', 'blowing', 'going', 'hope', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:49: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(16 unique tokens: ['a', 'blowing', 'going', 'hope', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:49: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(17 unique tokens: ['apt', 'but', 'damn', 'flying', 'going']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:49: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(16 unique tokens: ['apt', 'but', 'damn', 'flying', 'going']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:47:49: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(7 unique tokens: ['and', 'going', 'helicopters', 'in', 'on']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:49: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(7 unique tokens: ['and', 'going', 'helicopters', 'in', 'on']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:49: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(3 unique tokens: ['by', 'has', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:49: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(3 unique tokens: ['by', 'is', 'it']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:47:49: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(11 unique tokens: ['a', 'and', 'conversation', 'good', 'just']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:49: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(11 unique tokens: ['a', 'and', 'conversation', 'good', 'just']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:49: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(6 unique tokens: ['did', 'if', 'smoke', 'up', 'you']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:49: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:49: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:49: built Dictionary(6 unique tokens: ['did', 'if', 'smoke', 'up', 'you']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:47:50: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(20 unique tokens: ['an', 'ash', 'breath', 'but', 'cigs']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:50: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(21 unique tokens: ['an', 'ash', 'breath', 'but', 'cigs']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:50: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(21 unique tokens: ['an', 'ash', 'breath', 'but', 'cigs']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:50: Removed 4 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(20 unique tokens: ['an', 'ash', 'breath', 'but', 'cigs']...) from 2 documents (total 42 corpus positions)\n",
      "INFO - 18:47:50: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(20 unique tokens: ['a', 'bar', 'coming', 'get', 'how']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:50: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(20 unique tokens: ['a', 'bar', 'coming', 'get', 'how']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:50: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(21 unique tokens: ['a', 'bar', 'coming', 'get', 'he']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:50: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(20 unique tokens: ['a', 'bar', 'coming', 'get', 'he']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:50: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(21 unique tokens: ['a', 'bar', 'coming', 'get', 'he']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:47:50: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(20 unique tokens: ['a', 'bar', 'coming', 'get', 'he']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:47:50: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(11 unique tokens: ['captured', 'in', 'incredible', 'on', 'photo']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:50: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:50: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:50: built Dictionary(11 unique tokens: ['captured', 'in', 'incredible', 'on', 'photo']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:51: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:51: built Dictionary(21 unique tokens: ['about', 'dance', 'for', 'hasil', 'in']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:47:51: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(20 unique tokens: ['about', 'dance', 'for', 'hasil', 'in']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:47:51: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(10 unique tokens: ['all', 'could', 'okay', 'storm', 'that']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:51: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(10 unique tokens: ['all', 'could', 'okay', 'storm', 'that']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:51: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(12 unique tokens: ['but', 'clear', 'has', 'head', 'in']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:51: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(13 unique tokens: ['but', 'clear', 'has', 'head', 'in']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:51: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(15 unique tokens: ['a', 'after', 'anything', 'better', 'capoeira']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:51: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(15 unique tokens: ['a', 'after', 'anything', 'better', 'capoeira']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:51: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(13 unique tokens: ['a', 'being', 'do', 'not', 'on']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:51: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(13 unique tokens: ['a', 'being', 'do', 'not', 'on']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:51: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(11 unique tokens: ['a', 'failure', 'maybe', 'more', 'of']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:51: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:51: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:51: built Dictionary(11 unique tokens: ['a', 'failure', 'maybe', 'more', 'of']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:52: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:52: built Dictionary(14 unique tokens: ['a', 'constantly', 'emerging', 'engine', 'excessive']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:52: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:52: built Dictionary(14 unique tokens: ['a', 'constantly', 'emerging', 'engine', 'excessive']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:52: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:52: built Dictionary(22 unique tokens: ['a', 'bomb', 'but', 'choice', 'choices']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:52: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:52: built Dictionary(23 unique tokens: ['a', 'bomb', 'but', 'choice', 'choices']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:52: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:52: built Dictionary(5 unique tokens: ['a', 'bomb', 'suicide', 'has', 'she']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:52: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:52: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:52: built Dictionary(5 unique tokens: ['a', 'bomb', 'suicide', 'is', 'she']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:47:53: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(10 unique tokens: ['been', 'bombing', 'from', 'having', 'his']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:53: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(10 unique tokens: ['been', 'bombing', 'from', 'having', 'his']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:53: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(14 unique tokens: ['&', 'bombing', 'chopping', 'gays', 'heads']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:53: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(14 unique tokens: ['&', 'bombing', 'chopping', 'gays', 'heads']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:53: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(12 unique tokens: ['as', 'bombing', 'like', 'maybe', 'payback']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:53: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(12 unique tokens: ['as', 'bombing', 'like', 'maybe', 'payback']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:53: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(10 unique tokens: ['center', 'has', 'in', 'no', 'not']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:53: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(11 unique tokens: ['center', 'has', 'in', 'no', 'not']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:53: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:53: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:53: built Dictionary(15 unique tokens: ['able', 'am', 'and', 'any', 'be']...) from 2 documents (total 33 corpus positions)\n",
      "INFO - 18:47:54: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(15 unique tokens: ['able', 'am', 'and', 'any', 'be']...) from 2 documents (total 33 corpus positions)\n",
      "INFO - 18:47:54: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:54: built Dictionary(10 unique tokens: ['.', 'but', 'final', 'have', 'is']...) from 2 documents (total 19 corpus positions)\n",
      "INFO - 18:47:54: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(10 unique tokens: ['.', 'but', 'final', 'have', 'is']...) from 2 documents (total 19 corpus positions)\n",
      "INFO - 18:47:54: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(23 unique tokens: ['&', 'a', 'been', 'can', 'going']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:47:54: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(23 unique tokens: ['&', 'a', 'been', 'can', 'going']...) from 2 documents (total 52 corpus positions)\n",
      "INFO - 18:47:54: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(11 unique tokens: ['a', 'cannot', 'day', 'i', 'like']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:54: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(11 unique tokens: ['a', 'cannot', 'day', 'i', 'like']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:47:54: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(5 unique tokens: ['see', 'survive', 'tomorrow', 'you', 'shall']) from 2 documents (total 9 corpus positions)\n",
      "INFO - 18:47:54: Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(5 unique tokens: ['see', 'survive', 'tomorrow', 'you', 'will']) from 2 documents (total 9 corpus positions)\n",
      "INFO - 18:47:54: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(23 unique tokens: ['a', 'are', 'bets', 'days', 'if']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:54: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(23 unique tokens: ['a', 'are', 'bets', 'days', 'if']...) from 2 documents (total 48 corpus positions)\n",
      "INFO - 18:47:54: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(17 unique tokens: ['and', 'do', 'exactly', 'fled', 'he']...) from 2 documents (total 37 corpus positions)\n",
      "INFO - 18:47:54: Removed 5 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(17 unique tokens: ['and', 'do', 'exactly', 'fled', 'he']...) from 2 documents (total 37 corpus positions)\n",
      "INFO - 18:47:54: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(11 unique tokens: ['a', 'be', 'even', 'ever', 'generation']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:54: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(11 unique tokens: ['a', 'be', 'even', 'ever', 'generation']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:54: Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(10 unique tokens: ['a', 'am', 'day', 'glad', 'lonely']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:54: Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:54: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:54: built Dictionary(11 unique tokens: ['a', 'am', 'day', 'glad', 'lonely']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:55: Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:55: built Dictionary(11 unique tokens: ['a', 'am', 'day', 'glad', 'lonely']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:55: Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:55: built Dictionary(10 unique tokens: ['a', 'am', 'day', 'glad', 'lonely']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:55: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:55: built Dictionary(13 unique tokens: ['and', 'for', 'good.', 'hope', 'let']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:55: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:55: built Dictionary(13 unique tokens: ['and', 'for', 'good.', 'hope', 'let']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:55: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:55: built Dictionary(20 unique tokens: ['-', 'are', 'awaits', 'be', 'better']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:55: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:55: built Dictionary(21 unique tokens: ['-', 'are', 'awaits', 'be', 'better']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:47:55: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:55: built Dictionary(14 unique tokens: ['+', 'a', 'about', 'by', 'documentary']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:55: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:55: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:55: built Dictionary(14 unique tokens: ['+', 'a', 'about', 'by', 'documentary']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:56: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:56: built Dictionary(21 unique tokens: ['a', 'already', 'by', 'deal', 'done']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:56: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:56: built Dictionary(21 unique tokens: ['a', 'already', 'by', 'deal', 'done']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:47:56: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:56: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:47:56: Removed 11 and 10 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:56: At least one of the documents had no words that were in the vocabulary. Aborting (returning inf).\n",
      "INFO - 18:47:56: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:56: built Dictionary(16 unique tokens: ['-->', 'already', 'and', 'biggest', 'for']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:56: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:56: built Dictionary(16 unique tokens: ['-->', 'already', 'and', 'biggest', 'for']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:56: built Dictionary(12 unique tokens: ['a', 'dick', 'hes', 'like', 'never']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:47:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:56: built Dictionary(12 unique tokens: ['a', 'dick', 'hes', 'like', 'never']...) from 2 documents (total 21 corpus positions)\n",
      "INFO - 18:47:56: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:56: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:56: built Dictionary(13 unique tokens: ['and', 'its', 'lightning', 'missed', 'rly']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:57: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(13 unique tokens: ['and', 'its', 'lightning', 'missed', 'rly']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:47:57: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(12 unique tokens: ['#', '#back', 'and', 'gotta', 'listen']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:57: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(12 unique tokens: ['#', '#back', 'and', 'gotta', 'listen']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:47:57: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(13 unique tokens: ['#', '#back', 'and', 'gotta', 'is']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:47:57: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(12 unique tokens: ['#', '#back', 'and', 'gotta', 'is']...) from 2 documents (total 25 corpus positions)\n",
      "INFO - 18:47:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(15 unique tokens: ['&', 'ever', 'gloomy', 'it', 'lol']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:57: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(15 unique tokens: ['&', 'ever', 'gloomy', 'it', 'lol']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:57: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(3 unique tokens: ['bet', 'do', 'shall']) from 2 documents (total 5 corpus positions)\n",
      "INFO - 18:47:57: Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(3 unique tokens: ['bet', 'do', 'will']) from 2 documents (total 5 corpus positions)\n",
      "INFO - 18:47:57: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:57: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:57: built Dictionary(9 unique tokens: ['a', 'happens', 'meets', 'tornado', 'volcano']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:58: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(9 unique tokens: ['a', 'happens', 'meets', 'tornado', 'volcano']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:47:58: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(13 unique tokens: ['but', 'get', 'hard', 'is', 'means']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:58: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(12 unique tokens: ['but', 'get', 'hard', 'is', 'means']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:47:58: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(6 unique tokens: ['a', 'have', 'nose', 'wonderful', 'has']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:58: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(6 unique tokens: ['a', 'have', 'nose', 'wonderful', 'is']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:47:58: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(17 unique tokens: ['a', 'am', 'and', 'but', 'did']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:58: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(18 unique tokens: ['a', 'am', 'and', 'but', 'did']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:58: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(18 unique tokens: ['a', 'am', 'and', 'but', 'did']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:58: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(18 unique tokens: ['a', 'am', 'and', 'but', 'did']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:58: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(18 unique tokens: ['a', 'am', 'and', 'but', 'did']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:47:58: Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(15 unique tokens: ['and', 'but', 'cat', 'going', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:58: Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(16 unique tokens: ['and', 'but', 'cat', 'going', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:58: Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(16 unique tokens: ['and', 'but', 'cat', 'going', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:58: Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:58: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:58: built Dictionary(16 unique tokens: ['and', 'but', 'cat', 'going', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:59: Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(16 unique tokens: ['and', 'but', 'cat', 'going', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:59: Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(16 unique tokens: ['and', 'but', 'cat', 'going', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:59: Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(16 unique tokens: ['and', 'but', 'cat', 'going', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:59: Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(15 unique tokens: ['and', 'but', 'cat', 'going', 'in']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:59: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(21 unique tokens: ['a', 'about', 'and', 'as', 'do']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:47:59: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(22 unique tokens: ['a', 'about', 'and', 'as', 'do']...) from 2 documents (total 46 corpus positions)\n",
      "INFO - 18:47:59: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(4 unique tokens: ['still', 'traumatised', 'has', 'he']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:59: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(4 unique tokens: ['still', 'traumatised', 'he', 'is']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:47:59: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(6 unique tokens: ['a', 'lot', 'of', 'traumatised', 'has']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:47:59: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(6 unique tokens: ['a', 'lot', 'of', 'traumatised', 'is']...) from 2 documents (total 10 corpus positions)\n",
      "INFO - 18:47:59: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(14 unique tokens: ['and', 'are', 'be', 'car', 'for']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:59: Removed 6 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(14 unique tokens: ['and', 'are', 'be', 'car', 'for']...) from 2 documents (total 29 corpus positions)\n",
      "INFO - 18:47:59: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(15 unique tokens: ['and', 'are', 'be', 'car', 'for']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:59: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:47:59: built Dictionary(16 unique tokens: ['and', 'are', 'be', 'car', 'for']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:47:59: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:47:59: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(13 unique tokens: ['cannot', 'face', 'her', 'is', 'pepper']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:48:00: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(12 unique tokens: ['cannot', 'face', 'her', 'is', 'pepper']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:48:00: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(10 unique tokens: ['haha', 'is', 'just', 'more', 'stupid']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:48:00: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(9 unique tokens: ['haha', 'is', 'just', 'more', 'stupid']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:48:00: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(11 unique tokens: ['haha', 'he', 'is', 'just', 'more']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:00: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(10 unique tokens: ['haha', 'he', 'is', 'just', 'more']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:00: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(16 unique tokens: ['a', 'and', 'boy', 'chain', 'gave']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:48:00: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(16 unique tokens: ['a', 'and', 'boy', 'chain', 'gave']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:48:00: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(3 unique tokens: ['to', 'shall', 'you']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:48:00: Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(3 unique tokens: ['to', 'will', 'you']) from 2 documents (total 4 corpus positions)\n",
      "INFO - 18:48:00: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(11 unique tokens: ['been', 'given', 'guys', 'hear', 'that']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:00: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(11 unique tokens: ['been', 'given', 'guys', 'hear', 'that']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:00: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(11 unique tokens: ['again.', 'for', 'happing', 'hey', 'shooting']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:00: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(11 unique tokens: ['again.', 'for', 'happing', 'hey', 'shooting']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:00: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(8 unique tokens: ['got', 'have', 'in', 'must', 'trouble']...) from 2 documents (total 17 corpus positions)\n",
      "INFO - 18:48:00: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(8 unique tokens: ['got', 'have', 'in', 'must', 'trouble']...) from 2 documents (total 17 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:48:00: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(7 unique tokens: ['know', 'to', 'trouble', 'who', 'you']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:48:00: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(7 unique tokens: ['know', 'to', 'trouble', 'who', 'you']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:48:00: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(20 unique tokens: ['all', 'are', 'bats', 'defense', 'in']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:48:00: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:00: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:00: built Dictionary(19 unique tokens: ['all', 'are', 'bats', 'defense', 'in']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:48:01: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(9 unique tokens: ['going', 'one', 'pretty', 'seems', 'sure']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:48:01: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(9 unique tokens: ['going', 'one', 'pretty', 'seems', 'sure']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:48:01: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(11 unique tokens: ['going', 'is', 'one', 'pretty', 'seems']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:01: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(10 unique tokens: ['going', 'is', 'one', 'pretty', 'seems']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:01: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(11 unique tokens: ['all', 'go', 'just', 'my', 'out']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:01: Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(11 unique tokens: ['all', 'go', 'just', 'my', 'out']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:01: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(18 unique tokens: ['a', 'been', 'but', 'crap', 'forever']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:48:01: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(18 unique tokens: ['a', 'been', 'but', 'crap', 'forever']...) from 2 documents (total 34 corpus positions)\n",
      "INFO - 18:48:01: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(10 unique tokens: ['alil', 'at', 'end', 'like', 'nah']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:48:01: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(10 unique tokens: ['alil', 'at', 'end', 'like', 'nah']...) from 2 documents (total 18 corpus positions)\n",
      "INFO - 18:48:01: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(6 unique tokens: ['a', 'give', 'him', 'titty', 'twister']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:48:01: Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(6 unique tokens: ['a', 'give', 'him', 'titty', 'twister']...) from 2 documents (total 11 corpus positions)\n",
      "INFO - 18:48:01: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:01: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:01: built Dictionary(14 unique tokens: ['all', 'by', 'fantastic', 'for', 'now']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:48:01: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:02: built Dictionary(14 unique tokens: ['all', 'by', 'fantastic', 'for', 'now']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:48:02: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:02: built Dictionary(9 unique tokens: ['a', 'as', 'do', 'life', 'not']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:48:02: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:02: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:02: built Dictionary(9 unique tokens: ['a', 'as', 'do', 'life', 'not']...) from 2 documents (total 16 corpus positions)\n",
      "INFO - 18:48:03: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(11 unique tokens: ['a', 'and', 'bad', 'else', 'for']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:03: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(11 unique tokens: ['a', 'and', 'bad', 'else', 'for']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:03: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(11 unique tokens: ['a', 'and', 'bad', 'else', 'for']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:03: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(11 unique tokens: ['a', 'and', 'bad', 'else', 'for']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:03: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(11 unique tokens: ['a', 'and', 'bad', 'else', 'for']...) from 2 documents (total 20 corpus positions)\n",
      "INFO - 18:48:03: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(14 unique tokens: ['a', 'ball', 'by', 'foul', 'freak']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:48:03: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(13 unique tokens: ['a', 'ball', 'by', 'foul', 'freak']...) from 2 documents (total 30 corpus positions)\n",
      "INFO - 18:48:03: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(7 unique tokens: ['a', 'going', 'like', 'war', 'zone']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:48:03: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(7 unique tokens: ['a', 'going', 'like', 'war', 'zone']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:48:03: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(19 unique tokens: ['a', 'ahead', 'as', 'best', 'go']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:48:03: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(19 unique tokens: ['a', 'ahead', 'as', 'best', 'go']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:48:03: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(19 unique tokens: ['a', 'ahead', 'as', 'best', 'go']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:48:03: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(19 unique tokens: ['a', 'ahead', 'as', 'best', 'go']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:48:03: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(19 unique tokens: ['a', 'ahead', 'as', 'best', 'go']...) from 2 documents (total 40 corpus positions)\n",
      "INFO - 18:48:03: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(21 unique tokens: ['a', 'ahead', 'as', 'best', 'go']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:48:03: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(21 unique tokens: ['a', 'ahead', 'as', 'best', 'go']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:48:03: Removed 7 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(11 unique tokens: ['allow', 'me', 'mindset', 'not', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:48:03: Removed 7 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(12 unique tokens: ['allow', 'me', 'mindset', 'not', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:48:03: Removed 7 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:03: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:03: built Dictionary(12 unique tokens: ['allow', 'me', 'mindset', 'not', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:48:04: Removed 7 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(11 unique tokens: ['allow', 'me', 'mindset', 'not', 'of']...) from 2 documents (total 26 corpus positions)\n",
      "INFO - 18:48:04: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(12 unique tokens: ['-', 'automatic', 'especially', 'for', 'legitimate']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:48:04: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(12 unique tokens: ['-', 'automatic', 'especially', 'for', 'legitimate']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:48:04: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(15 unique tokens: ['carry', 'duty', 'in', 'military', 'not']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:48:04: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(15 unique tokens: ['carry', 'duty', 'in', 'military', 'not']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:48:04: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(16 unique tokens: ['a', 'are', 'be', 'bit', 'death']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:48:04: Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(16 unique tokens: ['a', 'are', 'be', 'bit', 'death']...) from 2 documents (total 32 corpus positions)\n",
      "INFO - 18:48:04: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(23 unique tokens: ['about', 'alone', 'and', 'be', 'calm']...) from 2 documents (total 51 corpus positions)\n",
      "INFO - 18:48:04: Removed 4 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:04: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:04: built Dictionary(23 unique tokens: ['about', 'alone', 'and', 'be', 'calm']...) from 2 documents (total 51 corpus positions)\n",
      "INFO - 18:48:05: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:05: built Dictionary(12 unique tokens: ['about', 'amongst', 'bomb', 'lives', 'millions']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:05: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:05: built Dictionary(12 unique tokens: ['about', 'amongst', 'bomb', 'lives', 'millions']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:05: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:05: built Dictionary(7 unique tokens: ['helping', 'in', 'the', 'wild', 'with']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:48:05: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:05: built Dictionary(7 unique tokens: ['helping', 'in', 'the', 'wild', 'with']...) from 2 documents (total 12 corpus positions)\n",
      "INFO - 18:48:05: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:05: built Dictionary(5 unique tokens: ['from', 'lady', 'the', 'has', 'that']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:48:05: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:05: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:05: built Dictionary(5 unique tokens: ['from', 'lady', 'the', 'is', 'that']) from 2 documents (total 8 corpus positions)\n",
      "INFO - 18:48:06: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:06: built Dictionary(4 unique tokens: ['getting', 'here', 'has', 'he']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:48:06: Removed 8 and 7 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:06: built Dictionary(4 unique tokens: ['getting', 'here', 'he', 'is']) from 2 documents (total 6 corpus positions)\n",
      "INFO - 18:48:06: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:06: adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:48:06: built Dictionary(21 unique tokens: ['a', 'at', 'could', 'go', 'has']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:48:06: Removed 6 and 5 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:06: built Dictionary(21 unique tokens: ['a', 'at', 'could', 'go', 'has']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:48:06: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:06: built Dictionary(12 unique tokens: ['and', 'either', 'helping', 'my', 'surgical']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:06: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:06: built Dictionary(12 unique tokens: ['and', 'either', 'helping', 'my', 'surgical']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:06: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:06: built Dictionary(12 unique tokens: ['and', 'either', 'helping', 'my', 'surgical']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:06: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:06: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:06: built Dictionary(12 unique tokens: ['and', 'either', 'helping', 'my', 'surgical']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:07: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(12 unique tokens: ['and', 'either', 'helping', 'my', 'surgical']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:07: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(15 unique tokens: ['based', 'broken', 'court', 'is', 'its']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:48:07: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(14 unique tokens: ['based', 'broken', 'court', 'is', 'its']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:48:07: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(17 unique tokens: ['a', 'do', 'frozen', 'i', 'it']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:48:07: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(17 unique tokens: ['a', 'do', 'frozen', 'i', 'it']...) from 2 documents (total 36 corpus positions)\n",
      "INFO - 18:48:07: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(12 unique tokens: ['&', 'again.', 'an', 'and', 'hour']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:07: Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(12 unique tokens: ['&', 'again.', 'an', 'and', 'hour']...) from 2 documents (total 22 corpus positions)\n",
      "INFO - 18:48:07: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(23 unique tokens: ['=', 'a', 'although', 'and', 'car']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:48:07: Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:07: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:07: built Dictionary(22 unique tokens: ['=', 'a', 'although', 'and', 'car']...) from 2 documents (total 44 corpus positions)\n",
      "INFO - 18:48:08: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:08: built Dictionary(13 unique tokens: ['felt', 'got', 'in', 'life.', 'low']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:48:08: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:08: built Dictionary(13 unique tokens: ['felt', 'got', 'in', 'life.', 'low']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:48:08: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:08: built Dictionary(12 unique tokens: ['be', 'been', 'beyond', 'hope', 'it']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:48:08: Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:08: built Dictionary(12 unique tokens: ['be', 'been', 'beyond', 'hope', 'it']...) from 2 documents (total 24 corpus positions)\n",
      "INFO - 18:48:08: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:08: built Dictionary(13 unique tokens: ['be', 'been', 'beyond', 'hope', 'is']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:48:08: Removed 4 and 3 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:08: built Dictionary(12 unique tokens: ['be', 'been', 'beyond', 'hope', 'is']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:48:08: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:08: built Dictionary(15 unique tokens: ['a', 'cake', 'can', 'goner', 'her']...) from 2 documents (total 28 corpus positions)\n",
      "INFO - 18:48:08: Removed 7 and 6 OOV words from document 1 and 2 (respectively).\n",
      "INFO - 18:48:08: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 18:48:08: built Dictionary(15 unique tokens: ['a', 'cake', 'can', 'goner', 'her']...) from 2 documents (total 28 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "for line in df_text.values:\n",
    "    #print(line[ ])\n",
    "    words_list.append(text_preprocessing(line[0]))\n",
    "    #print(clean_text)\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deed', 'reason', 'earthquake', 'allah', 'forgive'],\n",
       " ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada'],\n",
       " ['resident',\n",
       "  'ask',\n",
       "  'shelter',\n",
       "  'place',\n",
       "  'notify',\n",
       "  'officer',\n",
       "  'no',\n",
       "  'evacuation',\n",
       "  'shelter',\n",
       "  'place',\n",
       "  'order',\n",
       "  'expect'],\n",
       " ['people', 'receive', 'wildfire', 'evacuation', 'order', 'california'],\n",
       " ['get',\n",
       "  'send',\n",
       "  'photo',\n",
       "  'ruby',\n",
       "  'alaska',\n",
       "  'smoke',\n",
       "  'wildfire',\n",
       "  'pour',\n",
       "  'school'],\n",
       " ['rockyfire',\n",
       "  'update',\n",
       "  'california',\n",
       "  'hwy',\n",
       "  'close',\n",
       "  'direction',\n",
       "  'lake',\n",
       "  'county',\n",
       "  'fire',\n",
       "  'cafire',\n",
       "  'wildfire'],\n",
       " ['flood',\n",
       "  'disaster',\n",
       "  'heavy',\n",
       "  'rain',\n",
       "  'cause',\n",
       "  'flash',\n",
       "  'flooding',\n",
       "  'street',\n",
       "  'manitou',\n",
       "  'colorado',\n",
       "  'spring',\n",
       "  'area'],\n",
       " ['hill', 'fire', 'wood'],\n",
       " ['emergency', 'evacuation', 'happen', 'building', 'street'],\n",
       " ['afraid', 'tornado', 'come', 'area'],\n",
       " ['people', 'die', 'heat', 'wave', 'far'],\n",
       " ['haha',\n",
       "  'south',\n",
       "  'tampa',\n",
       "  'get',\n",
       "  'flood',\n",
       "  'hah',\n",
       "  'wait',\n",
       "  'second',\n",
       "  'live',\n",
       "  'south',\n",
       "  'tampa',\n",
       "  'go',\n",
       "  'go',\n",
       "  'fvck',\n",
       "  'flooding'],\n",
       " ['rain', 'flood', 'florida', 'tampabay', 'tampa', 'day', 'lose', 'count'],\n",
       " ['flood', 'bago', 'myanmar', 'arrive', 'bago'],\n",
       " ['damage', 'school', 'bus', 'multi', 'car', 'crash', 'breaking'],\n",
       " ['man'],\n",
       " ['love', 'fruit'],\n",
       " ['summer', 'lovely'],\n",
       " ['car', 'fast'],\n",
       " ['goooooooaaaaaal'],\n",
       " ['ridiculous'],\n",
       " ['london', 'cool'],\n",
       " ['love', 'skiing'],\n",
       " ['wonderful', 'day'],\n",
       " ['looooool'],\n",
       " ['no', 'wayi', 'not', 'eat', 'shit'],\n",
       " ['nyc', 'week'],\n",
       " ['love', 'girlfriend'],\n",
       " ['cooool'],\n",
       " ['like', 'pasta'],\n",
       " ['end'],\n",
       " ['wholesale', 'market', 'ablaze'],\n",
       " ['try', 'bring', 'heavy', 'metal', 'rt'],\n",
       " ['africanbaze', 'break', 'newsnigeria', 'flag', 'set', 'ablaze', 'aba'],\n",
       " ['cry', 'set', 'ablaze'],\n",
       " ['plus', 'look', 'sky', 'night', 'ablaze'],\n",
       " ['mufc',\n",
       "  'build',\n",
       "  'hype',\n",
       "  'new',\n",
       "  'acquisition',\n",
       "  'doubt',\n",
       "  'set',\n",
       "  'epl',\n",
       "  'ablaze',\n",
       "  'season'],\n",
       " ['inec', 'office', 'abia', 'set', 'ablaze'],\n",
       " ['barbados',\n",
       "  'bridgetown',\n",
       "  'jamaica',\n",
       "  'uo',\n",
       "  'car',\n",
       "  'set',\n",
       "  'ablaze',\n",
       "  'santa',\n",
       "  'cruz',\n",
       "  'uo',\n",
       "  'head',\n",
       "  'st',\n",
       "  'elizabeth',\n",
       "  'police',\n",
       "  'superintende'],\n",
       " ['ablaze', 'lord'],\n",
       " ['check', 'nsfw'],\n",
       " ['outside', 'ablaze', 'alive', 'dead', 'inside'],\n",
       " ['awesome',\n",
       "  'time',\n",
       "  'visit',\n",
       "  'cfc',\n",
       "  'head',\n",
       "  'office',\n",
       "  'ancop',\n",
       "  'site',\n",
       "  'ablaze',\n",
       "  'thank',\n",
       "  'tita',\n",
       "  'vida',\n",
       "  'take',\n",
       "  'care'],\n",
       " ['soooo', 'pumped', 'ablaze'],\n",
       " ['want', 'set', 'chicago', 'ablaze', 'preaching', 'not', 'hotel'],\n",
       " ['gain', 'follower', 'week', 'know', 'stat', 'grow'],\n",
       " ['west', 'burn', 'thousand', 'wildfire', 'ablaze', 'california'],\n",
       " ['build', 'perfect', 'tracklist', 'life', 'leave', 'street', 'ablaze'],\n",
       " ['check', 'nsfw'],\n",
       " ['night', 'retainer', 'weird', 'well', 'wear', 'single', 'night', 'year'],\n",
       " ['deputy', 'man', 'shot', 'brighton', 'home', 'set', 'ablaze'],\n",
       " ['man', 'wife', 'year', 'jail', 'set', 'ablaze', 'niece'],\n",
       " ['santa',\n",
       "  'cruz',\n",
       "  'uo',\n",
       "  'head',\n",
       "  'st',\n",
       "  'elizabeth',\n",
       "  'police',\n",
       "  'superintendent',\n",
       "  'lanford',\n",
       "  'salmon',\n",
       "  'r'],\n",
       " ['police',\n",
       "  'arsonist',\n",
       "  'deliberately',\n",
       "  'set',\n",
       "  'black',\n",
       "  'church',\n",
       "  'north',\n",
       "  'carolinaaeablaze'],\n",
       " ['noches',\n",
       "  'elbestia',\n",
       "  'happy',\n",
       "  'teammate',\n",
       "  'train',\n",
       "  'hard',\n",
       "  'goodnight',\n",
       "  'gunner'],\n",
       " ['kurds',\n",
       "  'trample',\n",
       "  'turkmen',\n",
       "  'flag',\n",
       "  'later',\n",
       "  'set',\n",
       "  'ablaze',\n",
       "  'vandalize',\n",
       "  'office',\n",
       "  'turkmen',\n",
       "  'diyala'],\n",
       " ['truck',\n",
       "  'ablaze',\n",
       "  'r21',\n",
       "  'voortrekker',\n",
       "  'ave',\n",
       "  'outside',\n",
       "  'tambo',\n",
       "  'intl',\n",
       "  'cargo',\n",
       "  'section'],\n",
       " ['set', 'heart', 'ablaze', 'city', 'gift', 'skyline', 'like', 'kiss', 'lip'],\n",
       " ['sky',\n",
       "  'ablaze',\n",
       "  'tonight',\n",
       "  'los',\n",
       "  'angeles',\n",
       "  'expect',\n",
       "  'ig',\n",
       "  'fb',\n",
       "  'fill',\n",
       "  'sunset',\n",
       "  'shot',\n",
       "  'know',\n",
       "  'peep'],\n",
       " ['west',\n",
       "  'burn',\n",
       "  'thousand',\n",
       "  'wildfire',\n",
       "  'ablaze',\n",
       "  'california',\n",
       "  'climate',\n",
       "  'energy'],\n",
       " ['revel',\n",
       "  'wmv',\n",
       "  'video',\n",
       "  'mean',\n",
       "  'mac',\n",
       "  'farewell',\n",
       "  'ablaze',\n",
       "  'wmv',\n",
       "  'en',\n",
       "  'route',\n",
       "  'dvd',\n",
       "  'gtxrwm'],\n",
       " ['progressive',\n",
       "  'greeting',\n",
       "  'month',\n",
       "  'student',\n",
       "  'set',\n",
       "  'pen',\n",
       "  'ablaze',\n",
       "  'torch',\n",
       "  'publication'],\n",
       " ['rene',\n",
       "  'ablaze',\n",
       "  'jacinta',\n",
       "  'secret',\n",
       "  '2k13',\n",
       "  'fallen',\n",
       "  'skies',\n",
       "  'edit',\n",
       "  'mar'],\n",
       " ['steve',\n",
       "  'fire',\n",
       "  'california',\n",
       "  'tinderbox',\n",
       "  'clown',\n",
       "  'set',\n",
       "  'hood',\n",
       "  'ablaze'],\n",
       " ['nowplaying', 'rene', 'ablaze', 'ian', 'buff', 'magnitude', 'edm'],\n",
       " ['huge', 'fire', 'wholesale', 'market', 'ablaze'],\n",
       " ['time', 'talk', 'not', 'know', 'work'],\n",
       " ['not',\n",
       "  'kid',\n",
       "  'cuz',\n",
       "  'get',\n",
       "  'bicycle',\n",
       "  'accident',\n",
       "  'split',\n",
       "  'testicle',\n",
       "  'impossible',\n",
       "  'kid',\n",
       "  'michael',\n",
       "  'father'],\n",
       " ['accident',\n",
       "  'i24',\n",
       "  'w',\n",
       "  'nashvilletraffic',\n",
       "  'traffic',\n",
       "  'move',\n",
       "  'm',\n",
       "  'slow',\n",
       "  'usual'],\n",
       " ['accident',\n",
       "  'center',\n",
       "  'lane',\n",
       "  'block',\n",
       "  'santaclara',\n",
       "  'us101',\n",
       "  'nb',\n",
       "  'great',\n",
       "  'america',\n",
       "  'pkwy',\n",
       "  'bayarea',\n",
       "  'traffic'],\n",
       " ['personalinjury',\n",
       "  'accident',\n",
       "  'summer',\n",
       "  'read',\n",
       "  'advice',\n",
       "  'solicitor',\n",
       "  'help',\n",
       "  'otleyhour'],\n",
       " ['stlouis',\n",
       "  'caraccidentlawyer',\n",
       "  'speed',\n",
       "  'cause',\n",
       "  'teen',\n",
       "  'accident',\n",
       "  'car',\n",
       "  'accident',\n",
       "  'teeu'],\n",
       " ['report',\n",
       "  'motor',\n",
       "  'vehicle',\n",
       "  'accident',\n",
       "  'curry',\n",
       "  'herman',\n",
       "  'rd',\n",
       "  'near',\n",
       "  'stephenson',\n",
       "  'involve',\n",
       "  'overturn',\n",
       "  'vehicle',\n",
       "  'use'],\n",
       " ['bigrigradio', 'live', 'accident', 'awareness'],\n",
       " ['i77',\n",
       "  'mile',\n",
       "  'marker',\n",
       "  'south',\n",
       "  'mooresville',\n",
       "  'iredell',\n",
       "  'vehicle',\n",
       "  'accident',\n",
       "  'ramp',\n",
       "  'close',\n",
       "  'pm'],\n",
       " ['rt', 'sleep', 'pill', 'double', 'risk', 'car', 'accident'],\n",
       " ['accident', 'know', 'gon', 'happen'],\n",
       " ['traffic', 'accident', 'n', 'cabrillo', 'hwymagellan', 'av', 'mir'],\n",
       " ['i77',\n",
       "  'mile',\n",
       "  'marker',\n",
       "  'south',\n",
       "  'mooresville',\n",
       "  'iredell',\n",
       "  'vehicle',\n",
       "  'accident',\n",
       "  'congestion',\n",
       "  'pm'],\n",
       " ['pastor', 'not', 'scene', 'accidentwho', 'owner', 'range', 'rover'],\n",
       " ['mom',\n",
       "  'not',\n",
       "  'home',\n",
       "  'fast',\n",
       "  'wish',\n",
       "  'mom',\n",
       "  'accident',\n",
       "  'truck',\n",
       "  'spill',\n",
       "  'mayonnaise'],\n",
       " ['horrible',\n",
       "  'car',\n",
       "  'accident',\n",
       "  'past',\n",
       "  'sunday',\n",
       "  'finally',\n",
       "  'able',\n",
       "  'thank',\n",
       "  'god'],\n",
       " ['wait', 'pissed', 'donnie', 'tell', 'accident'],\n",
       " ['truckcrash', 'overturn', 'fortworth', 'interstate', 'click', 'crash'],\n",
       " ['accident', 'ashville', 'sb', 'sr', 'traffic'],\n",
       " ['carolina',\n",
       "  'accident',\n",
       "  'motorcyclist',\n",
       "  'die',\n",
       "  'i540',\n",
       "  'crash',\n",
       "  'car',\n",
       "  'cross',\n",
       "  'median',\n",
       "  'motorcycle',\n",
       "  'rider',\n",
       "  'travel'],\n",
       " ['fyi',\n",
       "  'cadfyi',\n",
       "  'accident',\n",
       "  'property',\n",
       "  'damagenhs999',\n",
       "  'piner',\n",
       "  'rdhorndale',\n",
       "  'dr'],\n",
       " ['rt',\n",
       "  'naayf',\n",
       "  'accident',\n",
       "  'year',\n",
       "  'turn',\n",
       "  'chandanee',\n",
       "  'magu',\n",
       "  'near',\n",
       "  'mma',\n",
       "  'taxi',\n",
       "  'ram',\n",
       "  'halfway',\n",
       "  'turn',\n",
       "  'confu'],\n",
       " ['accident',\n",
       "  'leave',\n",
       "  'lane',\n",
       "  'block',\n",
       "  'manchester',\n",
       "  'rt',\n",
       "  'nb',\n",
       "  'eddy',\n",
       "  'rd',\n",
       "  'stop',\n",
       "  'traffic',\n",
       "  'nh3a',\n",
       "  'delay',\n",
       "  'min',\n",
       "  'traffic'],\n",
       " ['accident', 'property', 'damage', 'piner', 'rdhorndale', 'dr'],\n",
       " ['accident'],\n",
       " ['fyi', 'cadfyi', 'accident', 'property', 'damagewpd1600', 's', '17th', 'st'],\n",
       " ['pm', 'traffic', 'accident', 'no', 'injury', 'willis', 'foreman', 'rd'],\n",
       " ['aashiqui', 'actress', 'anu', 'aggarwal', 'nearfatal', 'accident'],\n",
       " ['suffield', 'alberta', 'accident'],\n",
       " ['mile',\n",
       "  'backup',\n",
       "  'i77',\n",
       "  'southaccident',\n",
       "  'block',\n",
       "  'right',\n",
       "  'lane',\n",
       "  'exit',\n",
       "  'langtree',\n",
       "  'rdconsider',\n",
       "  'nc',\n",
       "  'nc',\n",
       "  'nc',\n",
       "  'alternate'],\n",
       " ['accident',\n",
       "  'change',\n",
       "  'life',\n",
       "  'help',\n",
       "  'determine',\n",
       "  'option',\n",
       "  'financially',\n",
       "  'support',\n",
       "  'life',\n",
       "  'care',\n",
       "  'plan',\n",
       "  'ongoing',\n",
       "  'treatment'],\n",
       " ['break',\n",
       "  'deadly',\n",
       "  'motorcycle',\n",
       "  'car',\n",
       "  'accident',\n",
       "  'happen',\n",
       "  'hagerstown',\n",
       "  'today',\n",
       "  'detail',\n",
       "  'whag'],\n",
       " ['marinade', 'accident'],\n",
       " ['car',\n",
       "  'not',\n",
       "  'week',\n",
       "  'get',\n",
       "  'fucking',\n",
       "  'car',\n",
       "  'accident',\n",
       "  'mfs',\n",
       "  'not',\n",
       "  'fuck',\n",
       "  'drive'],\n",
       " ['bahrain',\n",
       "  'police',\n",
       "  'previously',\n",
       "  'die',\n",
       "  'road',\n",
       "  'accident',\n",
       "  'not',\n",
       "  'kill',\n",
       "  'explosion'],\n",
       " ['not',\n",
       "  'hear',\n",
       "  'church',\n",
       "  'leader',\n",
       "  'kenya',\n",
       "  'come',\n",
       "  'forward',\n",
       "  'comment',\n",
       "  'accident',\n",
       "  'issue',\n",
       "  'disciplinary',\n",
       "  'measuresarrestpastornganga'],\n",
       " ['scuf', 'ps', 'live', 'game', 'cya'],\n",
       " ['man',\n",
       "  'drive',\n",
       "  'effort',\n",
       "  'get',\n",
       "  'painful',\n",
       "  'man',\n",
       "  'win',\n",
       "  'roger',\n",
       "  'bannister'],\n",
       " ['ir',\n",
       "  'icemoon',\n",
       "  'aftershock',\n",
       "  'dubstep',\n",
       "  'trapmusic',\n",
       "  'dnb',\n",
       "  'edm',\n",
       "  'dance',\n",
       "  'icesu'],\n",
       " ['no',\n",
       "  'victory',\n",
       "  'bargain',\n",
       "  'basement',\n",
       "  'price',\n",
       "  'dwight',\n",
       "  'david',\n",
       "  'eisenhower'],\n",
       " ['ir',\n",
       "  'icemoon',\n",
       "  'aftershock',\n",
       "  'dubstep',\n",
       "  'trapmusic',\n",
       "  'dnb',\n",
       "  'edm',\n",
       "  'dance',\n",
       "  'icesu'],\n",
       " ['remember', 'come', 'second', 'charles', 'schulz'],\n",
       " ['speak', 'scuf', 'xb1', 'people', 'end', 'get', 'ps'],\n",
       " ['hard', 'conflict', 'glorious', 'triumph', 'thomas', 'paine'],\n",
       " ['growingupspoiled', 'go', 'clay', 'pigeon', 'shoot', 'cry', 'aftershock'],\n",
       " ['guess', 'no', 'actually', 'want', 'free', 'aftershock', 'tc'],\n",
       " ['aftershock', 'terrifying', 'good', 'roller', 'coaster', 'disclaimer'],\n",
       " ['aftershock'],\n",
       " ['ir',\n",
       "  'icemoon',\n",
       "  'aftershock',\n",
       "  'dubstep',\n",
       "  'trapmusic',\n",
       "  'dnb',\n",
       "  'edm',\n",
       "  'dance',\n",
       "  'icesu'],\n",
       " ['ir',\n",
       "  'icemoon',\n",
       "  'aftershock',\n",
       "  'dubstep',\n",
       "  'trapmusic',\n",
       "  'dnb',\n",
       "  'edm',\n",
       "  'dance',\n",
       "  'icesu'],\n",
       " ['ir',\n",
       "  'icemoon',\n",
       "  'aftershock',\n",
       "  'dubstep',\n",
       "  'trapmusic',\n",
       "  'dnb',\n",
       "  'edm',\n",
       "  'dance',\n",
       "  'icesu'],\n",
       " ['see', 'issue', 'aftershock'],\n",
       " ['ir',\n",
       "  'icemoon',\n",
       "  'aftershock',\n",
       "  'dubstep',\n",
       "  'trapmusic',\n",
       "  'dnb',\n",
       "  'edm',\n",
       "  'dance',\n",
       "  'icesu'],\n",
       " ['ir',\n",
       "  'icemoon',\n",
       "  'aftershock',\n",
       "  'dubstep',\n",
       "  'trapmusic',\n",
       "  'dnb',\n",
       "  'edm',\n",
       "  'dance',\n",
       "  'icesu'],\n",
       " ['wisdomwed',\n",
       "  'bonus',\n",
       "  'minute',\n",
       "  'daily',\n",
       "  'habit',\n",
       "  'improve',\n",
       "  'life',\n",
       "  'lifehack'],\n",
       " ['aftershock',\n",
       "  'protect',\n",
       "  'profit',\n",
       "  'global',\n",
       "  'financial',\n",
       "  'meltdown',\n",
       "  'david',\n",
       "  'wiedemer',\n",
       "  'http'],\n",
       " ['moment',\n",
       "  'scary',\n",
       "  'roller',\n",
       "  'coaster',\n",
       "  'guy',\n",
       "  'scream',\n",
       "  'bloody',\n",
       "  'murder',\n",
       "  'silverwood',\n",
       "  'aftershock'],\n",
       " ['aftershock', 'ac', 'fullac', 'streaming', 'youtube'],\n",
       " ['aftershock', 'protect', 'profit', 'global', 'financial', 'book'],\n",
       " ['face', 'difficulty', 'not', 'wrong', 'right', 'joel', 'osteen'],\n",
       " ['thing',\n",
       "  'stand',\n",
       "  'dream',\n",
       "  'try',\n",
       "  'belief',\n",
       "  'actually',\n",
       "  'possible',\n",
       "  'joel',\n",
       "  'brown'],\n",
       " ['praise', 'god', 'ministry', 'tell', 'like', 'wdyouth', 'biblestudy'],\n",
       " ['remember',\n",
       "  'go',\n",
       "  'die',\n",
       "  'good',\n",
       "  'way',\n",
       "  'know',\n",
       "  'avoid',\n",
       "  'trap',\n",
       "  'think',\n",
       "  'lose',\n",
       "  'uo',\n",
       "  'steve',\n",
       "  'job'],\n",
       " ['try', 'orange', 'aftershock', 'today', 'life'],\n",
       " ['love', 'bb'],\n",
       " ['aftershock'],\n",
       " ['aftershock',\n",
       "  'school',\n",
       "  'kick',\n",
       "  'great',\n",
       "  'want',\n",
       "  'thank',\n",
       "  'make',\n",
       "  'possible',\n",
       "  'great',\n",
       "  'night'],\n",
       " ['people', 'not', 'not', 'interrupt', 'uo', 'george', 'bernard', 'shaw'],\n",
       " ['man',\n",
       "  'get',\n",
       "  'oyster',\n",
       "  'second',\n",
       "  'man',\n",
       "  'get',\n",
       "  'shell',\n",
       "  'andrew',\n",
       "  'carnegie'],\n",
       " ['need', 'pyou', 'tonight', 'play', 'hybrid', 'slayer', 'ps4', 'eu', 'hmu'],\n",
       " ['expert',\n",
       "  'france',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  'airplane',\n",
       "  'debris',\n",
       "  'find',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accident',\n",
       "  'expert',\n",
       "  'o',\n",
       "  'news'],\n",
       " ['strict',\n",
       "  'liability',\n",
       "  'context',\n",
       "  'airplane',\n",
       "  'accident',\n",
       "  'pilot',\n",
       "  'error',\n",
       "  'common',\n",
       "  'component',\n",
       "  'aviation',\n",
       "  'cr'],\n",
       " ['lifetime', 'odd', 'die', 'airplane', 'accident'],\n",
       " ['expert',\n",
       "  'france',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  'airplane',\n",
       "  'debris',\n",
       "  'find',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accident',\n",
       "  'expert',\n",
       "  'wedn'],\n",
       " ['awwww', 'airplane', 'accident', 'go', 'die', 'cutie', 'good', 'job'],\n",
       " ['family',\n",
       "  'member',\n",
       "  'osama',\n",
       "  'bin',\n",
       "  'laden',\n",
       "  'die',\n",
       "  'airplane',\n",
       "  'accident',\n",
       "  'ironic',\n",
       "  'mhmmm',\n",
       "  'gov',\n",
       "  'shit',\n",
       "  'suspect'],\n",
       " ['man', 'go', 'airplane', 'engine', 'accident'],\n",
       " ['horrible', 'accident', 'man', 'die', 'wing', 'airplane'],\n",
       " ['cessna',\n",
       "  'airplane',\n",
       "  'accident',\n",
       "  'ocampo',\n",
       "  'coahuila',\n",
       "  'mexico',\n",
       "  'july',\n",
       "  'kill',\n",
       "  'man',\n",
       "  'include',\n",
       "  'state',\n",
       "  'coahuila',\n",
       "  'government',\n",
       "  'official'],\n",
       " ['horrible', 'accident', 'man', 'die', 'wing', 'airplane', 'watchthevideo'],\n",
       " ['expert',\n",
       "  'france',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  'airplane',\n",
       "  'debris',\n",
       "  'find',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accident',\n",
       "  'expert',\n",
       "  'wednesdayu'],\n",
       " ['expert',\n",
       "  'france',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  'airplane',\n",
       "  'debris',\n",
       "  'find',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accident',\n",
       "  'expert',\n",
       "  'wednesday',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  't'],\n",
       " ['kca',\n",
       "  'votejkt48id',\n",
       "  'mbataweel',\n",
       "  'rip',\n",
       "  'binladen',\n",
       "  'family',\n",
       "  'member',\n",
       "  'kill',\n",
       "  'airplane',\n",
       "  'accident'],\n",
       " ['send', 'coworker', 'nude', 'accident', 'thank', 'god', 'airplane', 'mode'],\n",
       " ['kill', 'airplane', 'accident', 'night', 'car', 'wreck', 'politic', 'good'],\n",
       " ['expert',\n",
       "  'france',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  'airplane',\n",
       "  'debris',\n",
       "  'find',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accident',\n",
       "  'expert',\n",
       "  'mlb'],\n",
       " ['unbelievably',\n",
       "  'insane',\n",
       "  'man',\n",
       "  'airport',\n",
       "  'airplane',\n",
       "  'aircraft',\n",
       "  'aeroplane',\n",
       "  'runway',\n",
       "  'accident',\n",
       "  'freakyu'],\n",
       " ['horrible', 'accident', 'man', 'die', 'wing', 'airplaneae29072015'],\n",
       " ['horrible', 'accident', 'man', 'die', 'wing', 'airplane'],\n",
       " ['usama',\n",
       "  'bin',\n",
       "  'ladins',\n",
       "  'family',\n",
       "  'dead',\n",
       "  'airplane',\n",
       "  'crash',\n",
       "  'naturally',\n",
       "  'no',\n",
       "  'accident'],\n",
       " ['pilot',\n",
       "  'die',\n",
       "  'plane',\n",
       "  'crash',\n",
       "  'car',\n",
       "  'festival',\n",
       "  'crash',\n",
       "  'aircraft',\n",
       "  'airplane',\n",
       "  'pilot',\n",
       "  'death',\n",
       "  'accident',\n",
       "  'carfest'],\n",
       " ['strict', 'liability', 'context', 'airplane', 'accident'],\n",
       " ['dtn',\n",
       "  'brazil',\n",
       "  'expert',\n",
       "  'france',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  'airplane',\n",
       "  'debris',\n",
       "  'find',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accident',\n",
       "  'exp'],\n",
       " ['expert',\n",
       "  'france',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  'airplane',\n",
       "  'debris',\n",
       "  'find',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accident',\n",
       "  'expert',\n",
       "  'wedn'],\n",
       " ['horrible',\n",
       "  'accident',\n",
       "  'man',\n",
       "  'die',\n",
       "  'wing',\n",
       "  'uiairplaneu',\n",
       "  'wtf',\n",
       "  'canuat',\n",
       "  'believe',\n",
       "  'eye',\n",
       "  'uo'],\n",
       " ['nicole',\n",
       "  'fletcher',\n",
       "  'victim',\n",
       "  'crashed',\n",
       "  'airplane',\n",
       "  'time',\n",
       "  'ago',\n",
       "  'accident',\n",
       "  'leave',\n",
       "  'little',\n",
       "  'bit',\n",
       "  'trauma'],\n",
       " ['omg', 'horrible', 'accident', 'man', 'die', 'wing', 'airplane'],\n",
       " ['omg',\n",
       "  'not',\n",
       "  'believe',\n",
       "  'rip',\n",
       "  'bro',\n",
       "  'airplane',\n",
       "  'accident',\n",
       "  'jetengine',\n",
       "  'turbojet',\n",
       "  'boing',\n",
       "  'g90'],\n",
       " ['expert',\n",
       "  'france',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  'airplane',\n",
       "  'debris',\n",
       "  'find',\n",
       "  'reunion',\n",
       "  'island',\n",
       "  'french',\n",
       "  'air',\n",
       "  'accident',\n",
       "  'expert',\n",
       "  'wednesday',\n",
       "  'begin',\n",
       "  'examine',\n",
       "  't'],\n",
       " ['airplane', 'accident'],\n",
       " ['phone', 'look', 'like', 'car', 'ship', 'airplane', 'accident', 'terrible'],\n",
       " ['statistically',\n",
       "  'risk',\n",
       "  'get',\n",
       "  'kill',\n",
       "  'cop',\n",
       "  'die',\n",
       "  'airplane',\n",
       "  'accident'],\n",
       " ['airplane', 'crash', 'house', 'colombia', 'people', 'die', 'accident'],\n",
       " ['shooting', 'airplane', 'accident'],\n",
       " ['drone',\n",
       "  'airplane',\n",
       "  'accident',\n",
       "  'pilot',\n",
       "  'worry',\n",
       "  'use',\n",
       "  'drone',\n",
       "  'esp',\n",
       "  'close',\n",
       "  'vicinity',\n",
       "  'airport'],\n",
       " ['early',\n",
       "  'wake',\n",
       "  'sister',\n",
       "  'beg',\n",
       "  'come',\n",
       "  'ride',\n",
       "  'wher',\n",
       "  'ambulance',\n",
       "  'hospital',\n",
       "  'rodkiai'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['air', 'ambulance', 'scene', 'crash', 'car', 'lorry', 'emsneu'],\n",
       " ['fear',\n",
       "  'kill',\n",
       "  'pakistani',\n",
       "  'air',\n",
       "  'ambulance',\n",
       "  'helicopter',\n",
       "  'crash',\n",
       "  'reuters',\n",
       "  'yugvani'],\n",
       " ['lead',\n",
       "  'emergency',\n",
       "  'service',\n",
       "  'boss',\n",
       "  'welcome',\n",
       "  'new',\n",
       "  'ambulance',\n",
       "  'charity'],\n",
       " ['travel',\n",
       "  'aberystwythshrewsbury',\n",
       "  'right',\n",
       "  'incident',\n",
       "  'service',\n",
       "  'halt',\n",
       "  'outside',\n",
       "  'shrew',\n",
       "  'ambulance',\n",
       "  'scene'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['ambulance',\n",
       "  'sprinter',\n",
       "  'automatic',\n",
       "  'frontline',\n",
       "  'vehicle',\n",
       "  'choice',\n",
       "  'lez',\n",
       "  'compliant',\n",
       "  'ebay'],\n",
       " ['new', 'nanotech', 'device', 'able', 'target', 'destroy', 'blood', 'clot'],\n",
       " ['hella', 'crazy', 'fight', 'ambulance', 'couple', 'mosh', 'pit'],\n",
       " ['run', 'ambulance', 'lucky', 'justsaying', 'randomthought'],\n",
       " ['news',\n",
       "  'fear',\n",
       "  'kill',\n",
       "  'pakistani',\n",
       "  'air',\n",
       "  'ambulance',\n",
       "  'helicopter',\n",
       "  'crash',\n",
       "  'tilnow',\n",
       "  'dna'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['wait', 'ambulance'],\n",
       " ['ok', 'need', 'ambulance', 'hahahah', 'good'],\n",
       " ['ambulance',\n",
       "  'sprinter',\n",
       "  'automatic',\n",
       "  'frontline',\n",
       "  'vehicle',\n",
       "  'choice',\n",
       "  'lez',\n",
       "  'compliant',\n",
       "  'ebay'],\n",
       " ['pakistan', 'air', 'ambulance', 'helicopter', 'crash', 'kill'],\n",
       " ['nissan', 'ok', 'need', 'medical', 'assistance', 'ambulance', 'need'],\n",
       " ['ems1',\n",
       "  'ny',\n",
       "  'emts',\n",
       "  'petition',\n",
       "  'hour',\n",
       "  'youminimum',\n",
       "  'wageua',\n",
       "  'ems',\n",
       "  'paramedic',\n",
       "  'ambulance'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['ambulance',\n",
       "  'sprinter',\n",
       "  'automatic',\n",
       "  'frontline',\n",
       "  'vehicle',\n",
       "  'choice',\n",
       "  'lez',\n",
       "  'compliant',\n",
       "  'ebay'],\n",
       " ['ambulance',\n",
       "  'sprinter',\n",
       "  'automatic',\n",
       "  'frontline',\n",
       "  'vehicle',\n",
       "  'choice',\n",
       "  'lez',\n",
       "  'compliant',\n",
       "  'ebay'],\n",
       " ['check', 'parking', 'lot', 'say', 'year', 'ambulance', 'st', 'johns'],\n",
       " ['not', 'know', 'way', 'ambulance', 'come'],\n",
       " ['reuters',\n",
       "  'fear',\n",
       "  'kill',\n",
       "  'pakistani',\n",
       "  'air',\n",
       "  'ambulance',\n",
       "  'helicopter',\n",
       "  'crash'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['ambulance', 'right', 'outside', 'work'],\n",
       " ['ui', 'dog', 'think', 'ambulance'],\n",
       " ['happen',\n",
       "  'hatzolah',\n",
       "  'ems',\n",
       "  'ambulance',\n",
       "  'respond',\n",
       "  'dual',\n",
       "  'siren',\n",
       "  'andu'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['fear',\n",
       "  'kill',\n",
       "  'pakistani',\n",
       "  'air',\n",
       "  'ambulance',\n",
       "  'helicopter',\n",
       "  'crash',\n",
       "  'worldnew'],\n",
       " ['fear',\n",
       "  'kill',\n",
       "  'pakistani',\n",
       "  'air',\n",
       "  'ambulance',\n",
       "  'helicopter',\n",
       "  'crash',\n",
       "  'worldnew'],\n",
       " ['police', 'ambulance', 'number', 'lesotho', 'body', 'know'],\n",
       " ['surprised',\n",
       "  'not',\n",
       "  'standardise',\n",
       "  'clinical',\n",
       "  'practice',\n",
       "  'nhs',\n",
       "  'ambulance',\n",
       "  'trust'],\n",
       " ['fear', 'kill', 'pakistani', 'air', 'ambulance', 'helicopter', 'crash'],\n",
       " ['people', 'try', 'jwalk', 'ambulance', 'pass', 'hate'],\n",
       " ['episode',\n",
       "  'trunk',\n",
       "  'annihilate',\n",
       "  'freiza',\n",
       "  'clean',\n",
       "  'shit',\n",
       "  'show',\n",
       "  'nigga',\n",
       "  'no',\n",
       "  'mercy'],\n",
       " ['shall',\n",
       "  'annihilate',\n",
       "  'petebest',\n",
       "  'dessicate',\n",
       "  'lay',\n",
       "  'bare',\n",
       "  'shall',\n",
       "  'kneel'],\n",
       " ['uribe', 'annihilate', 'baseball', 'met'],\n",
       " ['heysundowns',\n",
       "  'annihilate',\n",
       "  'previous',\n",
       "  'meeting',\n",
       "  'celticindeed',\n",
       "  'improvement'],\n",
       " ['mizzou',\n",
       "  'annihilate',\n",
       "  'florida',\n",
       "  'past',\n",
       "  'season',\n",
       "  'end',\n",
       "  'muschamps',\n",
       "  'career',\n",
       "  'not',\n",
       "  'compete',\n",
       "  'bama'],\n",
       " ['annihilate', 'abs'],\n",
       " ['annihilate',\n",
       "  'status',\n",
       "  'education',\n",
       "  'mba',\n",
       "  'behalf',\n",
       "  'easy',\n",
       "  'street',\n",
       "  'careen',\n",
       "  'eovm'],\n",
       " ['luka', 'die', 'annihilate', 'alois', 'trancy'],\n",
       " ['good', 'un', 'fella', 'sorry', 'not', 'annihilate'],\n",
       " ['cop', 'pull', 'drunk', 'driver', 'safety', 'second', 'car', 'hit', 'train'],\n",
       " ['annihilate'],\n",
       " ['cop', 'pull', 'drunk', 'driver', 'safety', 'second', 'car', 'hit', 'train'],\n",
       " ['boom', 'country', 'entirely', 'annihilate', 'hu', 'uo', 'britain'],\n",
       " ['annihilate', 'thank'],\n",
       " ['thing',\n",
       "  'suregod',\n",
       "  'promise',\n",
       "  'israel',\n",
       "  'not',\n",
       "  'annihilate',\n",
       "  'butthe',\n",
       "  'horror',\n",
       "  'iran',\n",
       "  'wnuke'],\n",
       " ['guess', 'ok', 'armenians', 'spend', 'history', 'get', 'annihilate'],\n",
       " ['year',\n",
       "  'annihilate',\n",
       "  'people',\n",
       "  'instantly',\n",
       "  'aware',\n",
       "  'ability',\n",
       "  'annihilate',\n",
       "  'humanity'],\n",
       " ['day',\n",
       "  'tryout',\n",
       "  'go',\n",
       "  'good',\n",
       "  'minus',\n",
       "  'fact',\n",
       "  'stop',\n",
       "  'quickly',\n",
       "  'short',\n",
       "  'ball',\n",
       "  'annihilate',\n",
       "  'toenail',\n",
       "  'injury'],\n",
       " ['1960', 'oryx', 'symbol', 'arabian', 'peninsula', 'annihilate', 'hunter'],\n",
       " ['luka', 'die', 'annihilate', 'alois', 'trancy'],\n",
       " ['ready', 'annihilate', 'bucs', 'game'],\n",
       " ['people', 'annihilate', 'night', 'weather', 'philip', 'think', 'forecast'],\n",
       " ['domain',\n",
       "  'sophistication',\n",
       "  'annihilate',\n",
       "  'closely',\n",
       "  'uptotheminute',\n",
       "  'feat',\n",
       "  'zrnf'],\n",
       " ['see',\n",
       "  'judas',\n",
       "  'priest',\n",
       "  'rob',\n",
       "  'come',\n",
       "  'scorpion',\n",
       "  'support',\n",
       "  'fucking',\n",
       "  'annihilate',\n",
       "  'place',\n",
       "  'astonish',\n",
       "  'gig'],\n",
       " ['officially',\n",
       "  'skip',\n",
       "  'fantasticfourfant4sticwhatever',\n",
       "  'hashtag',\n",
       "  'get',\n",
       "  'annihilate',\n",
       "  'review',\n",
       "  'bummer'],\n",
       " ['explain', 'annihilate', 'case', 'survivor', 'evolve', 'godlike'],\n",
       " ['completely', 'annihilate', 'cech', 'paul', 'keegan', 'time', 'alive'],\n",
       " ['annihilate',\n",
       "  'legion',\n",
       "  'survivor',\n",
       "  'imperfect',\n",
       "  'hybrid',\n",
       "  'project',\n",
       "  'quickly',\n",
       "  'form',\n",
       "  'new',\n",
       "  'secret',\n",
       "  'cell'],\n",
       " ['exactly',\n",
       "  'lesnarcena',\n",
       "  'match',\n",
       "  'summerslam',\n",
       "  'year',\n",
       "  'great',\n",
       "  'brock',\n",
       "  'annihilate',\n",
       "  'guy'],\n",
       " ['cop', 'pull', 'drunk', 'driver', 'safety', 'second', 'car', 'hit', 'train'],\n",
       " ['annihilate',\n",
       "  'damascus',\n",
       "  'syrian',\n",
       "  'army',\n",
       "  'grind',\n",
       "  'youalloosh',\n",
       "  'gang',\n",
       "  'manure',\n",
       "  'pile'],\n",
       " ['ok',\n",
       "  'not',\n",
       "  'completely',\n",
       "  'forthright',\n",
       "  'food',\n",
       "  'coma',\n",
       "  'bc',\n",
       "  'kebabtahinipickle',\n",
       "  'annihilate',\n",
       "  'wfrie'],\n",
       " ['fun',\n",
       "  'fill',\n",
       "  'happyhour',\n",
       "  'simmons',\n",
       "  'bar',\n",
       "  'camden',\n",
       "  'handsome',\n",
       "  'get',\n",
       "  'annihilate',\n",
       "  'apart',\n",
       "  'game'],\n",
       " ['juanny', 'beisbol', 'sr', 'annihilate', 'ball', 'lgm'],\n",
       " ['hell', 'fraction', 'belief', 'total', 'annihilation', 'destruction', 'usa'],\n",
       " ['maybe', 'israel', 'tell', 'sorry', 'pres', 'sell', 'river', 'annihilation'],\n",
       " ['evildead', 'annihilation', 'civilization'],\n",
       " ['national',\n",
       "  'park',\n",
       "  'services',\n",
       "  'tonto',\n",
       "  'national',\n",
       "  'forest',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'wild',\n",
       "  'horse'],\n",
       " ['annihilate', 'quarterstaff', 'annihilation'],\n",
       " ['world',\n",
       "  'annihilation',\n",
       "  'vs',\n",
       "  'self',\n",
       "  'transformation',\n",
       "  'alien',\n",
       "  'attack',\n",
       "  'exterminate',\n",
       "  'human'],\n",
       " ['starmade', 'stardate', 'planetary', 'annihilation'],\n",
       " ['national',\n",
       "  'park',\n",
       "  'services',\n",
       "  'tonto',\n",
       "  'national',\n",
       "  'forest',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'wild',\n",
       "  'horse'],\n",
       " ['national',\n",
       "  'park',\n",
       "  'services',\n",
       "  'tonto',\n",
       "  'national',\n",
       "  'forest',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'wild',\n",
       "  'horse'],\n",
       " ['sign', 'share', 'petition', 'save', 'wild', 'horse', 'arizona'],\n",
       " ['national',\n",
       "  'park',\n",
       "  'services',\n",
       "  'tonto',\n",
       "  'national',\n",
       "  'forest',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'wild',\n",
       "  'horse'],\n",
       " ['check',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'horse',\n",
       "  'help',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'happen',\n",
       "  'signatureschange',\n",
       "  'org',\n",
       "  'thx'],\n",
       " ['soul', 'punish', 'withaeannihilation'],\n",
       " ['not', 'mention', 'major', 'contributor', 'annihilation', 'israel'],\n",
       " ['need',\n",
       "  'help',\n",
       "  'horse',\n",
       "  'dieplease',\n",
       "  'rt',\n",
       "  'sign',\n",
       "  'petitiontake',\n",
       "  'stand',\n",
       "  'voice'],\n",
       " ['reject',\n",
       "  'law',\n",
       "  'misguided',\n",
       "  'false',\n",
       "  'prophet',\n",
       "  'imprison',\n",
       "  'nation',\n",
       "  'fuel',\n",
       "  'self',\n",
       "  'annihilation'],\n",
       " ['national',\n",
       "  'park',\n",
       "  'services',\n",
       "  'tonto',\n",
       "  'national',\n",
       "  'forest',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'wild',\n",
       "  'horse'],\n",
       " ['annihilation',\n",
       "  'jeb',\n",
       "  'christie',\n",
       "  'kasich',\n",
       "  'hour',\n",
       "  'away',\n",
       "  'god',\n",
       "  'allow',\n",
       "  'day'],\n",
       " ['need', 'helphorses', 'die', 'rt', 'sign', 'petition', 'stand', 'voice'],\n",
       " ['need',\n",
       "  'help',\n",
       "  'horse',\n",
       "  'dieplease',\n",
       "  'rt',\n",
       "  'sign',\n",
       "  'petitiontake',\n",
       "  'stand',\n",
       "  'voice'],\n",
       " ['hey',\n",
       "  'az',\n",
       "  'sign',\n",
       "  'petition',\n",
       "  'save',\n",
       "  'wildhorses',\n",
       "  'tantonationalforest',\n",
       "  'singalong',\n",
       "  'order'],\n",
       " ['stop', 'annihilation', 'salt', 'river', 'wild', 'horse'],\n",
       " ['current',\n",
       "  'nova',\n",
       "  'bookslast',\n",
       "  'checkedhe',\n",
       "  'tie',\n",
       "  'book',\n",
       "  'rider',\n",
       "  'die',\n",
       "  'annihilation'],\n",
       " ['national',\n",
       "  'park',\n",
       "  'services',\n",
       "  'tonto',\n",
       "  'national',\n",
       "  'forest',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'wild',\n",
       "  'horse'],\n",
       " ['sign', 'rt', 'save', 'saltriverwildhorse'],\n",
       " ['thanks', 'dante', 'join', 'follow', 'zone', 'johnny'],\n",
       " ['world',\n",
       "  'annihilation',\n",
       "  'vs',\n",
       "  'self',\n",
       "  'transformation',\n",
       "  'alien',\n",
       "  'attack',\n",
       "  'exterminate',\n",
       "  'human'],\n",
       " ['national',\n",
       "  'park',\n",
       "  'services',\n",
       "  'tonto',\n",
       "  'national',\n",
       "  'forest',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'wild',\n",
       "  'horse'],\n",
       " ['national',\n",
       "  'park',\n",
       "  'services',\n",
       "  'tonto',\n",
       "  'national',\n",
       "  'forest',\n",
       "  'stop',\n",
       "  'annihilation',\n",
       "  'salt',\n",
       "  'river',\n",
       "  'wild',\n",
       "  'horse'],\n",
       " ['go', 'fight', 'taylor', 'soon'],\n",
       " ['ohh',\n",
       "  'no',\n",
       "  'fukurodani',\n",
       "  'not',\n",
       "  'survive',\n",
       "  'apocalypse',\n",
       "  'bokuto',\n",
       "  'feel',\n",
       "  'horrible',\n",
       "  'poor',\n",
       "  'boy',\n",
       "  'ppor',\n",
       "  'child'],\n",
       " ['jocelyn', 'birthday', 'apocalypse'],\n",
       " ['rt',\n",
       "  'janenelson097',\n",
       "  'rt',\n",
       "  'stephenscifi',\n",
       "  'adaptation',\n",
       "  'watch',\n",
       "  'human',\n",
       "  'apocalypse',\n",
       "  'optione',\n",
       "  'film',\n",
       "  'sciencefiction',\n",
       "  'u'],\n",
       " ['apocalypse'],\n",
       " ['hour',\n",
       "  'august',\n",
       "  '0802pm',\n",
       "  'heres',\n",
       "  'red',\n",
       "  'rover',\n",
       "  'zombie',\n",
       "  'apocalypse',\n",
       "  'internetradio',\n",
       "  'collegeradiu'],\n",
       " ['feel', 'like', 'pullup', 'stage', 'apocalypse'],\n",
       " ['kind',\n",
       "  'hot',\n",
       "  'play',\n",
       "  'radio',\n",
       "  'today',\n",
       "  'disease',\n",
       "  'apocalypse',\n",
       "  'start',\n",
       "  'careful'],\n",
       " ['apocalypse', 'lol', 'gf', 'm8'],\n",
       " ['know', 'question', 'interpretation', 'sign', 'apocalypse', 'call'],\n",
       " ['julie', 'r', 'apocalypse', 'version', 'romeo', 'juliet', 'warmbodie'],\n",
       " ['apocalypse'],\n",
       " ['rt',\n",
       "  'fittscott',\n",
       "  'minecraft',\n",
       "  'night',\n",
       "  'lucky',\n",
       "  'block',\n",
       "  'mod',\n",
       "  'bob',\n",
       "  'apocalypse',\n",
       "  'wither',\n",
       "  'mod',\n",
       "  'showcase',\n",
       "  'popularmmos',\n",
       "  'viu'],\n",
       " ['begin', 'day', 'snow', 'apocalypse'],\n",
       " ['rt',\n",
       "  'ourmothermary',\n",
       "  'short',\n",
       "  'reading',\n",
       "  'apocalypse',\n",
       "  'spirit',\n",
       "  'angel',\n",
       "  'take',\n",
       "  'enormous',\n",
       "  'high',\n",
       "  'mountain',\n",
       "  'u'],\n",
       " ['candylit',\n",
       "  'imagine',\n",
       "  'sarumi',\n",
       "  'zombie',\n",
       "  'apocalypse',\n",
       "  'fight',\n",
       "  'heart',\n",
       "  'heart',\n",
       "  'conversation'],\n",
       " ['rt',\n",
       "  'like',\n",
       "  'youtube',\n",
       "  'video',\n",
       "  'minecraft',\n",
       "  'night',\n",
       "  'lucky',\n",
       "  'block',\n",
       "  'mod',\n",
       "  'bob',\n",
       "  'apocalypse',\n",
       "  'wither',\n",
       "  'mou'],\n",
       " ['planet', 'lone', 'audience', 'apocalypse'],\n",
       " ['dad',\n",
       "  'buy',\n",
       "  'dvd',\n",
       "  'look',\n",
       "  'like',\n",
       "  'science',\n",
       "  'doc',\n",
       "  'read',\n",
       "  'actually',\n",
       "  'impending',\n",
       "  'biblical',\n",
       "  'apocalypse'],\n",
       " ['apocalypse', 'come', 'week', 'know'],\n",
       " ['look',\n",
       "  'grizzly',\n",
       "  'peak',\n",
       "  'right',\n",
       "  'look',\n",
       "  'like',\n",
       "  'beginning',\n",
       "  'dystopian',\n",
       "  'apocalypse',\n",
       "  'movie'],\n",
       " ['niece', 'ask', 'scare', 'apocalypse'],\n",
       " ['storm', 'cairo', 'late', 'xmen', 'apocalypse', 'set', 'photo'],\n",
       " ['minecraft',\n",
       "  'night',\n",
       "  'lucky',\n",
       "  'block',\n",
       "  'mod',\n",
       "  'bob',\n",
       "  'apocalypse',\n",
       "  'wither',\n",
       "  'mod',\n",
       "  'showcase',\n",
       "  'popularmmos'],\n",
       " ['shoot', 'heart', 'xv', 'go', 'totally', 'love', 'bad', 'heart', 'pierc'],\n",
       " ['rt',\n",
       "  'geekapocalypse',\n",
       "  'p.m.',\n",
       "  'gmt',\n",
       "  'hesse',\n",
       "  'play',\n",
       "  'dark',\n",
       "  'soul',\n",
       "  'day',\n",
       "  'etcpb'],\n",
       " ['know', 'zombie'],\n",
       " ['late', 'reveal', 'storm', 'queen', 'apocalypse'],\n",
       " ['shadow', 'box', 'apocalypse'],\n",
       " ['short',\n",
       "  'reading',\n",
       "  'apocalypse',\n",
       "  'spirit',\n",
       "  'angel',\n",
       "  'take',\n",
       "  'enormous',\n",
       "  'high',\n",
       "  'mountain'],\n",
       " ['enjoy',\n",
       "  'liveaction',\n",
       "  'attack',\n",
       "  'titan',\n",
       "  'time',\n",
       "  'poster',\n",
       "  'remind',\n",
       "  'freshly',\n",
       "  'clean',\n",
       "  'coiffed',\n",
       "  'apocalypse'],\n",
       " ['like',\n",
       "  'video',\n",
       "  'minecraft',\n",
       "  'night',\n",
       "  'lucky',\n",
       "  'block',\n",
       "  'mod',\n",
       "  'bob',\n",
       "  'apocalypse',\n",
       "  'wither',\n",
       "  'mod',\n",
       "  'showcase'],\n",
       " ['pbban',\n",
       "  'temporary300',\n",
       "  'avysss',\n",
       "  'not',\n",
       "  'kill',\n",
       "  'flag',\n",
       "  'fast',\n",
       "  'xp',\n",
       "  'reason'],\n",
       " ['pbban', 'temporary300', 'not', 'kill', 'flag', 'fast', 'xp', 'reason'],\n",
       " ['official', 'vid', 'doublecups', 'prod', 'armageddon'],\n",
       " ['ouvindo', 'peace', 'love', 'armageddon'],\n",
       " ['good', 'movie', 'see', 'armageddon'],\n",
       " ['bed', 'time', 'not', 'wake', 'revolution', 'armageddon', 'start'],\n",
       " ['red', 'faction', 'armageddon', 'microsoft', 'xbox', 'read', 'ebay'],\n",
       " ['know', 'shit', 'go', 'world', 'series', 'armageddon'],\n",
       " ['rt',\n",
       "  'love',\n",
       "  'truelove',\n",
       "  'romance',\n",
       "  'lith',\n",
       "  'voodoo',\n",
       "  'seduction',\n",
       "  'astrology',\n",
       "  'rtrrt',\n",
       "  'lotz',\n",
       "  'apocalypse',\n",
       "  'armageddon'],\n",
       " ['go', 'beat', 'armageddon', 'hsu', 'hao', 'get', 'flawless', 'try'],\n",
       " ['ben',\n",
       "  'afflecki',\n",
       "  'know',\n",
       "  'wifekid',\n",
       "  'girl',\n",
       "  'not',\n",
       "  'help',\n",
       "  'love',\n",
       "  'armageddon',\n",
       "  'eonlinechat'],\n",
       " ['long',\n",
       "  'coat',\n",
       "  'hand',\n",
       "  'wear',\n",
       "  'certainty',\n",
       "  'armageddon',\n",
       "  'bear',\n",
       "  'sense',\n",
       "  'occasion'],\n",
       " ['phone',\n",
       "  'spy',\n",
       "  'hide',\n",
       "  'door',\n",
       "  'nsa',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'software',\n",
       "  'financial',\n",
       "  'armageddon',\n",
       "  'blog'],\n",
       " ['pbban', 'hyiderghost2', 'not', 'kill', 'flag', 'fast', 'xp', 'reason'],\n",
       " ['rt',\n",
       "  'love',\n",
       "  'truelove',\n",
       "  'romance',\n",
       "  'lith',\n",
       "  'voodoo',\n",
       "  'seduction',\n",
       "  'astrology',\n",
       "  'rtrrt',\n",
       "  'lotz',\n",
       "  'apocalypse',\n",
       "  'armageddon'],\n",
       " ['pbban',\n",
       "  'temporary300',\n",
       "  'fighterdena',\n",
       "  'not',\n",
       "  'kill',\n",
       "  'flag',\n",
       "  'fast',\n",
       "  'xp',\n",
       "  'reason'],\n",
       " ['photo',\n",
       "  'sketch',\n",
       "  'base',\n",
       "  'taste',\n",
       "  'armageddon',\n",
       "  'episode',\n",
       "  'startrek',\n",
       "  'tos'],\n",
       " ['armageddon'],\n",
       " ['good', 'luck', 'tomorrow', 'night', 'coefficient', 'point', 'plz'],\n",
       " ['armageddon'],\n",
       " ['history',\n",
       "  'book',\n",
       "  'thank',\n",
       "  'justice',\n",
       "  'department',\n",
       "  'way',\n",
       "  'not',\n",
       "  'pay',\n",
       "  'income',\n",
       "  'tax'],\n",
       " ['vladimir',\n",
       "  'putin',\n",
       "  'issue',\n",
       "  'major',\n",
       "  'warning',\n",
       "  'late',\n",
       "  'escape',\n",
       "  'armageddon'],\n",
       " ['gods',\n",
       "  'kingdom',\n",
       "  'heavenly',\n",
       "  'govt',\n",
       "  'rule',\n",
       "  'people',\n",
       "  'earth',\n",
       "  'armageddon'],\n",
       " ['l',\n",
       "  'b',\n",
       "  'entertainment',\n",
       "  'lot',\n",
       "  'brucewillis',\n",
       "  'movies',\n",
       "  'dvd',\n",
       "  'die',\n",
       "  'hard',\n",
       "  'monkey',\n",
       "  'armageddon',\n",
       "  'sixth',\n",
       "  'ebay',\n",
       "  'auction'],\n",
       " ['letuas', 'talk', 'goof', 'guild', 'saunder', 'come', 'right', 'stage'],\n",
       " ['niece',\n",
       "  'gain',\n",
       "  'ability',\n",
       "  'stand',\n",
       "  'get',\n",
       "  'prepare',\n",
       "  'toddler',\n",
       "  'apocalypse',\n",
       "  'armageddon'],\n",
       " ['check',\n",
       "  'prepper',\n",
       "  'doomsday',\n",
       "  'library',\n",
       "  'collection',\n",
       "  'cd',\n",
       "  'shtf',\n",
       "  'preppertalk',\n",
       "  'survival',\n",
       "  '2a',\n",
       "  'prepper'],\n",
       " ['eep', 'think', 'yesterday', 'see', 'hella', 'scary', 'hail', 'armageddon'],\n",
       " ['ahamedis',\n",
       "  'think',\n",
       "  'messiah',\n",
       "  'come',\n",
       "  'year',\n",
       "  'ago',\n",
       "  'armageddon',\n",
       "  'dajaal',\n",
       "  'gog',\n",
       "  'magog'],\n",
       " ['sadly', 'windows', 'reveal', 'microsofts', 'ethic', 'armageddon'],\n",
       " ['armageddon', 'avert', 'el', 'patron', 'ultimalucha'],\n",
       " ['time',\n",
       "  'hour',\n",
       "  'late',\n",
       "  'series',\n",
       "  'ww1',\n",
       "  'blueprint',\n",
       "  'armageddon',\n",
       "  'extremely',\n",
       "  'impressive'],\n",
       " ['european', 'fitba', 'till', 'christmas', 'armageddon'],\n",
       " ['christians',\n",
       "  'unite',\n",
       "  'israel',\n",
       "  'cufi',\n",
       "  'jews',\n",
       "  'convert',\n",
       "  'soon',\n",
       "  'die',\n",
       "  'armageddon'],\n",
       " ['official', 'vid', 'doublecups', 'prod', 'armageddon'],\n",
       " ['tomorrow', 'day', 'start', 'armageddon', 'preseasonworkout'],\n",
       " ['lee',\n",
       "  'comedy',\n",
       "  'ui',\n",
       "  'working',\n",
       "  'class',\n",
       "  'tories',\n",
       "  'prepare',\n",
       "  'armageddon',\n",
       "  'interestrateriseu'],\n",
       " ['chart',\n",
       "  'prove',\n",
       "  'financial',\n",
       "  'crisis',\n",
       "  'begin',\n",
       "  'financial',\n",
       "  'armageddon',\n",
       "  'economic',\n",
       "  'collapse',\n",
       "  'blog',\n",
       "  'track',\n",
       "  'tren'],\n",
       " ['paul',\n",
       "  'craig',\n",
       "  'roberts',\n",
       "  'uo',\n",
       "  'vladimir',\n",
       "  'putin',\n",
       "  'issue',\n",
       "  'major',\n",
       "  'warning',\n",
       "  'late',\n",
       "  'escape',\n",
       "  'brics',\n",
       "  'roberts',\n",
       "  'russia'],\n",
       " ['stage', 'right', 'light', 'not', 'funny', 'goofball', 'staff', 'pd'],\n",
       " ['official', 'vid', 'thereal', 'armageddon', 'comin', 'soon'],\n",
       " ['celtic',\n",
       "  'finger',\n",
       "  'cross',\n",
       "  'aberdeen',\n",
       "  'tomorrow',\n",
       "  'night',\n",
       "  'armageddon',\n",
       "  'eh'],\n",
       " ['beyonce', 'pick', 'fan', 'army', 'beyhive'],\n",
       " ['direction', 'pick', 'fan', 'army', 'directioner'],\n",
       " ['second', 'summer', 'pick', 'fan', 'army'],\n",
       " ['pick', 'fan', 'army', 'beyhive'],\n",
       " ['pick', 'fan', 'army', 'beyhive'],\n",
       " ['direction', 'pick', 'fan', 'army', 'directioner', 'x1411'],\n",
       " ['see',\n",
       "  'army',\n",
       "  'whitewalkers',\n",
       "  'thing',\n",
       "  'slightly',\n",
       "  'intrigue',\n",
       "  'get',\n",
       "  'far'],\n",
       " ['build',\n",
       "  'kingdom',\n",
       "  'lead',\n",
       "  'army',\n",
       "  'victory',\n",
       "  'start',\n",
       "  'g',\n",
       "  'friend',\n",
       "  'code',\n",
       "  'lzktjnox'],\n",
       " ['salvation',\n",
       "  'army',\n",
       "  'host',\n",
       "  'rally',\n",
       "  'reconnect',\n",
       "  'father',\n",
       "  'child',\n",
       "  'salvation',\n",
       "  'army',\n",
       "  'host',\n",
       "  'school',\n",
       "  'rallyu'],\n",
       " ['vote', 'directioner', 'vs', 'queen', '5th', 'round', 'fanarmyfaceoff'],\n",
       " ['build', 'army', 'dog', 'leader', 'lion', 'dog', 'fight', 'like', 'lion'],\n",
       " ['hero', 'tv', 'review'],\n",
       " ['direction', 'pick', 'fan', 'army', 'directioner'],\n",
       " ['infantry',\n",
       "  'mens',\n",
       "  'lume',\n",
       "  'dial',\n",
       "  'army',\n",
       "  'analog',\n",
       "  'quartz',\n",
       "  'wrist',\n",
       "  'watch',\n",
       "  'sport',\n",
       "  'blue',\n",
       "  'nylon',\n",
       "  'fabric',\n",
       "  'reau'],\n",
       " ['direction', 'pick', 'fan', 'army', 'directioner', 'x1441'],\n",
       " ['victorinox', 'swiss', 'army', 'date', 'womens', 'rubber', 'mop', 'watch'],\n",
       " ['rt', 'drayesha4', 'indiakomuntorjawabdo', 'indian', 'army', 'kiu'],\n",
       " ['second', 'summer', 'pick', 'fan', 'army', '5sosfam', 'in1'],\n",
       " ['beyonce', 'pick', 'fan', 'army', 'beyhive'],\n",
       " ['direction', 'pick', 'fan', 'army', 'directioner', 'x1386'],\n",
       " ['da',\n",
       "  'mtvsummerstar',\n",
       "  'videoveranomtv',\n",
       "  'mtvhottest',\n",
       "  'britney',\n",
       "  'spears',\n",
       "  'lana',\n",
       "  'del',\n",
       "  'rey'],\n",
       " ['direction', 'pick', 'fan', 'army', 'directioner'],\n",
       " ['stony',\n",
       "  'jackson',\n",
       "  'americas',\n",
       "  'hope',\n",
       "  'lead',\n",
       "  'army',\n",
       "  'felon',\n",
       "  'army',\n",
       "  'reject',\n",
       "  'army',\n",
       "  'o',\n",
       "  'satan'],\n",
       " ['wwi',\n",
       "  'wwii',\n",
       "  'japanese',\n",
       "  'army',\n",
       "  'navy',\n",
       "  'military',\n",
       "  'japan',\n",
       "  'leather',\n",
       "  'watch',\n",
       "  'war',\n",
       "  'mido',\n",
       "  'ww1',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['beyonce', 'pick', 'fan', 'army', 'beyhive'],\n",
       " ['7beyonce', 'pick', 'fan', 'army', 'beyhive'],\n",
       " ['beyonce', 'pick', 'fan', 'army', 'beyhive'],\n",
       " ['6beyonce', 'pick', 'fan', 'army', 'beyhive'],\n",
       " ['potus',\n",
       "  'appoints',\n",
       "  'brig',\n",
       "  'gen',\n",
       "  'richard',\n",
       "  'g',\n",
       "  'kaiser',\n",
       "  'member',\n",
       "  'mississippi',\n",
       "  'river',\n",
       "  'commission',\n",
       "  'learn',\n",
       "  'mrc'],\n",
       " ['violent',\n",
       "  'country',\n",
       "  'army',\n",
       "  'involve',\n",
       "  'help',\n",
       "  'control',\n",
       "  'killing',\n",
       "  'bring',\n",
       "  'peace',\n",
       "  'poor',\n",
       "  'people'],\n",
       " ['wwi',\n",
       "  'wwii',\n",
       "  'japanese',\n",
       "  'army',\n",
       "  'navy',\n",
       "  'military',\n",
       "  'japan',\n",
       "  'leather',\n",
       "  'watch',\n",
       "  'war',\n",
       "  'mido',\n",
       "  'ww1',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['wwi',\n",
       "  'wwii',\n",
       "  'japanese',\n",
       "  'army',\n",
       "  'navy',\n",
       "  'military',\n",
       "  'japan',\n",
       "  'leather',\n",
       "  'watch',\n",
       "  'war',\n",
       "  'mido',\n",
       "  'ww1',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['direction', 'pick', 'fan', 'army', 'directioner', 'x1434'],\n",
       " ['direction', 'pick', 'fan', 'army', 'directioner'],\n",
       " ['jewish', 'terrorist', 'charge', 'historicchurch', 'arson', 'ugly', 'truth'],\n",
       " ['spokane',\n",
       "  'authority',\n",
       "  'struggle',\n",
       "  'solve',\n",
       "  'arson',\n",
       "  'case',\n",
       "  'like',\n",
       "  'today',\n",
       "  'hamilton'],\n",
       " ['thousand',\n",
       "  'attend',\n",
       "  'rally',\n",
       "  'organize',\n",
       "  'peace',\n",
       "  'protest',\n",
       "  'arson',\n",
       "  'attack',\n",
       "  'take',\n",
       "  'life'],\n",
       " ['add', 'familia', 'arson', 'squad'],\n",
       " ['fake', 'hate', 'crime', 'lesbian', 'burn', 'house', 'new'],\n",
       " ['los',\n",
       "  'angeles',\n",
       "  'times',\n",
       "  'arson',\n",
       "  'suspect',\n",
       "  'link',\n",
       "  'fire',\n",
       "  'catch',\n",
       "  'northern',\n",
       "  'newsintweet'],\n",
       " ['mourning',\n",
       "  'notice',\n",
       "  'stab',\n",
       "  'arson',\n",
       "  'victim',\n",
       "  'stir',\n",
       "  'youpolitic',\n",
       "  'griefua',\n",
       "  'israel'],\n",
       " ['mourning',\n",
       "  'notice',\n",
       "  'stab',\n",
       "  'arson',\n",
       "  'victim',\n",
       "  'stir',\n",
       "  'youpolitic',\n",
       "  'griefua',\n",
       "  'israel'],\n",
       " ['sound', 'arson'],\n",
       " ['owner', 'chicagoarea', 'gay', 'bar', 'admit', 'arson', 'scheme', 'lgbt'],\n",
       " ['wait'],\n",
       " ['arson', 'suspect', 'link', 'fire', 'catch', 'northern', 'california'],\n",
       " ['trial', 'date', 'set', 'man', 'charge', 'arson', 'burglary'],\n",
       " ['death',\n",
       "  'palestinian',\n",
       "  'toddler',\n",
       "  'arson',\n",
       "  'attack',\n",
       "  'israel',\n",
       "  'crack',\n",
       "  'jewish'],\n",
       " ['palestinian', 'teen', 'kill', 'amid', 'protest', 'arson', 'attack'],\n",
       " ['kisii',\n",
       "  'police',\n",
       "  'kisii',\n",
       "  'hunt',\n",
       "  'student',\n",
       "  'fail',\n",
       "  'arson',\n",
       "  'plot',\n",
       "  'police',\n",
       "  'kisii',\n",
       "  'hunt',\n",
       "  'student'],\n",
       " ['mariah', 'get', 'thick', 'shoulder', 'poor', 'girl'],\n",
       " ['mourning',\n",
       "  'notice',\n",
       "  'stab',\n",
       "  'arson',\n",
       "  'victim',\n",
       "  'stir',\n",
       "  'youpolitic',\n",
       "  'griefua',\n",
       "  'israel',\n",
       "  'poster',\n",
       "  'shira',\n",
       "  'banki'],\n",
       " ['relaxinpr',\n",
       "  'miprv',\n",
       "  'rt',\n",
       "  'latimes',\n",
       "  'arson',\n",
       "  'suspect',\n",
       "  'link',\n",
       "  'fire',\n",
       "  'catch',\n",
       "  'northern',\n",
       "  'california'],\n",
       " ['jewish',\n",
       "  'leader',\n",
       "  'pray',\n",
       "  'hospital',\n",
       "  'palestinian',\n",
       "  'family',\n",
       "  'treat',\n",
       "  'arson'],\n",
       " ['owner', 'chicagoarea', 'gay', 'bar', 'admit', 'arson', 'scheme', 'lgbt'],\n",
       " ['angel', 'arson'],\n",
       " ['kisii',\n",
       "  'police',\n",
       "  'kisii',\n",
       "  'hunt',\n",
       "  'student',\n",
       "  'fail',\n",
       "  'arson',\n",
       "  'plot',\n",
       "  'police',\n",
       "  'kisii',\n",
       "  'hunt',\n",
       "  'student'],\n",
       " ['mourning',\n",
       "  'notice',\n",
       "  'stab',\n",
       "  'arson',\n",
       "  'victim',\n",
       "  'stir',\n",
       "  'youpolitic',\n",
       "  'griefua',\n",
       "  'israel'],\n",
       " ['owner', 'chicagoarea', 'gay', 'bar', 'admit', 'arson', 'scheme', 'lgbt'],\n",
       " ['owner', 'chicagoarea', 'gay', 'bar', 'admit', 'arson', 'scheme', 'lgbt'],\n",
       " ['mourning',\n",
       "  'notice',\n",
       "  'stab',\n",
       "  'arson',\n",
       "  'victim',\n",
       "  'stir',\n",
       "  'youpolitic',\n",
       "  'griefua',\n",
       "  'israel'],\n",
       " ['owner', 'chicagoarea', 'gay', 'bar', 'admit', 'arson', 'scheme', 'lgbt'],\n",
       " ['arson', 'suspect', 'link', 'fire', 'catch', 'northern', 'california'],\n",
       " ['tennessee',\n",
       "  'lesbian',\n",
       "  'couple',\n",
       "  'fake',\n",
       "  'hate',\n",
       "  'crime',\n",
       "  'destroy',\n",
       "  'home',\n",
       "  'arsonu',\n",
       "  'lesbian'],\n",
       " ['arson',\n",
       "  'suspect',\n",
       "  'link',\n",
       "  'fire',\n",
       "  'catch',\n",
       "  'northern',\n",
       "  'california',\n",
       "  'los',\n",
       "  'angeles',\n",
       "  'times'],\n",
       " ['arson', 'suspect', 'link', 'fire', 'catch', 'northern', 'california'],\n",
       " ['hate', 'white', 'people', 'mo'],\n",
       " ['nowplaying', 'arsonist', 'mc', 'impressed'],\n",
       " ['allege', 'east', 'bay', 'serial', 'arsonist', 'arrest'],\n",
       " ['minor',\n",
       "  'citation',\n",
       "  'possesion',\n",
       "  'decriminalize',\n",
       "  'substance',\n",
       "  'not',\n",
       "  'face',\n",
       "  'time'],\n",
       " ['suspect', 'serial', 'arsonist', 'arrest', 'calif'],\n",
       " ['arson', 'suspect', 'link', 'fire', 'catch', 'northern', 'california'],\n",
       " ['legal', 'system', 'forget'],\n",
       " ['dick'],\n",
       " ['bloorossington',\n",
       "  'arsonist',\n",
       "  'burn',\n",
       "  'mattress',\n",
       "  'northumberland',\n",
       "  'st',\n",
       "  'cbcto'],\n",
       " ['arsonist'],\n",
       " ['not', 'nice', 'come', 'sit'],\n",
       " ['vegetarian',\n",
       "  'vegan',\n",
       "  'video',\n",
       "  'show',\n",
       "  'arsonist',\n",
       "  'torch',\n",
       "  'popular',\n",
       "  'bk',\n",
       "  'restaurant',\n",
       "  'strictly',\n",
       "  'vegetarian',\n",
       "  'govegan',\n",
       "  'uniteblue'],\n",
       " ['arsonist',\n",
       "  'arrest',\n",
       "  'set',\n",
       "  'fire',\n",
       "  'watch',\n",
       "  'tonightuas',\n",
       "  'headline',\n",
       "  'nightbeat'],\n",
       " ['video',\n",
       "  'capture',\n",
       "  'man',\n",
       "  'remove',\n",
       "  'american',\n",
       "  'flag',\n",
       "  'long',\n",
       "  'beach',\n",
       "  'home',\n",
       "  'burn',\n",
       "  'arsonist',\n",
       "  'seek'],\n",
       " ['trick', 'think', 'nasty', 'thing'],\n",
       " ['spotlight', 'paradise', 'arsonist', 'mc', 'wniagospel'],\n",
       " ['make'],\n",
       " ['town', 'salem', 'melt', 'ice', 'cube', 'bc', 'arsonist', 'd'],\n",
       " ['arsonist',\n",
       "  'blame',\n",
       "  'blaze',\n",
       "  'plastic',\n",
       "  'recycling',\n",
       "  'business',\n",
       "  'adelaide',\n",
       "  'report'],\n",
       " ['hotboy', 'shit'],\n",
       " ['zodiac',\n",
       "  'girl',\n",
       "  'feat',\n",
       "  'trey',\n",
       "  'dupree',\n",
       "  'produce',\n",
       "  'sparkz',\n",
       "  'beatz',\n",
       "  'chuck',\n",
       "  'da',\n",
       "  'arsonist'],\n",
       " ['lmfao'],\n",
       " ['allege', 'east', 'bay', 'serial', 'arsonist', 'arrest', 'sanfrancisco'],\n",
       " ['jokin', 'not', 'move'],\n",
       " ['guess', 'shit', 'think'],\n",
       " ['arsonist',\n",
       "  'set',\n",
       "  'nyc',\n",
       "  'vegetarian',\n",
       "  'restaurant',\n",
       "  'fire',\n",
       "  'police',\n",
       "  'newyork'],\n",
       " ['like', 'video', 'town', 'salem', 'win', 'arsonist'],\n",
       " ['crack', 'cuz'],\n",
       " ['smoke',\n",
       "  'good',\n",
       "  'fuck',\n",
       "  'eat',\n",
       "  'drink',\n",
       "  'drive',\n",
       "  'nice',\n",
       "  'car',\n",
       "  'wear',\n",
       "  'green',\n",
       "  'mink'],\n",
       " ['kill', 'get', 'court', 'day', 'earl'],\n",
       " ['lmao', 'real', 'live'],\n",
       " ['owner',\n",
       "  'chicagoarea',\n",
       "  'gay',\n",
       "  'bar',\n",
       "  'admit',\n",
       "  'arson',\n",
       "  'scheme',\n",
       "  'frank',\n",
       "  'elliott',\n",
       "  'plead',\n",
       "  'guilty',\n",
       "  'hire',\n",
       "  'arsonist'],\n",
       " ['trust',\n",
       "  'iran',\n",
       "  'stop',\n",
       "  'terrorism',\n",
       "  'like',\n",
       "  'invite',\n",
       "  'arsonist',\n",
       "  'join',\n",
       "  'fire',\n",
       "  'brigade',\n",
       "  'telegraph'],\n",
       " ['big', 'burn', 'true', 'story', 'arsonist', 'missing', 'girl', 'u'],\n",
       " ['stay',\n",
       "  'vigilent',\n",
       "  'civil',\n",
       "  'liberty',\n",
       "  'constant',\n",
       "  'attack',\n",
       "  'nativehuman',\n",
       "  'myreligion'],\n",
       " ['credit', 'inspire', 'rediscover', 'fantabulous', 'tbt'],\n",
       " ['nashville',\n",
       "  'theater',\n",
       "  'attack',\n",
       "  'gun',\n",
       "  'grabber',\n",
       "  'demand',\n",
       "  'uihatchet',\n",
       "  'controlyou'],\n",
       " ['break', 'terror', 'attack', 'police', 'post', 'udhampur'],\n",
       " ['demi', 'stans', 'think', 'heart', 'attack', 'sell', 'copy'],\n",
       " ['scare',\n",
       "  'new',\n",
       "  'version',\n",
       "  'nuclear',\n",
       "  'attack',\n",
       "  'warning',\n",
       "  'like',\n",
       "  'know',\n",
       "  'government',\n",
       "  'prepare'],\n",
       " ['isil', 'claim', 'suicide', 'bombing', 'saudi', 'mosque', 'kill'],\n",
       " ['funny', 'twitter', 'feminist', 'try', 'attack'],\n",
       " ['horrific', 'attack', 'wife', 'muslim', 'italy', 'liveleak', 'news'],\n",
       " ['ua93',\n",
       "  'blast',\n",
       "  'accuse',\n",
       "  'yeda',\n",
       "  'yakub',\n",
       "  'die',\n",
       "  'karachi',\n",
       "  'heart',\n",
       "  'attack',\n",
       "  'mumbai'],\n",
       " ['drone', 'attack', 'kill', 'militant', 'north', 'waziristan'],\n",
       " ['suspect', 'late', 'theater', 'attack', 'psychological', 'issue'],\n",
       " ['militant',\n",
       "  'attack',\n",
       "  'police',\n",
       "  'post',\n",
       "  'udhampur',\n",
       "  'spos',\n",
       "  'injure',\n",
       "  'livemint',\n",
       "  'allthenews'],\n",
       " ['break',\n",
       "  'obama',\n",
       "  'official',\n",
       "  'give',\n",
       "  'muslim',\n",
       "  'terrorist',\n",
       "  'weapon',\n",
       "  'texas',\n",
       "  'attack'],\n",
       " ['delhi',\n",
       "  'government',\n",
       "  'provide',\n",
       "  'free',\n",
       "  'treatment',\n",
       "  'acid',\n",
       "  'attack',\n",
       "  'victim',\n",
       "  'private',\n",
       "  'hospital'],\n",
       " ['new',\n",
       "  'post',\n",
       "  'new',\n",
       "  'smb',\n",
       "  'relay',\n",
       "  'attack',\n",
       "  'steal',\n",
       "  'user',\n",
       "  'credential',\n",
       "  'internet'],\n",
       " ['israeli',\n",
       "  'force',\n",
       "  'raid',\n",
       "  'home',\n",
       "  'alleged',\n",
       "  'car',\n",
       "  'attack',\n",
       "  'suspect',\n",
       "  'palestine'],\n",
       " ['heart',\n",
       "  'attack',\n",
       "  'think',\n",
       "  'goat',\n",
       "  'dead',\n",
       "  'not',\n",
       "  'worry',\n",
       "  'rocket',\n",
       "  'okay'],\n",
       " ['not', 'go', 'lie', 'kind', 'ready', 'attack', 'senior', 'year'],\n",
       " ['left',\n",
       "  'hand',\n",
       "  'diamond',\n",
       "  'graveyard',\n",
       "  'shift',\n",
       "  'attack',\n",
       "  'defend',\n",
       "  'right',\n",
       "  'handside',\n",
       "  'no',\n",
       "  'fucking',\n",
       "  'idiot'],\n",
       " ['volleyball',\n",
       "  'attack',\n",
       "  'ii',\n",
       "  'volleyball',\n",
       "  'training',\n",
       "  'machine',\n",
       "  'set',\n",
       "  'simulation'],\n",
       " ['notleys',\n",
       "  'tactful',\n",
       "  'direct',\n",
       "  'response',\n",
       "  'harpers',\n",
       "  'attack',\n",
       "  'albertas',\n",
       "  'govt',\n",
       "  'hell',\n",
       "  'yeah',\n",
       "  'premier',\n",
       "  'ableg',\n",
       "  'cdnpoli'],\n",
       " ['police',\n",
       "  'assailant',\n",
       "  'late',\n",
       "  'movie',\n",
       "  'theatre',\n",
       "  'attack',\n",
       "  'homeless',\n",
       "  'psychological',\n",
       "  'issue'],\n",
       " ['thank', 'damn', 'heart', 'attack'],\n",
       " ['suspect', 'late', 'theatre', 'attack', 'psychological', 'issue'],\n",
       " ['india',\n",
       "  'shud',\n",
       "  'not',\n",
       "  'evidence',\n",
       "  'pakthey',\n",
       "  'share',\n",
       "  'terrorist',\n",
       "  'use',\n",
       "  'attackshare',\n",
       "  'oth',\n",
       "  'contries'],\n",
       " ['illegal',\n",
       "  'alien',\n",
       "  'release',\n",
       "  'obamadhs',\n",
       "  'time',\n",
       "  'charge',\n",
       "  'rape',\n",
       "  'murder',\n",
       "  'santa',\n",
       "  'maria',\n",
       "  'woman',\n",
       "  'prior',\n",
       "  'offense'],\n",
       " ['strongly',\n",
       "  'condemn',\n",
       "  'attack',\n",
       "  'ary',\n",
       "  'news',\n",
       "  'team',\n",
       "  'karachi',\n",
       "  'cowardly',\n",
       "  'act',\n",
       "  'simply',\n",
       "  'try',\n",
       "  'job'],\n",
       " ['nashville',\n",
       "  'theater',\n",
       "  'attack',\n",
       "  'gun',\n",
       "  'grabber',\n",
       "  'demand',\n",
       "  'uihatchet',\n",
       "  'controlyou'],\n",
       " ['fact',\n",
       "  'atomic',\n",
       "  'bomb',\n",
       "  'call',\n",
       "  'little',\n",
       "  'boy',\n",
       "  'fat',\n",
       "  'man',\n",
       "  'say',\n",
       "  'lot',\n",
       "  'mentality',\n",
       "  'go',\n",
       "  'attack'],\n",
       " ['not',\n",
       "  'ignoranceshe',\n",
       "  'latinoand',\n",
       "  'benothing',\n",
       "  'morebut',\n",
       "  'attack',\n",
       "  'dog',\n",
       "  'hate',\n",
       "  'group',\n",
       "  'gop'],\n",
       " ['heart', 'disease', 'prevention', 'secondhand', 'smoke'],\n",
       " ['daytonarea', 'org', 'tell', 'hit', 'cyber', 'attack'],\n",
       " ['attack', 'titan', 'game', 'ps', 'vita', 'yay', 'not', 'wait'],\n",
       " ['infowars',\n",
       "  'nashville',\n",
       "  'theater',\n",
       "  'attack',\n",
       "  'gun',\n",
       "  'grabber',\n",
       "  'demand',\n",
       "  'uihatchet',\n",
       "  'controlyou',\n",
       "  'nwo'],\n",
       " ['anxiety', 'attack'],\n",
       " ['dog', 'attack', 'food', 'pugprobs'],\n",
       " ['cop',\n",
       "  'injure',\n",
       "  'gunfight',\n",
       "  'militant',\n",
       "  'attack',\n",
       "  'udhampur',\n",
       "  'police',\n",
       "  'post',\n",
       "  'suspect',\n",
       "  'militant',\n",
       "  'attack',\n",
       "  'police',\n",
       "  'post'],\n",
       " ['ask', 'feel', 'attack', 'julie', 'ask', 'frail'],\n",
       " ['feel', 'attack'],\n",
       " ['feel', 'attacked'],\n",
       " ['black',\n",
       "  'man',\n",
       "  'not',\n",
       "  'way',\n",
       "  'white',\n",
       "  'man',\n",
       "  'black',\n",
       "  'man',\n",
       "  'get',\n",
       "  'attack'],\n",
       " ['not',\n",
       "  'believe',\n",
       "  'fucking',\n",
       "  'cis',\n",
       "  'female',\n",
       "  'go',\n",
       "  'claim',\n",
       "  'offend',\n",
       "  'transgendere',\n",
       "  'female',\n",
       "  'attack',\n",
       "  'medium'],\n",
       " ['israeli',\n",
       "  'helicopter',\n",
       "  'attack',\n",
       "  'civilian',\n",
       "  'gaza',\n",
       "  'complete',\n",
       "  'exercise',\n",
       "  'greece'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['pt',\n",
       "  'unit',\n",
       "  'attack',\n",
       "  'responsible',\n",
       "  'target',\n",
       "  'muslim',\n",
       "  'scholar',\n",
       "  'imprison',\n",
       "  'youth'],\n",
       " ['telnet', 'attack', 'streamyxhomesouthern'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['tbt', 'remember', 'time', 'patrick', 'kane', 'attack', 'cab', 'driver'],\n",
       " ['feel', 'attack'],\n",
       " ['ik',\n",
       "  'troll',\n",
       "  'pol',\n",
       "  'rival',\n",
       "  'literally',\n",
       "  'abuse',\n",
       "  'attack',\n",
       "  'family',\n",
       "  'literally',\n",
       "  'abuse',\n",
       "  'ik',\n",
       "  'looser'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['bad', 'person', 'question', 'julie', 'attack', 'guy', 'no', 'empathy'],\n",
       " ['kelly',\n",
       "  'osbourne',\n",
       "  'attack',\n",
       "  'racist',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'remark',\n",
       "  'latinos',\n",
       "  'view'],\n",
       " ['aiii', 'need', 'chill', 'answer', 'calmly', 'not', 'like', 'attack'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['attack',\n",
       "  'robotlvl',\n",
       "  'earn',\n",
       "  'total',\n",
       "  'free',\n",
       "  'satoshis',\n",
       "  'robotcoingame',\n",
       "  'bitcoin',\n",
       "  'freebitcoin'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['see', 'tweet', 'feel', 'attack'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['tv',\n",
       "  'program',\n",
       "  'see',\n",
       "  'say',\n",
       "  'air',\n",
       "  'plane',\n",
       "  'fly',\n",
       "  'uranium',\n",
       "  'fukushima',\n",
       "  'attack',\n",
       "  'machine',\n",
       "  'gun',\n",
       "  'student',\n",
       "  'army',\n",
       "  'dig'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['thing', 'attack'],\n",
       " ['christian',\n",
       "  'attack',\n",
       "  'muslims',\n",
       "  'temple',\n",
       "  'mount',\n",
       "  'wave',\n",
       "  'israeli',\n",
       "  'flag',\n",
       "  'pamela',\n",
       "  'geller'],\n",
       " ['welovela',\n",
       "  'nhlducks',\n",
       "  'avalanche',\n",
       "  'defense',\n",
       "  'match',\n",
       "  'vs',\n",
       "  'st',\n",
       "  'louis',\n",
       "  'blue',\n",
       "  'sportsroadhouse'],\n",
       " ['like', 'video', 'kalle', 'mattson', 'avalanche', 'official', 'video'],\n",
       " ['crash', 'like', 'avalanche'],\n",
       " ['colorado',\n",
       "  'avalanche',\n",
       "  'mens',\n",
       "  'official',\n",
       "  'colorado',\n",
       "  'avalanche',\n",
       "  'reebok',\n",
       "  'tshirt',\n",
       "  'xl',\n",
       "  'blue',\n",
       "  'cotton',\n",
       "  'nhl',\n",
       "  'hockey'],\n",
       " ['tix',\n",
       "  'frozen',\n",
       "  'fury',\n",
       "  'xvii',\n",
       "  'los',\n",
       "  'angeles',\n",
       "  'king',\n",
       "  'v',\n",
       "  'avalanche',\n",
       "  'rowaa',\n",
       "  'mgm',\n",
       "  'grand'],\n",
       " ['bet', 'not', 'know', 'kick', 'box'],\n",
       " ['little',\n",
       "  'piece',\n",
       "  'write',\n",
       "  'avalanche',\n",
       "  'design',\n",
       "  'blog',\n",
       "  'appreciate',\n",
       "  'greatly',\n",
       "  'check'],\n",
       " ['patrick',\n",
       "  'roy',\n",
       "  'upper',\n",
       "  'deck',\n",
       "  'spx',\n",
       "  'finite',\n",
       "  'colorado',\n",
       "  'avalanche',\n",
       "  'mint'],\n",
       " ['musician',\n",
       "  'kalle',\n",
       "  'mattson',\n",
       "  'recreate',\n",
       "  'classic',\n",
       "  'album',\n",
       "  'cover',\n",
       "  'clever',\n",
       "  'music',\n",
       "  'video',\n",
       "  'avalanche'],\n",
       " ['beautiful',\n",
       "  'sweet',\n",
       "  'avalanche',\n",
       "  'faith',\n",
       "  'akito',\n",
       "  'rose',\n",
       "  'lot',\n",
       "  'frothy',\n",
       "  'gyp',\n",
       "  'weddinghour'],\n",
       " ['tix',\n",
       "  'calgary',\n",
       "  'flame',\n",
       "  'vs',\n",
       "  'col',\n",
       "  'avalanche',\n",
       "  'preseason',\n",
       "  'scotiabank',\n",
       "  'saddledome'],\n",
       " ['secret',\n",
       "  'avalanche',\n",
       "  'catechize',\n",
       "  'inner',\n",
       "  'self',\n",
       "  'confidential',\n",
       "  'communication',\n",
       "  'respect',\n",
       "  'create',\n",
       "  'worth',\n",
       "  'len'],\n",
       " ['fall',\n",
       "  'leave',\n",
       "  'poplar',\n",
       "  'fully',\n",
       "  'ordain',\n",
       "  'tumbling',\n",
       "  'avalanche',\n",
       "  'spurgeon'],\n",
       " ['see',\n",
       "  'great',\n",
       "  'punk',\n",
       "  'band',\n",
       "  'make',\n",
       "  'original',\n",
       "  'music',\n",
       "  'week',\n",
       "  'check',\n",
       "  'em'],\n",
       " ['great',\n",
       "  'performance',\n",
       "  'chip',\n",
       "  'fuelgas',\n",
       "  'saver',\n",
       "  'chevy',\n",
       "  'tahoeblazeravalanches10'],\n",
       " ['musician',\n",
       "  'kalle',\n",
       "  'mattson',\n",
       "  'recreate',\n",
       "  'classic',\n",
       "  'album',\n",
       "  'cover',\n",
       "  'clever',\n",
       "  'music',\n",
       "  'video',\n",
       "  'youavalancheua'],\n",
       " ['drive', 'avalanche', 'have', 'car', 'week', 'like', 'drive', 'tank'],\n",
       " ['free',\n",
       "  'ebay',\n",
       "  'snipe',\n",
       "  'rt',\n",
       "  'chevrolet',\n",
       "  'avalanche',\n",
       "  'ltz',\n",
       "  'lift',\n",
       "  'truck',\n",
       "  'favorite',\n",
       "  'share'],\n",
       " ['chiasson', 'sens', 'not', 'come', 'deal', 'coloradoavalanche', 'avalanche'],\n",
       " ['paul',\n",
       "  'rudd',\n",
       "  'emile',\n",
       "  'hirsch',\n",
       "  'david',\n",
       "  'gordon',\n",
       "  'green',\n",
       "  'prince',\n",
       "  'avalanche',\n",
       "  'qa',\n",
       "  'filmmaker',\n",
       "  'google',\n",
       "  'entretenimento',\n",
       "  'video'],\n",
       " ['great',\n",
       "  'time',\n",
       "  'deal',\n",
       "  'avalanche',\n",
       "  'music',\n",
       "  'purchase',\n",
       "  'neal',\n",
       "  'rigga',\n",
       "  'shirt'],\n",
       " ['draft', 'rd', 'overall', 'play', 'season'],\n",
       " ['get', 'video', 'rapper', 'get', 'song'],\n",
       " ['yo', 'flex'],\n",
       " ['avalanche', 'world', 'away', 'believe', 'wide', 'awake'],\n",
       " ['possible', 'new', 'jersey', 'avalanche', 'year'],\n",
       " ['feat', 'watch', 'bts', 'incredible', 'music', 'video', 'avalanche'],\n",
       " ['avalanche', 'city', 'sunset', 'nowplay', 'listen', 'radio'],\n",
       " ['chevrolet',\n",
       "  'avalanche',\n",
       "  'lt',\n",
       "  'lt',\n",
       "  'l',\n",
       "  'v',\n",
       "  'v',\n",
       "  'automatic',\n",
       "  'wd',\n",
       "  'pickup',\n",
       "  'truck',\n",
       "  'premium',\n",
       "  'bu'],\n",
       " ['no', 'snowflake', 'avalanche', 'feel', 'responsible'],\n",
       " ['star',\n",
       "  'wars',\n",
       "  'power',\n",
       "  'jedi',\n",
       "  'collection',\n",
       "  'battle',\n",
       "  'droid',\n",
       "  'hasbro',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['civil',\n",
       "  'war',\n",
       "  'general',\n",
       "  'battle',\n",
       "  'bull',\n",
       "  'run',\n",
       "  'hero',\n",
       "  'colonel',\n",
       "  '2nd',\n",
       "  'new',\n",
       "  'hampshire',\n",
       "  'letter',\n",
       "  'sign'],\n",
       " ['dragon', 'ball', 'z', 'battle', 'god', 'rotten', 'tomato'],\n",
       " ['add',\n",
       "  'video',\n",
       "  'playlist',\n",
       "  'world',\n",
       "  'tank',\n",
       "  'battle',\n",
       "  'assistant',\n",
       "  'mod',\n",
       "  'bat',\n",
       "  'chat',\n",
       "  'arti',\n",
       "  'kaboom'],\n",
       " ['ya', 'boy', 'clip', 'vs', 'battle'],\n",
       " ['fully', 'aware', 'battle', 'support', 'fight'],\n",
       " ['baaaack',\n",
       "  'petersens',\n",
       "  'bowhunting',\n",
       "  'battle',\n",
       "  'bow',\n",
       "  'sure',\n",
       "  'head',\n",
       "  'cast',\n",
       "  'vote'],\n",
       " ['tb', 'throwback', 'want', 'battle', 'here', 'war'],\n",
       " ['kelby',\n",
       "  'tomlinson',\n",
       "  'mildmannere',\n",
       "  '2nd',\n",
       "  'baseman',\n",
       "  'great',\n",
       "  'metropolitan',\n",
       "  'team',\n",
       "  'fight',\n",
       "  'neverending',\n",
       "  'battle',\n",
       "  'hit',\n",
       "  'rbi',\n",
       "  'sfgiants',\n",
       "  'way'],\n",
       " ['black',\n",
       "  'eye',\n",
       "  'space',\n",
       "  'battle',\n",
       "  'occur',\n",
       "  'star',\n",
       "  'm27329',\n",
       "  'involve',\n",
       "  'fleet',\n",
       "  'total',\n",
       "  'ship',\n",
       "  'destroy'],\n",
       " ['happen',\n",
       "  'take',\n",
       "  'king',\n",
       "  'story',\n",
       "  'trailer',\n",
       "  'space',\n",
       "  'battle',\n",
       "  'rip',\n",
       "  'hole',\n",
       "  'saturn'],\n",
       " ['battle', 'animation', 'fucking'],\n",
       " ['happen', 'no', 'battle', 'block', 'finally'],\n",
       " ['du19', 'gon', 'rap', 'battle'],\n",
       " ['thing', 'battle', 'internal', 'vs', 'external', 'motivation'],\n",
       " ['check', 'item', 'get', 'phantasmal', 'cummerbund', 'warcraft'],\n",
       " ['young', 'german', 'stormtrooper', 'engage', 'battle', 'somme'],\n",
       " ['like', 'video', 'marvel', 'vs', 'dc', 'avengers', 'battle'],\n",
       " ['sigh', 'daily', 'battle'],\n",
       " ['come',\n",
       "  'realization',\n",
       "  'not',\n",
       "  'attention',\n",
       "  'span',\n",
       "  'mass',\n",
       "  'battle',\n",
       "  'game',\n",
       "  'painting',\n",
       "  'playing'],\n",
       " ['interested', 'win', 'battle'],\n",
       " ['battle', 'goat'],\n",
       " ['star',\n",
       "  'wars',\n",
       "  'power',\n",
       "  'jedi',\n",
       "  'collection',\n",
       "  'battle',\n",
       "  'droid',\n",
       "  'hasbro',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['black',\n",
       "  'eye',\n",
       "  'space',\n",
       "  'battle',\n",
       "  'occur',\n",
       "  'star',\n",
       "  'o784',\n",
       "  'involve',\n",
       "  'fleet',\n",
       "  'total',\n",
       "  'ship',\n",
       "  'destroy'],\n",
       " ['lonepine',\n",
       "  'remember',\n",
       "  'australia',\n",
       "  'descendant',\n",
       "  'grow',\n",
       "  'gallipoli',\n",
       "  'ww1'],\n",
       " ['miss',\n",
       "  'gary',\n",
       "  'buseys',\n",
       "  'son',\n",
       "  'play',\n",
       "  'dixie',\n",
       "  'electronic',\n",
       "  'green',\n",
       "  'fiddle',\n",
       "  'postbattle',\n",
       "  'celebration',\n",
       "  'sequence'],\n",
       " ['news',\n",
       "  'fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['fedex', 'no', 'longer', 'transport', 'bioterror', 'germ'],\n",
       " ['usa',\n",
       "  'today',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'pathogen',\n",
       "  'newsintweets'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['jacksonville',\n",
       "  'busines',\n",
       "  'fedex',\n",
       "  'stop',\n",
       "  'ship',\n",
       "  'potential',\n",
       "  'bioterror',\n",
       "  'pathogen'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'pathogen',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['respond',\n",
       "  'fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['news',\n",
       "  'fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap',\n",
       "  'aefedex',\n",
       "  'no',\n",
       "  'tcot'],\n",
       " ['fedex',\n",
       "  'stop',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'lab',\n",
       "  'mishap',\n",
       "  'fedex',\n",
       "  'stop',\n",
       "  'transport',\n",
       "  'certain',\n",
       "  'research'],\n",
       " ['fedex', 'no', 'longer', 'ship', 'potential', 'bioterror', 'pathogen'],\n",
       " ['frontpage',\n",
       "  'bioterror',\n",
       "  'lab',\n",
       "  'face',\n",
       "  'secret',\n",
       "  'sanction',\n",
       "  'rickperry',\n",
       "  'not',\n",
       "  'cut',\n",
       "  'gopdebate'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['house',\n",
       "  'energy',\n",
       "  'amp',\n",
       "  'commerce',\n",
       "  'subcommittee',\n",
       "  'hold',\n",
       "  'hearing',\n",
       "  'cdc',\n",
       "  'oversight',\n",
       "  'bioterror',\n",
       "  'labs',\n",
       "  'army',\n",
       "  'anthrax',\n",
       "  'mishap',\n",
       "  'httu'],\n",
       " ['fedex',\n",
       "  'not',\n",
       "  'willing',\n",
       "  'transport',\n",
       "  'research',\n",
       "  'specimen',\n",
       "  'potential',\n",
       "  'bioterror',\n",
       "  'pathogen',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['world',\n",
       "  'fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap',\n",
       "  'news',\n",
       "  'phone',\n",
       "  'apple',\n",
       "  'mobile'],\n",
       " ['rt',\n",
       "  'alisonannyoung',\n",
       "  'exclusive',\n",
       "  'fedex',\n",
       "  'no',\n",
       "  'long',\n",
       "  'transport',\n",
       "  'research',\n",
       "  'specimen',\n",
       "  'bioterror',\n",
       "  'pathogen',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap',\n",
       "  'u'],\n",
       " ['hmmthis',\n",
       "  'problem',\n",
       "  'researcher',\n",
       "  'fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'select',\n",
       "  'agent'],\n",
       " ['world',\n",
       "  'fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'ship',\n",
       "  'potential',\n",
       "  'bioterror',\n",
       "  'pathogens',\n",
       "  'fedex',\n",
       "  'corp',\n",
       "  'nyse',\n",
       "  'fdx',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'deliver',\n",
       "  'package'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'ship',\n",
       "  'potential',\n",
       "  'bioterror',\n",
       "  'pathogens',\n",
       "  'atlanta',\n",
       "  'business',\n",
       "  'chronicle'],\n",
       " ['thank', 'no', 'longer', 'ship', 'live', 'microbe', 'department', 'defense'],\n",
       " ['nowplaying',\n",
       "  'orchardalley',\n",
       "  'les',\n",
       "  'nyc',\n",
       "  'bioterror',\n",
       "  'manufacture',\n",
       "  'fear',\n",
       "  'state',\n",
       "  'repression',\n",
       "  'garden'],\n",
       " ['fedex', 'no', 'longer', 'transport', 'bioterror', 'germ'],\n",
       " ['jax',\n",
       "  'biz',\n",
       "  'journal',\n",
       "  'fedex',\n",
       "  'stop',\n",
       "  'ship',\n",
       "  'potential',\n",
       "  'bioterror',\n",
       "  'pathogen'],\n",
       " ['usatoday',\n",
       "  'todays',\n",
       "  'frontpage',\n",
       "  'bioterror',\n",
       "  'lab',\n",
       "  'face',\n",
       "  'secret',\n",
       "  'sanction',\n",
       "  'rickperry',\n",
       "  'not',\n",
       "  'cut',\n",
       "  'foxnewu'],\n",
       " ['breakingnews',\n",
       "  'fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'willing',\n",
       "  'transport',\n",
       "  'research',\n",
       "  'specimen',\n",
       "  'potential',\n",
       "  'bioteru'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['fedex', 'stop', 'ship', 'potential', 'bioterror', 'pathogen', 'trucking'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['fedex', 'no', 'longer', 'ship', 'bioterror', 'germ', 'wxiatv'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['usatoday',\n",
       "  'todays',\n",
       "  'frontpage',\n",
       "  'bioterror',\n",
       "  'lab',\n",
       "  'face',\n",
       "  'secret',\n",
       "  'sanction',\n",
       "  'rickperry',\n",
       "  'not',\n",
       "  'cut',\n",
       "  'foxnewu'],\n",
       " ['fedex',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'transport',\n",
       "  'bioterror',\n",
       "  'germ',\n",
       "  'wake',\n",
       "  'anthrax',\n",
       "  'lab',\n",
       "  'mishap'],\n",
       " ['biolab',\n",
       "  'safety',\n",
       "  'concern',\n",
       "  'grow',\n",
       "  'fedex',\n",
       "  'stop',\n",
       "  'transport',\n",
       "  'certain',\n",
       "  'specimen',\n",
       "  'research',\n",
       "  'facility',\n",
       "  'dumbfound',\n",
       "  'action'],\n",
       " ['fight', 'bioterrorism', 'sir'],\n",
       " ['like',\n",
       "  'video',\n",
       "  'fema',\n",
       "  'region',\n",
       "  'iii',\n",
       "  'target',\n",
       "  'bioterrorism',\n",
       "  'nasa',\n",
       "  'japan',\n",
       "  'rocket',\n",
       "  'launch',\n",
       "  'lithium'],\n",
       " ['anthrax',\n",
       "  'bioterrorism',\n",
       "  'cdc',\n",
       "  'carry',\n",
       "  'extensive',\n",
       "  'review',\n",
       "  'lab',\n",
       "  'safety',\n",
       "  'pathogen',\n",
       "  'handling',\n",
       "  'procedure'],\n",
       " ['firepower',\n",
       "  'lab',\n",
       "  'electronic',\n",
       "  'resource',\n",
       "  'automation',\n",
       "  'fight',\n",
       "  'infectious',\n",
       "  'disease',\n",
       "  'bioterrorism',\n",
       "  'u'],\n",
       " ['collude',\n",
       "  'wht',\n",
       "  'f',\n",
       "  'auth',\n",
       "  'hostage2',\n",
       "  'look',\n",
       "  'blk',\n",
       "  'wbioterrorismuse',\n",
       "  'lglorg',\n",
       "  'idis',\n",
       "  'would'],\n",
       " ['great',\n",
       "  'iran',\n",
       "  'deal',\n",
       "  'cover',\n",
       "  'bioterrorism',\n",
       "  'get',\n",
       "  'cut',\n",
       "  'terrible',\n",
       "  'good',\n",
       "  'work'],\n",
       " ['tale',\n",
       "  'pox',\n",
       "  'body',\n",
       "  'horror',\n",
       "  'virus',\n",
       "  'infectiousdiseases',\n",
       "  'bioterrorism'],\n",
       " ['bioterrorism',\n",
       "  'public',\n",
       "  'health',\n",
       "  'superbug',\n",
       "  'biolabs',\n",
       "  'epidemics',\n",
       "  'biosurveillance',\n",
       "  'outbreak',\n",
       "  'homeland',\n",
       "  'security',\n",
       "  'news',\n",
       "  'wire'],\n",
       " ['creation',\n",
       "  'ai',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'bioterrorism',\n",
       "  'mass',\n",
       "  'automation',\n",
       "  'workforce',\n",
       "  'contact',\n",
       "  'life',\n",
       "  'wealth',\n",
       "  'inequality',\n",
       "  'yea',\n",
       "  'get',\n",
       "  'easy'],\n",
       " ['lie',\n",
       "  'trust',\n",
       "  'dvd',\n",
       "  'cia',\n",
       "  'hollywood',\n",
       "  'bioterrorism',\n",
       "  'len',\n",
       "  'horowitz',\n",
       "  'vaccine',\n",
       "  'nwo'],\n",
       " ['satans',\n",
       "  'daughter',\n",
       "  'shadow',\n",
       "  'warrior',\n",
       "  'ft',\n",
       "  'woman',\n",
       "  'aka',\n",
       "  'transgender',\n",
       "  'mode',\n",
       "  'ps',\n",
       "  'nyc',\n",
       "  'fold',\n",
       "  'extra',\n",
       "  'extra',\n",
       "  'center',\n",
       "  'bioterrorism'],\n",
       " ['yes',\n",
       "  'think',\n",
       "  'not',\n",
       "  'college',\n",
       "  'difficult',\n",
       "  'not',\n",
       "  'think',\n",
       "  'bioterrorism',\n",
       "  'esp',\n",
       "  'bc',\n",
       "  'disperse'],\n",
       " ['volunteer',\n",
       "  'need',\n",
       "  'participate',\n",
       "  'emergency',\n",
       "  'preparedness',\n",
       "  'drill',\n",
       "  'simulate',\n",
       "  'bioterrorism',\n",
       "  'disaster'],\n",
       " ['major',\n",
       "  'american',\n",
       "  'newspaper',\n",
       "  'run',\n",
       "  'series',\n",
       "  'allege',\n",
       "  'bioterrorism',\n",
       "  'research',\n",
       "  'go'],\n",
       " ['fight', 'bioterrorism', 'sir'],\n",
       " ['cdc', 'pretty', 'cool', 'list', 'bioterrorism', 'agent'],\n",
       " ['government',\n",
       "  'expert',\n",
       "  'concern',\n",
       "  'possible',\n",
       "  'bioterrorism',\n",
       "  'gm',\n",
       "  'organism',\n",
       "  'scientist',\n",
       "  'concerned'],\n",
       " ['prepare',\n",
       "  'hhs',\n",
       "  'select',\n",
       "  'regional',\n",
       "  'special',\n",
       "  'pathogen',\n",
       "  'treatment',\n",
       "  'center',\n",
       "  'bioterrorism',\n",
       "  'infectious',\n",
       "  'ebola'],\n",
       " ['support',\n",
       "  'sys',\n",
       "  'auth',\n",
       "  'take',\n",
       "  'hostage',\n",
       "  'blk',\n",
       "  'clergyforced',\n",
       "  'exist',\n",
       "  'youngergrossly',\n",
       "  'disfigure',\n",
       "  'bioterrorism'],\n",
       " ['win',\n",
       "  'think',\n",
       "  'possibility',\n",
       "  'transformation',\n",
       "  'impossible',\n",
       "  'not',\n",
       "  'like',\n",
       "  'medical',\n",
       "  'mystery',\n",
       "  'bioterrorism',\n",
       "  'suck'],\n",
       " ['fight', 'bioterrorism', 'sir'],\n",
       " ['book',\n",
       "  'describe',\n",
       "  'future',\n",
       "  'therapy',\n",
       "  'technology',\n",
       "  'sport',\n",
       "  'sexuality',\n",
       "  'bioterrorism',\n",
       "  'diagnosis',\n",
       "  'digitalhealth',\n",
       "  'hcsm'],\n",
       " ['wugliness', 'ugly', 'frat', 'bioterrorismim', 'would', 'tolewantg', 'home'],\n",
       " ['collude',\n",
       "  'wht',\n",
       "  'f',\n",
       "  'auth',\n",
       "  'hostage2',\n",
       "  'look',\n",
       "  'blk',\n",
       "  'wbioterrorismuse',\n",
       "  'lglorg',\n",
       "  'idis',\n",
       "  'would'],\n",
       " ['fight', 'bioterrorism', 'sir'],\n",
       " ['collude',\n",
       "  'wht',\n",
       "  'f',\n",
       "  'auth',\n",
       "  'hostage2',\n",
       "  'look',\n",
       "  'blk',\n",
       "  'wbioterrorismuse',\n",
       "  'lglorg',\n",
       "  'idis',\n",
       "  'would'],\n",
       " ['bioterrorism', 'evade', 'lgl', 'effort', 'prosecute', 'blks', 'kidnapg'],\n",
       " ['irandeal',\n",
       "  'cover',\n",
       "  'nuclear',\n",
       "  'activity',\n",
       "  'bioterrorism',\n",
       "  'iran',\n",
       "  'break',\n",
       "  'agreement'],\n",
       " ['threat', 'anthrax', 'cdc'],\n",
       " ['bioterrorism',\n",
       "  'authority',\n",
       "  'allay',\n",
       "  'glanders',\n",
       "  'fear',\n",
       "  'ahead',\n",
       "  'rio',\n",
       "  'olympic',\n",
       "  'equestrian',\n",
       "  'test',\n",
       "  'event'],\n",
       " ['crave'],\n",
       " ['pool', 'day', 'raisinfinger'],\n",
       " ['know', 'look', 'hammond', 'share', 'listing'],\n",
       " ['life',\n",
       "  'amazin',\n",
       "  'time',\n",
       "  'crazy',\n",
       "  'niggas',\n",
       "  'dey',\n",
       "  'want',\n",
       "  'blaze',\n",
       "  'hate',\n",
       "  'take',\n",
       "  'dedication',\n",
       "  'n',\n",
       "  'motivation'],\n",
       " ['blaze', 'bro'],\n",
       " ['yes', 'love'],\n",
       " ['virtual',\n",
       "  'tour',\n",
       "  'listing',\n",
       "  'fir',\n",
       "  'st',\n",
       "  'cannon',\n",
       "  'beach',\n",
       "  'list',\n",
       "  'dorrie',\n",
       "  'caruana'],\n",
       " ['pendleton',\n",
       "  'media',\n",
       "  'office',\n",
       "  'say',\n",
       "  'fire',\n",
       "  'base',\n",
       "  'right',\n",
       "  'horno',\n",
       "  'blaze'],\n",
       " ['welcome', 'family', 'cleveland', 'standup'],\n",
       " ['love', 'live', 'blaze', 'inside', 'apt', 'balcony'],\n",
       " ['alrighty', 'hit', 'blaze'],\n",
       " ['blaze', 'jays', 'fuck', 'dutch', 'slave', 'trade'],\n",
       " ['hair', 'poverty', 'moment', 'need', 'fade', 'weekend', 'get'],\n",
       " ['property',\n",
       "  'loss',\n",
       "  'california',\n",
       "  'wildfire',\n",
       "  'nearly',\n",
       "  'double',\n",
       "  'weekold',\n",
       "  'blaze',\n",
       "  'rages'],\n",
       " ['break',\n",
       "  'firefighter',\n",
       "  'battle',\n",
       "  'blaze',\n",
       "  'east',\n",
       "  'cary',\n",
       "  'condo',\n",
       "  'building'],\n",
       " ['niggas', 'love', 'hate'],\n",
       " ['not', 'like', 'babes', 'ball', 'fun', 'lan'],\n",
       " ['like',\n",
       "  'video',\n",
       "  'minecraft',\n",
       "  'skywars',\n",
       "  'o',\n",
       "  'blaze',\n",
       "  'que',\n",
       "  'usa',\n",
       "  'hack',\n",
       "  'e',\n",
       "  'flechadas',\n",
       "  'sinistras'],\n",
       " ['dem',\n",
       "  'blaze',\n",
       "  'cover',\n",
       "  'month',\n",
       "  'agochicago',\n",
       "  'police',\n",
       "  'detain',\n",
       "  'thousand',\n",
       "  'black',\n",
       "  'americans',\n",
       "  'interrogation',\n",
       "  'facility'],\n",
       " ['yo', 'get', 'bar', 'not', 'rapper'],\n",
       " ['ugh', 'y', 'blaze', 'calorie', 'pizza', 'ok', 'cool', 'thisispublichealth'],\n",
       " ['pic',\n",
       "  'blaze',\n",
       "  'fort',\n",
       "  'kid',\n",
       "  'look',\n",
       "  'like',\n",
       "  'jackass',\n",
       "  'stuffin',\n",
       "  'face',\n",
       "  'like'],\n",
       " ['look', 'like', 'year', 'writing', 'computer', 'ahead'],\n",
       " ['let', 'know'],\n",
       " ['mad', 'not', 'blaze'],\n",
       " ['yeah',\n",
       "  'know',\n",
       "  'blaze',\n",
       "  'blue',\n",
       "  'not',\n",
       "  'twitter',\n",
       "  'lol',\n",
       "  'draw',\n",
       "  'week',\n",
       "  'ago'],\n",
       " ['socialmedia',\n",
       "  'news',\n",
       "  'new',\n",
       "  'facebook',\n",
       "  'page',\n",
       "  'feature',\n",
       "  'seek',\n",
       "  'help',\n",
       "  'personalize',\n",
       "  'customer',\n",
       "  'experience'],\n",
       " ['shout', 'blaze', 'hot', 'dj', 'sothwest'],\n",
       " ['like', 'video', 'minecraft', 'episode', 'blaze', 'farm', 'beginning'],\n",
       " ['property',\n",
       "  'loss',\n",
       "  'california',\n",
       "  'wildfire',\n",
       "  'nearly',\n",
       "  'double',\n",
       "  'weekold',\n",
       "  'blaze',\n",
       "  'rage',\n",
       "  'fireu'],\n",
       " ['not', 'work', 'new', 'vela', 'short', 'blaze'],\n",
       " ['total', 'tweet', 'fail', 'beautiful', 'inside', 'blaze'],\n",
       " ['mixtape', 'come', 'promise', 'go', 'right'],\n",
       " ['huh', 'leo', 'start', 'year', 'people', 'blaze'],\n",
       " ['bedroom', 'bath', 'sale', 'palm', 'youtube', 'video'],\n",
       " ['yes', 'gun'],\n",
       " ['daem', 'girl', 'smooth', 'asf', 'c'],\n",
       " ['artisteoftheweekfact',\n",
       "  'conversation',\n",
       "  'coast2coastdjs',\n",
       "  'agree',\n",
       "  's',\n",
       "  'jiwonle',\n",
       "  'hiphop',\n",
       "  'clubbanger'],\n",
       " ['bright', 'blaze', 'fireman', 'birthday', 'party', 'wedding'],\n",
       " ['real',\n",
       "  'vibez',\n",
       "  'radio',\n",
       "  'blaze',\n",
       "  'good',\n",
       "  'vibez',\n",
       "  'nowplaying',\n",
       "  'listenlive'],\n",
       " ['montgomery',\n",
       "  'come',\n",
       "  'blaze',\n",
       "  'hot',\n",
       "  'weatherstay',\n",
       "  'stds',\n",
       "  'reject',\n",
       "  'city',\n",
       "  'slogan'],\n",
       " ['come',\n",
       "  'join',\n",
       "  'tomorrow',\n",
       "  'august',\n",
       "  'transcendblaze',\n",
       "  'trail',\n",
       "  'diversified',\n",
       "  'world',\n",
       "  'marketing'],\n",
       " ['morgan',\n",
       "  'silver',\n",
       "  'dollar',\n",
       "  's',\n",
       "  'gem',\n",
       "  'bu',\n",
       "  'dmpl',\n",
       "  'cameo',\n",
       "  'rev',\n",
       "  'blazing',\n",
       "  'ms',\n",
       "  'high',\n",
       "  'grade',\n",
       "  'read',\n",
       "  'u'],\n",
       " ['morgan',\n",
       "  'silver',\n",
       "  'dollar',\n",
       "  's',\n",
       "  'gem',\n",
       "  'bu',\n",
       "  'dmpl',\n",
       "  'cameo',\n",
       "  'rev',\n",
       "  'blazing',\n",
       "  'ms',\n",
       "  'high',\n",
       "  'grade',\n",
       "  'read',\n",
       "  'u'],\n",
       " ['bowl', 'get', 'think', 'damn', 'blaze', 'damn', 'long'],\n",
       " ['blaze', 'game', 'good', 'stealth', 'skill', 'beat', 'silence', 'm4'],\n",
       " ['bit',\n",
       "  'pacquiao',\n",
       "  'vs',\n",
       "  'marquez',\n",
       "  'unfilled',\n",
       "  'blaze',\n",
       "  'swarm',\n",
       "  'online',\n",
       "  'durvod'],\n",
       " ['turn', 'radio', 'stoponesounds', 'live', 'airwave', 'fm', 'blaze', 'hit'],\n",
       " ['inch', 'dynasty'],\n",
       " ['weapon', 'master', 'let', 'gun', 'blaze', 'hinatobot'],\n",
       " ['not', 'sadly', 'come', 'expect', 'obama'],\n",
       " ['shouout', 'vocal', 'blaze', 'hot', 'like', 'weather'],\n",
       " ['s3xleak',\n",
       "  '19yrs',\n",
       "  'old',\n",
       "  'ash',\n",
       "  'lady',\n",
       "  'festac',\n",
       "  'town',\n",
       "  'delta',\n",
       "  'exp0sed',\n",
       "  'bbm',\n",
       "  'leak',\n",
       "  'picture'],\n",
       " ['oh',\n",
       "  'heart',\n",
       "  'race',\n",
       "  'temperature',\n",
       "  'blaze',\n",
       "  'roof',\n",
       "  'videoveranomtv',\n",
       "  'fifth',\n",
       "  'harmony'],\n",
       " ['haha', 'love'],\n",
       " ['colorado',\n",
       "  'spanish',\n",
       "  'word',\n",
       "  'latin',\n",
       "  'origin',\n",
       "  'mean',\n",
       "  'reddish',\n",
       "  'color',\n",
       "  'dummy',\n",
       "  'pronounce',\n",
       "  'wrong'],\n",
       " ['traffic', 'freeze', 'cold', 'blaze', 'hot', 'uo', 'heat', 'traffic'],\n",
       " ['crazy', 'run', 'degree', 'midday', 'heat', 'blaze', 'sun', 'place', 'notu'],\n",
       " ['blaze', 'rn', 'stop'],\n",
       " ['blaze'],\n",
       " ['thank', 'sweat', 'bullet', 'time', 'blaze', 'sun', 'beat'],\n",
       " ['no', 'blaze'],\n",
       " ['lose',\n",
       "  'word',\n",
       "  'new',\n",
       "  'fan',\n",
       "  'fam',\n",
       "  'crazy',\n",
       "  'skill',\n",
       "  'bless',\n",
       "  'blaze',\n",
       "  'dude',\n",
       "  'love',\n",
       "  'respect'],\n",
       " ['let', 'hot', 'blaze', 'fantasy', 'escort', 'dubai'],\n",
       " ['thank', 'missy', 'think', 'suit', 'blaze', 'hot', 'summertime', 'yeehaw'],\n",
       " ['magical', 'bag', 'blaze'],\n",
       " ['blaze', 'elwoods', 'not', 'bother', 'dougs', 'song', 'tune'],\n",
       " ['morgan',\n",
       "  'silver',\n",
       "  'dollar',\n",
       "  'p',\n",
       "  'ch',\n",
       "  'gem',\n",
       "  'bu',\n",
       "  'pl',\n",
       "  'blaze',\n",
       "  'ms',\n",
       "  'satin',\n",
       "  'rare',\n",
       "  'proof',\n",
       "  'like',\n",
       "  'reu'],\n",
       " ['blaze',\n",
       "  'hot',\n",
       "  'etisalat',\n",
       "  'free',\n",
       "  'mb',\n",
       "  'complete',\n",
       "  'month',\n",
       "  'etisalat',\n",
       "  'give',\n",
       "  'mb',\n",
       "  'tecno',\n",
       "  'q1',\n",
       "  'ime'],\n",
       " ['want',\n",
       "  'bitch',\n",
       "  'slap',\n",
       "  'gun',\n",
       "  'blaze',\n",
       "  'cake',\n",
       "  'throw',\n",
       "  'charles',\n",
       "  'showdown',\n",
       "  'worth',\n",
       "  'wait'],\n",
       " ['follow', 'check', 'hit', 'single', 'unpacked', 'man', 'blazing'],\n",
       " ['not',\n",
       "  'know',\n",
       "  'hour',\n",
       "  'physical',\n",
       "  'activity',\n",
       "  'blaze',\n",
       "  'sun',\n",
       "  'not',\n",
       "  'sport'],\n",
       " ['way', 'archetype', 'bleed', 'wellgrounded', 'readiness', 'fpoj'],\n",
       " ['throw', 'chicken', 'nugget', 'sister', 'lip', 'bleed'],\n",
       " ['slit', 'throat', 'apologize', 'bleed'],\n",
       " ['joe',\n",
       "  'landolina',\n",
       "  'gel',\n",
       "  'stop',\n",
       "  'bleed',\n",
       "  'instantly',\n",
       "  'arizona',\n",
       "  'realestate'],\n",
       " ['nose', 'bleed', 'like', 'year', 'ago'],\n",
       " ['bleed', 'typewriter', 'day', 'far', 'write', 'bunch', 'gunk'],\n",
       " ['not',\n",
       "  'help',\n",
       "  'hope',\n",
       "  'ok',\n",
       "  'text',\n",
       "  'need',\n",
       "  'talk',\n",
       "  'send',\n",
       "  'hug',\n",
       "  'way',\n",
       "  'ps',\n",
       "  'no',\n",
       "  'bleed',\n",
       "  'death',\n",
       "  'allow'],\n",
       " ['stab', 'promise', 'bleed'],\n",
       " ['apparently', 'bleed', 'people', 'look', 'weird', 'lol', 'fine', 'walk'],\n",
       " ['eat', 'takis', 'rub', 'eye', 'hand', 'eye', 'bleed', 'tear'],\n",
       " ['yea', 'hear', 'thatnot', 'come', 'vampiro', 'bleed'],\n",
       " ['look', 'broken', 'bleed'],\n",
       " ['hit', 'foot', 'toe', 'bleed'],\n",
       " ['long', 'madara', 'bleed', 'good'],\n",
       " ['think',\n",
       "  'step',\n",
       "  'broken',\n",
       "  'glass',\n",
       "  'pun',\n",
       "  'tak',\n",
       "  'sedar',\n",
       "  'not',\n",
       "  'feel',\n",
       "  'pain',\n",
       "  'bleed',\n",
       "  'shit'],\n",
       " ['ear', 'bleed'],\n",
       " ['wait', 'hour', 'cab', 'foot', 'bleed'],\n",
       " ['not', 'cute', 'dinner', 'date', 'til', 'cam', 'nose', 'start', 'bleed'],\n",
       " ['head', 'like', 'yo', 'nose', 'bleed'],\n",
       " ['bleed', 'silence', 'feel', 'safe', 'violence'],\n",
       " ['fall', 'someone', 'hit', 'head', 'concrete', 'bleed', 'n', 'shit'],\n",
       " ['yes', 'bleed', 'heart', 'liberal'],\n",
       " ['deadpool',\n",
       "  'favourite',\n",
       "  'marvel',\n",
       "  'character',\n",
       "  'know',\n",
       "  'wear',\n",
       "  'red',\n",
       "  'suit',\n",
       "  'bad',\n",
       "  'guy',\n",
       "  'not',\n",
       "  'tell',\n",
       "  'bleed'],\n",
       " ['defs', 'stop', 'bleed'],\n",
       " ['know', 'bleed', 'heart', 'wannabe', 'pickup', 'artist'],\n",
       " ['awesome', 'see', 'bleed', 'pretty', 'bad'],\n",
       " ['bleed', 'brain', 'not', 'know'],\n",
       " ['let', 'good', 'soccer', 'bleed', 'yo', 'face'],\n",
       " ['bleed', 'wild', 'thing', 'run', 'apartment', 'work', 'bar'],\n",
       " ['bleed', 'instinct', 'kick', 'look', 'away', 'scratch', 'head'],\n",
       " ['ear', 'bleed', 'hate', 'stefano'],\n",
       " ['ear', 'start', 'bleed'],\n",
       " ['see', 'pileup', 'tv', 'race', 'bleed'],\n",
       " ['rave', 'wedding', 'see', 'eye', 'bleed'],\n",
       " ['bad',\n",
       "  'thing',\n",
       "  'happen',\n",
       "  'reason',\n",
       "  'no',\n",
       "  'wise',\n",
       "  'word',\n",
       "  'go',\n",
       "  'stop',\n",
       "  'te',\n",
       "  'bleed'],\n",
       " ['yo', 'timeline', 'blow', 'damn', 'fast'],\n",
       " ['look', 'blow', 'w', 'atomic', 'bomb'],\n",
       " ['blow', 'oomf', 'instagrams', 'cute', 'active', 'follower'],\n",
       " ['crazy', 'line', 'blow'],\n",
       " ['instagram',\n",
       "  'blow',\n",
       "  'apparently',\n",
       "  'feature',\n",
       "  'jazz',\n",
       "  'tonight',\n",
       "  'cool',\n",
       "  'love'],\n",
       " ['team', 'usagi', 'blow', 'entire', 'solar', 'system', 'airhead', 'misstep'],\n",
       " ['remember', 'like', 'blow', 'game', 'prob', 'not', 'king', 'win', 'cup'],\n",
       " ['hate',\n",
       "  'people',\n",
       "  'tweet',\n",
       "  'receipt',\n",
       "  'know',\n",
       "  'wrong',\n",
       "  'not',\n",
       "  'bc',\n",
       "  'blow',\n",
       "  'literally',\n",
       "  'gtfo',\n",
       "  'desperate'],\n",
       " ['think', 'blow', 'notification', 'check', 'encouraging', 'love'],\n",
       " ['hw18',\n",
       "  'go',\n",
       "  'dude',\n",
       "  'keep',\n",
       "  'take',\n",
       "  'exit',\n",
       "  'pull',\n",
       "  'tell',\n",
       "  'blow',\n",
       "  'motor',\n",
       "  'lolol'],\n",
       " ['universe',\n",
       "  'not',\n",
       "  'actually',\n",
       "  'exist',\n",
       "  'scientist',\n",
       "  'sun',\n",
       "  'blow',\n",
       "  'earth',\n",
       "  'begin'],\n",
       " ['max', 'blow', 'tf', 'shot', 'fire', 'catfishmtv'],\n",
       " ['rick', 'morty', 'blow'],\n",
       " ['ty', 'blow', 'motor', 'go', 'flame', 'get', 'ok'],\n",
       " ['realize', 'dude', 'way', 'blow', 'tusky', 'game'],\n",
       " ['night', 'blow', 'rq'],\n",
       " ['blow', 'snapchat', 'no', 'reason'],\n",
       " ['queens',\n",
       "  'gambit',\n",
       "  'go',\n",
       "  'anakin',\n",
       "  'blow',\n",
       "  'droid',\n",
       "  'control',\n",
       "  'ship',\n",
       "  'oh',\n",
       "  'fun',\n",
       "  'not',\n",
       "  'stop',\n",
       "  'wbc2015'],\n",
       " ['ironically',\n",
       "  'michele',\n",
       "  'bachmann',\n",
       "  'bring',\n",
       "  'wron',\n",
       "  'paul',\n",
       "  'blow',\n",
       "  'call',\n",
       "  'hoax',\n",
       "  'finally',\n",
       "  'right'],\n",
       " ['catfish', 'retweete', 'notification', 'blow'],\n",
       " ['guy',\n",
       "  'whistle',\n",
       "  'parking',\n",
       "  'lot',\n",
       "  'not',\n",
       "  'help',\n",
       "  'wind',\n",
       "  'blow',\n",
       "  'skirt',\n",
       "  'get',\n",
       "  'car'],\n",
       " ['blow', 'mention'],\n",
       " ['1st', 'time', 'blow', 'phone', 'time', 'block', 'believe', 'catfish'],\n",
       " ['like', 'literally', 'blow'],\n",
       " ['freyas', 'video', 'blow'],\n",
       " ['easy', 'way', 'look', 'good', 'ray', 'rice', 'fiascothat', 'blow'],\n",
       " ['lmao', 'light', 'skin', 'guy', 'blow', 'twitter', 'talk', 'ugly', 'kid'],\n",
       " ['watch',\n",
       "  'honey',\n",
       "  'blow',\n",
       "  'economy',\n",
       "  'recommend',\n",
       "  'conservative',\n",
       "  'shoppe',\n",
       "  'horrors',\n",
       "  'harperanetflixshow',\n",
       "  'stopharper'],\n",
       " ['fireball', 'fall', 'earth', 'look', 'like', 'plane', 'blow'],\n",
       " ['ye', 'thing', 'big', 'sean', 'blow'],\n",
       " ['zayn', 'blow', 'twitter'],\n",
       " ['bitch', 'blow', 'shit'],\n",
       " ['not', 'damn', 'clue', 'try', 'click', 'blow', 'play'],\n",
       " ['great', 'pair', 'like', 'couple', 'graywardens', 'fight', 'blight'],\n",
       " ['actually',\n",
       "  'theory',\n",
       "  'magister',\n",
       "  'not',\n",
       "  'reason',\n",
       "  'blight',\n",
       "  'dwarve',\n",
       "  'one'],\n",
       " ['cycling',\n",
       "  'fan',\n",
       "  'feel',\n",
       "  'sorry',\n",
       "  'world',\n",
       "  'athletics',\n",
       "  'doping',\n",
       "  'blight',\n",
       "  'exacerbate',\n",
       "  'monetary',\n",
       "  'reward',\n",
       "  'lot',\n",
       "  'soul',\n",
       "  'search',\n",
       "  'require'],\n",
       " ['blight'],\n",
       " ['poor', 'jack'],\n",
       " ['agree',\n",
       "  'know',\n",
       "  'go',\n",
       "  'deep',\n",
       "  'road',\n",
       "  'find',\n",
       "  'blight',\n",
       "  'red',\n",
       "  'lyrium',\n",
       "  'not'],\n",
       " ['load', 'welfare', 'love', 'spongea', 'blight', 'society'],\n",
       " ['new',\n",
       "  'post',\n",
       "  'prysmian',\n",
       "  'secures',\n",
       "  'contract',\n",
       "  'blight',\n",
       "  'bank',\n",
       "  'wind',\n",
       "  'farm'],\n",
       " ['cleveland',\n",
       "  'heights',\n",
       "  'shaker',\n",
       "  'height',\n",
       "  'fight',\n",
       "  'blight',\n",
       "  'house',\n",
       "  'door'],\n",
       " ['want', 'info', 'blight', 'get'],\n",
       " ['carl',\n",
       "  'everest',\n",
       "  'rob',\n",
       "  'cobes',\n",
       "  'whitt',\n",
       "  'blight',\n",
       "  'frost',\n",
       "  'leo',\n",
       "  'snuff',\n",
       "  'godly',\n",
       "  'drink',\n",
       "  'beer',\n",
       "  'someday'],\n",
       " ['thda', 'kick', 'antiblight', 'loan', 'effort', 'memphis'],\n",
       " ['city',\n",
       "  'program',\n",
       "  'help',\n",
       "  'turn',\n",
       "  'blight',\n",
       "  'greenspace',\n",
       "  'tennessee',\n",
       "  'housing',\n",
       "  'developmentu',\n",
       "  'news'],\n",
       " ['yes', 'want', 'new', 'blight', 'leader'],\n",
       " ['release',\n",
       "  'planing',\n",
       "  'level',\n",
       "  'constellation',\n",
       "  'blight',\n",
       "  'gaia',\n",
       "  'iclowns',\n",
       "  'drumstep',\n",
       "  'remix',\n",
       "  'infinity'],\n",
       " ['cleveland',\n",
       "  'heights',\n",
       "  'shaker',\n",
       "  'height',\n",
       "  'fight',\n",
       "  'blight',\n",
       "  'house',\n",
       "  'door'],\n",
       " ['go',\n",
       "  'achieve',\n",
       "  'excellence',\n",
       "  'big',\n",
       "  'thing',\n",
       "  'develop',\n",
       "  'habit',\n",
       "  'little',\n",
       "  'matter',\n",
       "  'not',\n",
       "  'know',\n",
       "  'author'],\n",
       " ['sure', 'purdies', 'alive', 'blight'],\n",
       " ['tracy', 'blight', 'thank', 'follow'],\n",
       " ['apperception', 'bridgework', 'blight', 'xxhjesc'],\n",
       " ['dotish', 'blight', 'car', 'right', 'ahead', 'not'],\n",
       " ['sexual', 'revolutionblight', 'woman', 'story'],\n",
       " ['like',\n",
       "  'swear',\n",
       "  'secret',\n",
       "  'uncover',\n",
       "  'old',\n",
       "  'god',\n",
       "  'slumber',\n",
       "  'think',\n",
       "  'go',\n",
       "  'blight'],\n",
       " ['double', 'result', 'live', 'app'],\n",
       " ['zippoline',\n",
       "  'no',\n",
       "  'want',\n",
       "  'use',\n",
       "  'community',\n",
       "  'ask',\n",
       "  'blight',\n",
       "  'park',\n",
       "  'moveit'],\n",
       " ['look',\n",
       "  'policy',\n",
       "  'matter',\n",
       "  'ohio',\n",
       "  'report',\n",
       "  'cle',\n",
       "  'cuyahoga',\n",
       "  'county',\n",
       "  'blight',\n",
       "  'green',\n",
       "  'vacant',\n",
       "  'land',\n",
       "  'soon'],\n",
       " ['taint', 'magister', 'open', 'gate', 'let', 'blight', 'away'],\n",
       " ['theory',\n",
       "  'make',\n",
       "  'way',\n",
       "  'sense',\n",
       "  'say',\n",
       "  'dwarf',\n",
       "  'actual',\n",
       "  'origin',\n",
       "  'blight'],\n",
       " ['palestinian',\n",
       "  'refugee',\n",
       "  'tragedy',\n",
       "  'blight',\n",
       "  'humanity',\n",
       "  'shame',\n",
       "  'israeli',\n",
       "  'live'],\n",
       " ['article',\n",
       "  'michael',\n",
       "  'jackman',\n",
       "  'metro',\n",
       "  'times',\n",
       "  'detroit',\n",
       "  'group',\n",
       "  'later',\n",
       "  'downgrade',\n",
       "  'estimate',\n",
       "  'square',\n",
       "  'mile'],\n",
       " ['locksmithingart', 'respect', 'elaboration', 'blight', 'lock', 'lpdkl'],\n",
       " ['cunt'],\n",
       " ['win', 'ranked', 'play', 'give', 'special', 'card', 'cool', 'long', 'teu'],\n",
       " ['amazon', 'deal', 'wait', 'buy'],\n",
       " ['new', 'print', 'available', 'wait', 'long', 'pamela', 'blizzard'],\n",
       " ['free', 'art', 'kamon'],\n",
       " ['order',\n",
       "  'blizzard',\n",
       "  'pay',\n",
       "  'nut',\n",
       "  'ball',\n",
       "  'flavor',\n",
       "  'boom',\n",
       "  'free',\n",
       "  'ice',\n",
       "  'cream'],\n",
       " ['blizzard', 'auz', 'pm', 'cst', 'hrs', 'rock', 'hump', 'day', 'complete'],\n",
       " ['want',\n",
       "  'rolo',\n",
       "  'blizzard',\n",
       "  'mom',\n",
       "  'say',\n",
       "  'no',\n",
       "  'guess',\n",
       "  'no',\n",
       "  'dq',\n",
       "  'tonight'],\n",
       " ['hi',\n",
       "  'ashayo',\n",
       "  'believe',\n",
       "  'vod',\n",
       "  'youtube',\n",
       "  'presentation',\n",
       "  'like',\n",
       "  'see',\n",
       "  'live'],\n",
       " ['stat'],\n",
       " ['tweet4taiji',\n",
       "  'dolphin',\n",
       "  'worship',\n",
       "  'group',\n",
       "  'base',\n",
       "  'superstition',\n",
       "  'look',\n",
       "  'tweet'],\n",
       " ['lucio',\n",
       "  'let',\n",
       "  'overwatch',\n",
       "  'hype',\n",
       "  'train',\n",
       "  'roll',\n",
       "  'caution',\n",
       "  'not',\n",
       "  'break'],\n",
       " ['ight'],\n",
       " ['peanut', 'butter', 'cookie', 'dough', 'blizzard'],\n",
       " ['mic', 'controller', 'not', 'work', 'second'],\n",
       " ['tomorrow', 'announcement', 'vod'],\n",
       " ['update', 'window', 'error'],\n",
       " ['new', 'expansion', 'idea', 'bard', 'class', 'holy', 'trinity'],\n",
       " ['not',\n",
       "  'shocking',\n",
       "  'blizzard',\n",
       "  'lure',\n",
       "  'old',\n",
       "  'fanbase',\n",
       "  'wod',\n",
       "  'disappoint',\n",
       "  'hardcore',\n",
       "  'everyone',\n",
       "  'leave'],\n",
       " ['good', 'thing', 'dq', 'cotton', 'candy', 'blizzard'],\n",
       " ['lizard', 'wizard', 'blizzard', 'lwb'],\n",
       " ['need'],\n",
       " ['blizzard', 'love'],\n",
       " ['blizzard', 'clutch', 'asf'],\n",
       " ['like', 'link'],\n",
       " ['ok', 'ok', 'okayyyyyy', 'be', 'act', 'right', 'bout', 'blizzard', 'tho'],\n",
       " ['control', 'tora'],\n",
       " ['time', 'play', 'hearthstone', 'pc', 'thought'],\n",
       " ['love', 'cotton', 'candy', 'blizzard'],\n",
       " ['hard', 'reset', 'xbox'],\n",
       " ['want', 'brownie', 'batter', 'blizzard'],\n",
       " ['little', 'bit', 'blizzard'],\n",
       " ['market',\n",
       "  'news',\n",
       "  'activision',\n",
       "  'blizzard',\n",
       "  'cognizant',\n",
       "  'technology',\n",
       "  'solar'],\n",
       " ['sean', 'end', 'career', 'sg', 'blizzard', 'vs', 'knockout'],\n",
       " ['big', 'regret', 'hearthstone'],\n",
       " ['walk', 'dq', 'want', 'butterfinger', 'blizzard', 'bad'],\n",
       " ['horrible',\n",
       "  'moment',\n",
       "  'open',\n",
       "  'dryer',\n",
       "  'look',\n",
       "  'like',\n",
       "  'snowy',\n",
       "  'blizzard',\n",
       "  'cuz',\n",
       "  'leave',\n",
       "  'piece',\n",
       "  'paper',\n",
       "  'jean',\n",
       "  'pocket'],\n",
       " ['bruh', 'die'],\n",
       " ['rip', 'blood'],\n",
       " ['nah', 'young', 'blood', 'cook', 'go', 'cut', 'haha'],\n",
       " ['wall', 'invincible', 'history', 'blood', 'dance', 'floor'],\n",
       " ['break',\n",
       "  'nailreal',\n",
       "  'not',\n",
       "  'fake',\n",
       "  'morning',\n",
       "  'blood',\n",
       "  'ah',\n",
       "  'hurt',\n",
       "  'idea',\n",
       "  'treat',\n",
       "  'help',\n",
       "  'pretty'],\n",
       " ['star',\n",
       "  'reviewer',\n",
       "  'dragon',\n",
       "  'blood',\n",
       "  'boxset',\n",
       "  'lindsay',\n",
       "  'buroker',\n",
       "  'kindle'],\n",
       " ['blood',\n",
       "  'group',\n",
       "  've',\n",
       "  'associate',\n",
       "  'gastric',\n",
       "  'carcinoma',\n",
       "  'say',\n",
       "  'text',\n",
       "  'bookanother',\n",
       "  'fragile',\n",
       "  'gene',\n",
       "  'body'],\n",
       " ['friend', 'like', 'blood', 'not', 'come', 'wound'],\n",
       " ['go', 'blood'],\n",
       " ['people', 'tattoo', 'allow', 'donate', 'blood', 'receive', 'blood', 'not'],\n",
       " ['dialyses', 'grandpa', 'oh', 'lord', 'blood', 'make', 'light', 'head'],\n",
       " ['dad',\n",
       "  'not',\n",
       "  'claim',\n",
       "  'mean',\n",
       "  'not',\n",
       "  'right',\n",
       "  'look',\n",
       "  'eye',\n",
       "  'blood',\n",
       "  'xbox'],\n",
       " ['omron',\n",
       "  'hem712c',\n",
       "  'automatic',\n",
       "  'blood',\n",
       "  'pressure',\n",
       "  'monitor',\n",
       "  'standard',\n",
       "  'large',\n",
       "  'bp',\n",
       "  'cuff'],\n",
       " ['blood', 'pressure', 'roof', 'not', 'need', 'extra', 'shit'],\n",
       " ['yeah', 'speakingfromexperience'],\n",
       " ['not',\n",
       "  'believe',\n",
       "  'people',\n",
       "  'mid',\n",
       "  '20',\n",
       "  'not',\n",
       "  'high',\n",
       "  'blood',\n",
       "  'pressure',\n",
       "  'life',\n",
       "  'stressful',\n",
       "  'decisionsondecision'],\n",
       " ['guy', 'imouto', 'not', 'actually', 'relate', 'blood'],\n",
       " ['bruh',\n",
       "  'white',\n",
       "  'people',\n",
       "  'buy',\n",
       "  'ugly',\n",
       "  'shoe',\n",
       "  'super',\n",
       "  'tight',\n",
       "  'no',\n",
       "  'blood',\n",
       "  'go',\n",
       "  'foot'],\n",
       " ['day',\n",
       "  'excellent',\n",
       "  'porridge',\n",
       "  'seriously',\n",
       "  'people',\n",
       "  'blood',\n",
       "  'orange',\n",
       "  'porridge',\n",
       "  'phenomenal'],\n",
       " ['happy', 'birthday', 'young', 'blood'],\n",
       " ['not', 'blood'],\n",
       " ['no', 'kind', 'shii', 'nasty', 'blood', 'no', 'pun', 'intend'],\n",
       " ['not', 'no', 'hoe', 'blood'],\n",
       " ['today',\n",
       "  'hastle',\n",
       "  'get',\n",
       "  'drug',\n",
       "  'test',\n",
       "  'blood',\n",
       "  'drown',\n",
       "  'tb',\n",
       "  'shoot',\n",
       "  'document',\n",
       "  'fill'],\n",
       " ['man',\n",
       "  'somebody',\n",
       "  'get',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'sbee',\n",
       "  'dude',\n",
       "  'fuckin',\n",
       "  'funny',\n",
       "  'blood'],\n",
       " ['wind', 'gypsy', 'blood', 'time'],\n",
       " ['mad', 'man'],\n",
       " ['private', 'thirsty', 'nightsad', 'blood', 'rockn', 'roll'],\n",
       " ['rolandonabeats',\n",
       "  'ellie',\n",
       "  'goulding',\n",
       "  'blood',\n",
       "  'acesse',\n",
       "  'nosso',\n",
       "  'site',\n",
       "  'para',\n",
       "  'ouvir'],\n",
       " ['welcome',\n",
       "  'read',\n",
       "  'free',\n",
       "  'chapter',\n",
       "  'new',\n",
       "  'book',\n",
       "  'encounter',\n",
       "  'jesus',\n",
       "  'hope'],\n",
       " ['run', 'blood'],\n",
       " ['personal',\n",
       "  'favorite',\n",
       "  'include',\n",
       "  'paramore',\n",
       "  'muse',\n",
       "  'green',\n",
       "  'day',\n",
       "  'royal',\n",
       "  'blood'],\n",
       " ['innocent',\n",
       "  'blood',\n",
       "  'son',\n",
       "  'daughter',\n",
       "  'land',\n",
       "  'polluted',\n",
       "  'psalm',\n",
       "  'help',\n",
       "  'stop',\n",
       "  'sin',\n",
       "  'abortion'],\n",
       " ['se', 'pone', 'cantar', 'cry', 'lightning'],\n",
       " ['not', 'big', 'stab', 'deep', 'stab', 'like', 'blood', 'everwhe'],\n",
       " ['add', 'item', 'everyday', 'eating', 'habit', 'research', 'bloodu'],\n",
       " ['looooooooooool', 'bloody', 'hell'],\n",
       " ['infected', 'bloody', 'ear', 'piercing', 'fun'],\n",
       " ['aggressif', 'bloody', 'aggressive'],\n",
       " ['enter',\n",
       "  'win',\n",
       "  'entire',\n",
       "  'set',\n",
       "  'butterlondon',\n",
       "  'lip',\n",
       "  'crayon',\n",
       "  'enter',\n",
       "  'bblogger'],\n",
       " ['hey', 'sally', 'sorry', 'email', 'awol', 'bloody', 'work', 'argh'],\n",
       " ['listen', 'bloody', 'jay'],\n",
       " ['forrest', 'version', 'bloody', 'awful', 'xxx'],\n",
       " ['nightmare', 'elm', 'street', 'get', 'remade'],\n",
       " ['not',\n",
       "  'bloody',\n",
       "  'wait',\n",
       "  'sony',\n",
       "  'set',\n",
       "  'date',\n",
       "  'stephen',\n",
       "  'kinguas',\n",
       "  'youthe',\n",
       "  'dark',\n",
       "  'towerua',\n",
       "  'stephenking',\n",
       "  'thedarktower'],\n",
       " ['get', 'to', 'try', 'let', 'bloody', 'thing'],\n",
       " ['russia',\n",
       "  'play',\n",
       "  'reason',\n",
       "  'link',\n",
       "  'bs',\n",
       "  'okanowa',\n",
       "  'bloody',\n",
       "  'mainline',\n",
       "  'invasion',\n",
       "  'look',\n",
       "  'like',\n",
       "  'bloody'],\n",
       " ['bloody', 'insomnia', 'grrrr', 'insomnia'],\n",
       " ['try',\n",
       "  'eye',\n",
       "  'akame',\n",
       "  'ga',\n",
       "  'kill',\n",
       "  'tokyo',\n",
       "  'ghoul',\n",
       "  'damn',\n",
       "  'bloody',\n",
       "  'not',\n",
       "  'dare',\n",
       "  'watch'],\n",
       " ['suck', 'bloody', 'get', 'mean', 'amend'],\n",
       " ['weekend', 'bloody', 'mary', 'times', 'summer', 'newu'],\n",
       " ['bloody', 'hell', 'day', 'not', 'tired', 'think', 'vaca', 'help'],\n",
       " ['damn', 'bloody', 'hot'],\n",
       " ['bloody', 'sexy', 'drool'],\n",
       " ['know',\n",
       "  'effect',\n",
       "  'low',\n",
       "  'fast',\n",
       "  'son',\n",
       "  'product',\n",
       "  'acne',\n",
       "  'cream',\n",
       "  'effect',\n",
       "  'bloody',\n",
       "  'diarrhea'],\n",
       " ['ronda',\n",
       "  'rousey',\n",
       "  'close',\n",
       "  'make',\n",
       "  'floyd',\n",
       "  'mayweathers',\n",
       "  'money',\n",
       "  'fight',\n",
       "  'bloody',\n",
       "  'elbow',\n",
       "  'boxing'],\n",
       " ['awful', 'painting', 'agree', 'a3', 'landscape', 'bloody', 'oil', 'paint'],\n",
       " ['need',\n",
       "  'life',\n",
       "  'sin',\n",
       "  'girlfriend',\n",
       "  'ride',\n",
       "  'till',\n",
       "  'bloody',\n",
       "  'end',\n",
       "  'girlfriend'],\n",
       " ['think', 'bloody', 'barking'],\n",
       " ['eh',\n",
       "  'hello',\n",
       "  'cover',\n",
       "  'bloody',\n",
       "  'thigh',\n",
       "  'bloody',\n",
       "  'cleav',\n",
       "  'uo',\n",
       "  'eh',\n",
       "  'hello',\n",
       "  'expose',\n",
       "  'cleavage'],\n",
       " ['mega', 'bloody', 'marvellous'],\n",
       " ['bloody', 'mary', 'sink', 'beet', 'juice'],\n",
       " ['monty', 'python', 'date', 'bloody', 'far', 'want'],\n",
       " ['meet', 'bloody', 'rs5'],\n",
       " ['friday', 'suppose', 'happy', 'day', 'bloody', 'friday', 'hah', 'zzzz'],\n",
       " ['bloody', 'hope', 'say', 'fold', 'arm', 'sit', 'chair'],\n",
       " ['wait',\n",
       "  'tell',\n",
       "  'college',\n",
       "  'friend',\n",
       "  'reafs',\n",
       "  'bloody',\n",
       "  'mary',\n",
       "  'drama',\n",
       "  'cd'],\n",
       " ['come',\n",
       "  'kill',\n",
       "  'indiansfor',\n",
       "  'fun',\n",
       "  'video',\n",
       "  'smirking',\n",
       "  'remorseless',\n",
       "  'pakistani',\n",
       "  'killer',\n",
       "  'show',\n",
       "  'boast'],\n",
       " ['watch',\n",
       "  'episode',\n",
       "  's01e09',\n",
       "  'bloody',\n",
       "  'monday',\n",
       "  'bloodymonday',\n",
       "  'tvshowtime'],\n",
       " ['marlon',\n",
       "  'williams',\n",
       "  'elvis',\n",
       "  'presley',\n",
       "  'marlon',\n",
       "  'williams',\n",
       "  'steel',\n",
       "  'panther',\n",
       "  'shuffle',\n",
       "  'mode',\n",
       "  'like',\n",
       "  'bloody',\n",
       "  'legend'],\n",
       " ['black', 'friday', 'turn', 'bloody', 'shopping', 'mystery'],\n",
       " ['tell',\n",
       "  'soccer',\n",
       "  'mom',\n",
       "  'get',\n",
       "  'pic',\n",
       "  'blow',\n",
       "  'daughter',\n",
       "  'bedroom',\n",
       "  'wall',\n",
       "  'not'],\n",
       " ['s2',\n",
       "  'g',\n",
       "  'mention',\n",
       "  'blow',\n",
       "  'choice',\n",
       "  'deactivate',\n",
       "  'leave',\n",
       "  'good',\n",
       "  'respect',\n",
       "  'choice'],\n",
       " ['wow', 'wow', 'club', 'blow', 'money', 'fuck', 'player', 'like', 'pillory'],\n",
       " ['damn', 'greinke', 'get', 'blow', 'inning'],\n",
       " ['wake', 'blow', 'lol'],\n",
       " ['aunt', 'marge', 'blow', 'ehek'],\n",
       " ['yeah',\n",
       "  'not',\n",
       "  'worth',\n",
       "  'bc',\n",
       "  'spammer',\n",
       "  'twitter',\n",
       "  'probably',\n",
       "  'blow',\n",
       "  'second'],\n",
       " ['white',\n",
       "  'family',\n",
       "  'supposedly',\n",
       "  'represent',\n",
       "  'americas',\n",
       "  'great',\n",
       "  'value',\n",
       "  'get',\n",
       "  'blow',\n",
       "  'horrible',\n",
       "  'cgi',\n",
       "  'nuclear',\n",
       "  'strike',\n",
       "  'lmfaoooo'],\n",
       " ['man', 'not', 'blow', 'underground'],\n",
       " ['mention', 'blow', 'wtf'],\n",
       " ['thisdayinhistory', 'confederate', 'ship', 'blow', 'crew', 'read'],\n",
       " ['moment',\n",
       "  'ur',\n",
       "  'win',\n",
       "  'mini',\n",
       "  'uhc',\n",
       "  'blow',\n",
       "  'creeper',\n",
       "  'kick',\n",
       "  'fly',\n",
       "  'salty',\n",
       "  'rn'],\n",
       " ['bump', 'approval', 'probably', 'blow', 'musicadvisory'],\n",
       " ['truckload',\n",
       "  'soldier',\n",
       "  'blow',\n",
       "  'panic',\n",
       "  'little',\n",
       "  'lion',\n",
       "  'die',\n",
       "  'lose',\n",
       "  'mind'],\n",
       " ['hey',\n",
       "  'look',\n",
       "  'kashs',\n",
       "  'foundation',\n",
       "  'live',\n",
       "  'today',\n",
       "  'get',\n",
       "  'blow',\n",
       "  'people',\n",
       "  'magazines',\n",
       "  'website',\n",
       "  'todd',\n",
       "  'blake'],\n",
       " ['get', 'month', 'go', 'ee', 'shop', 'glad', 'not', 'blow'],\n",
       " ['time', 'tweet', 'blow', 'half', 'day', 'later'],\n",
       " ['weuare', 'blow', 'away', 'extension', 'weuave', 'see', 'option'],\n",
       " ['thank', 'let', 'stay', 'manor', 'blow', 'anyways', 'buddy'],\n",
       " ['loner',\n",
       "  'diarie',\n",
       "  'pattern',\n",
       "  'sand',\n",
       "  'blow',\n",
       "  'away',\n",
       "  'photo',\n",
       "  'choke',\n",
       "  'flame'],\n",
       " ['bsg',\n",
       "  'sufficiently',\n",
       "  'hype',\n",
       "  'year',\n",
       "  'delay',\n",
       "  'watch',\n",
       "  'utterly',\n",
       "  'utterly',\n",
       "  'blow',\n",
       "  'away'],\n",
       " ['vanessas',\n",
       "  'game',\n",
       "  'officially',\n",
       "  'blow',\n",
       "  'lady',\n",
       "  'gentlementhe',\n",
       "  'real',\n",
       "  'begin',\n",
       "  'bb17'],\n",
       " ['guarantee',\n",
       "  'bite',\n",
       "  'mutant',\n",
       "  'mosquito',\n",
       "  'ankle',\n",
       "  'blow',\n",
       "  'little',\n",
       "  'cunt'],\n",
       " ['thing',\n",
       "  'pick',\n",
       "  'tozlet',\n",
       "  'seat',\n",
       "  'butt',\n",
       "  'leprosy',\n",
       "  'fullblown',\n",
       "  'jerkface',\n",
       "  'syndrome',\n",
       "  'lateral',\n",
       "  'lisp',\n",
       "  'toilet',\n",
       "  'ricket'],\n",
       " ['ig', 'blow', 'hacker', 'need', 'stop', 'givebackkalinwhiteaccount'],\n",
       " ['direct',\n",
       "  'video',\n",
       "  'need',\n",
       "  'grab',\n",
       "  'nicki',\n",
       "  'minaj',\n",
       "  'recognition',\n",
       "  'mind',\n",
       "  'blow'],\n",
       " ['week',\n",
       "  'want',\n",
       "  '6th',\n",
       "  'sense',\n",
       "  'blow',\n",
       "  'far',\n",
       "  'good',\n",
       "  'james',\n",
       "  'win',\n",
       "  'huge',\n",
       "  'target',\n",
       "  'go',\n",
       "  'soon'],\n",
       " ['like', 'blow', 'amirite'],\n",
       " ['doguas', 'blow', 'kennel', 'uo', 'bloody', 'yorkshire', 'terrorist'],\n",
       " ['turn', 'espn2', 'blow'],\n",
       " ['go',\n",
       "  'beat',\n",
       "  'blow',\n",
       "  'thing',\n",
       "  'know',\n",
       "  'ra',\n",
       "  'al',\n",
       "  'ghul',\n",
       "  'bring',\n",
       "  'life',\n",
       "  'escape'],\n",
       " ['leos',\n",
       "  'ass',\n",
       "  'get',\n",
       "  'metaphorically',\n",
       "  'blow',\n",
       "  'piperwearsthepants',\n",
       "  'charm'],\n",
       " ['glononium',\n",
       "  '6c',\n",
       "  'help',\n",
       "  'blow',\n",
       "  'bash',\n",
       "  'bottle',\n",
       "  'nitroglycerin',\n",
       "  'book'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'woman',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'fashion',\n",
       "  'purse',\n",
       "  'reu'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'woman',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'fashion',\n",
       "  'purse',\n",
       "  'reu'],\n",
       " ['son',\n",
       "  'daughter',\n",
       "  'like',\n",
       "  'go',\n",
       "  'war',\n",
       "  'iran',\n",
       "  'come',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'let',\n",
       "  'republicans',\n",
       "  'know'],\n",
       " ['ditto', 'way', 'feel', 'drink', 'vodka', 'ice', 'body', 'bag'],\n",
       " ['handbag',\n",
       "  'genuine',\n",
       "  'mulberry',\n",
       "  'antony',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'messenger',\n",
       "  'bag',\n",
       "  'dark',\n",
       "  'oak',\n",
       "  'soft',\n",
       "  'buffalo',\n",
       "  'leather',\n",
       "  'date',\n",
       "  'w'],\n",
       " ['louis',\n",
       "  'vuitton',\n",
       "  'monogram',\n",
       "  'sophie',\n",
       "  'limited',\n",
       "  'edition',\n",
       "  'clutch',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['louis',\n",
       "  'vuitton',\n",
       "  'monogram',\n",
       "  'sophie',\n",
       "  'limited',\n",
       "  'edition',\n",
       "  'clutch',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['louis',\n",
       "  'vuitton',\n",
       "  'cultsierre',\n",
       "  'monogram',\n",
       "  'shoulder',\n",
       "  'bag',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag'],\n",
       " ['check',\n",
       "  'ameribag',\n",
       "  'healthy',\n",
       "  'bag',\n",
       "  'shoulder',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'backpack',\n",
       "  'khaki',\n",
       "  'tan',\n",
       "  'beige',\n",
       "  'nylon'],\n",
       " ['photo',\n",
       "  'bath',\n",
       "  'body',\n",
       "  'work',\n",
       "  'cosmetic',\n",
       "  'bag',\n",
       "  'periwinkle',\n",
       "  'blue',\n",
       "  'copper',\n",
       "  'piping',\n",
       "  'corner'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'woman',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'fashion',\n",
       "  'purse',\n",
       "  'reu'],\n",
       " ['new',\n",
       "  'summer',\n",
       "  'long',\n",
       "  'thin',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'hip',\n",
       "  'word',\n",
       "  'skirt',\n",
       "  'blue'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'hobo',\n",
       "  'purse',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'womens',\n",
       "  'rt',\n",
       "  'enu'],\n",
       " ['bruh',\n",
       "  'want',\n",
       "  'fight',\n",
       "  'meet',\n",
       "  'cage',\n",
       "  'bro',\n",
       "  'better',\n",
       "  'find',\n",
       "  'deal',\n",
       "  'end',\n",
       "  'body',\n",
       "  'bag'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'hobo',\n",
       "  'purse',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'womens',\n",
       "  'readu'],\n",
       " ['auth',\n",
       "  'louis',\n",
       "  'vuitton',\n",
       "  'brown',\n",
       "  'saumur',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'shoulder',\n",
       "  'bag',\n",
       "  'monogram',\n",
       "  'readu'],\n",
       " ['check',\n",
       "  'vintage',\n",
       "  'longaberger',\n",
       "  'floral',\n",
       "  'fabric',\n",
       "  'shoulder',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'brown',\n",
       "  'leather',\n",
       "  'strap'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'hobo',\n",
       "  'purse',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'womens'],\n",
       " ['new',\n",
       "  'summer',\n",
       "  'long',\n",
       "  'thin',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'hip',\n",
       "  'word',\n",
       "  'skirt',\n",
       "  'blue'],\n",
       " ['iu',\n",
       "  'new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'hobo',\n",
       "  'purse',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'womens',\n",
       "  'rt',\n",
       "  'enu'],\n",
       " ['louis',\n",
       "  'vuitton',\n",
       "  'monogram',\n",
       "  'sophie',\n",
       "  'limited',\n",
       "  'edition',\n",
       "  'clutch',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['new',\n",
       "  'woman',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'large',\n",
       "  'satchel',\n",
       "  'reu'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'hobo',\n",
       "  'purse',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'womens'],\n",
       " ['day', 'heart', 'go', 'zip', 'body', 'bag'],\n",
       " ['handbag',\n",
       "  'fashion',\n",
       "  'style',\n",
       "  'authentic',\n",
       "  'louis',\n",
       "  'vuitton',\n",
       "  'pochette',\n",
       "  'bosphore',\n",
       "  'shoulder',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bagu'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'hobo',\n",
       "  'purse',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'womens'],\n",
       " ['handbag',\n",
       "  'fashion',\n",
       "  'style',\n",
       "  'vintage',\n",
       "  'coach',\n",
       "  'purse',\n",
       "  'camera',\n",
       "  'bag',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bid',\n",
       "  'u'],\n",
       " ['genuine',\n",
       "  'leather',\n",
       "  'man',\n",
       "  'bag',\n",
       "  'messenger',\n",
       "  'fit',\n",
       "  'ipad',\n",
       "  'mini',\n",
       "  'tablet',\n",
       "  'case',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'air',\n",
       "  'jp',\n",
       "  'reau'],\n",
       " ['louis',\n",
       "  'vuitton',\n",
       "  'monogram',\n",
       "  'sophie',\n",
       "  'limited',\n",
       "  'edition',\n",
       "  'clutch',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'read',\n",
       "  'ebay'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'hobo',\n",
       "  'purse',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'womens'],\n",
       " ['nuu', 'fam', 'fwt', 'leave', 'body', 'bag'],\n",
       " ['new',\n",
       "  'lady',\n",
       "  'shoulder',\n",
       "  'tote',\n",
       "  'handbag',\n",
       "  'faux',\n",
       "  'leather',\n",
       "  'hobo',\n",
       "  'purse',\n",
       "  'cross',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'womens',\n",
       "  'readu'],\n",
       " ['no', 'not', 'say', 'lucky', 'home', 'family', 'not', 'body', 'bag'],\n",
       " ['oh', 'not', 'bag', 'body', 'bangin', 'say', 'go', 'rose'],\n",
       " ['bag', 'body', 'smoke', 'hot'],\n",
       " ['drake', 'body', 'bag', 'meek'],\n",
       " ['drake', 'kill', 'dude', 'tea', 'bag', 'dead', 'body', 'point'],\n",
       " ['wreck',\n",
       "  'league',\n",
       "  'ball',\n",
       "  'olympic',\n",
       "  'level',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'like',\n",
       "  'career',\n",
       "  'trollingtilmeekdiss'],\n",
       " ['idgaf',\n",
       "  'tough',\n",
       "  'canada',\n",
       "  'north',\n",
       "  'philly',\n",
       "  'meek',\n",
       "  'act',\n",
       "  'like',\n",
       "  'bitch',\n",
       "  'drake',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'ass',\n",
       "  'track'],\n",
       " ['body', 'bag', 'mfs'],\n",
       " ['skim', 'twitter', 'miss', 'body', 'bag'],\n",
       " ['g', 'body', 'bag', 'lyrically'],\n",
       " ['straight', 'body', 'bag'],\n",
       " ['body', 'bag', 'bitch'],\n",
       " ['no',\n",
       "  'well',\n",
       "  'feeling',\n",
       "  'see',\n",
       "  'stage',\n",
       "  'day',\n",
       "  'one',\n",
       "  'year',\n",
       "  'friendship',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'mic'],\n",
       " ['mopheme',\n",
       "  'bigstar',\n",
       "  'johnson',\n",
       "  'problem',\n",
       "  'game',\n",
       "  'body',\n",
       "  'bag',\n",
       "  'niggas',\n",
       "  'vuzuhustle'],\n",
       " ['body', 'bag', 'today4got', 'post', 'score'],\n",
       " ['let', 'bagging', 'bodys', 'begin', 'lol', 'not', 'cuff', 'not', 'bad'],\n",
       " ...]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:41:48: collecting all words and their counts\n",
      "INFO - 19:41:48: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 19:41:48: collected 12881 word types from a corpus of 61632 raw words and 7613 sentences\n",
      "INFO - 19:41:48: Loading a fresh vocabulary\n",
      "INFO - 19:41:48: effective_min_count=1 retains 12881 unique words (100% of original 12881, drops 0)\n",
      "INFO - 19:41:48: effective_min_count=1 leaves 61632 word corpus (100% of original 61632, drops 0)\n",
      "INFO - 19:41:48: deleting the raw counts dictionary of 12881 items\n",
      "INFO - 19:41:48: sample=0.001 downsamples 13 most-common words\n",
      "INFO - 19:41:48: downsampling leaves estimated 60518 word corpus (98.2% of prior 61632)\n",
      "INFO - 19:41:48: estimated required memory for 12881 words and 100 dimensions: 16745300 bytes\n",
      "INFO - 19:41:48: resetting layer weights\n",
      "INFO - 19:41:48: training model with 4 workers on 12881 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:41:48: EPOCH - 1 : training on 61632 raw words (60518 effective words) took 0.1s, 976905 effective words/s\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:41:48: EPOCH - 2 : training on 61632 raw words (60538 effective words) took 0.1s, 1059208 effective words/s\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:41:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:41:48: EPOCH - 3 : training on 61632 raw words (60526 effective words) took 0.1s, 1083020 effective words/s\n",
      "INFO - 19:41:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:41:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:41:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:41:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:41:49: EPOCH - 4 : training on 61632 raw words (60522 effective words) took 0.1s, 1150709 effective words/s\n",
      "INFO - 19:41:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:41:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:41:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:41:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:41:49: EPOCH - 5 : training on 61632 raw words (60499 effective words) took 0.1s, 1191568 effective words/s\n",
      "INFO - 19:41:49: training on a 308160 raw words (302603 effective words) took 0.4s, 845505 effective words/s\n",
      "WARNING - 19:41:49: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "#from gensim.models.phrases import Phrases, Phraser\n",
    "#phrases = Phrases(words_list, min_count=30, progress_per=10000)\n",
    "model = Word2Vec(words_list, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:33:50: collecting all words and their counts\n",
      "INFO - 19:33:50: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 19:33:50: collected 12881 word types from a corpus of 61632 raw words and 7613 sentences\n",
      "INFO - 19:33:50: Loading a fresh vocabulary\n",
      "INFO - 19:33:50: effective_min_count=1 retains 12881 unique words (100% of original 12881, drops 0)\n",
      "INFO - 19:33:50: effective_min_count=1 leaves 61632 word corpus (100% of original 61632, drops 0)\n",
      "INFO - 19:33:50: deleting the raw counts dictionary of 12881 items\n",
      "INFO - 19:33:50: sample=0.001 downsamples 13 most-common words\n",
      "INFO - 19:33:50: downsampling leaves estimated 60518 word corpus (98.2% of prior 61632)\n",
      "INFO - 19:33:50: estimated required memory for 12881 words and 100 dimensions: 16745300 bytes\n",
      "INFO - 19:33:50: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "#bigram = Phraser(phrases)\n",
    "#sentences = bigram[words_list]\n",
    "model.build_vocab(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:45:54: saving Word2VecKeyedVectors object under vectors.kv, separately None\n",
      "INFO - 19:45:54: not storing attribute vectors_norm\n",
      "INFO - 19:45:54: saved vectors.kv\n",
      "INFO - 19:45:54: loading Word2VecKeyedVectors object from vectors.kv\n",
      "INFO - 19:45:54: setting ignored attribute vectors_norm to None\n",
      "INFO - 19:45:54: loaded vectors.kv\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fname = \"vectors.kv\"\n",
    "word_vectors.save(fname)\n",
    "word_vectors = KeyedVectors.load(fname, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:47:15: loading projection weights from C:\\Users\\akash\\Anaconda3\\lib\\site-packages\\gensim\\test\\test_data\\word2vec_pre_kv_c\n",
      "INFO - 19:47:15: loaded (1750, 10) matrix from C:\\Users\\akash\\Anaconda3\\lib\\site-packages\\gensim\\test\\test_data\\word2vec_pre_kv_c\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9288\n"
     ]
    }
   ],
   "source": [
    "sim = word_vectors.n_similarity(['ablaze'], ['sky'])\n",
    "print(\"{:.4f}\".format(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12891"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'deed': 3,\n",
       "             'reason': 32,\n",
       "             'earthquake': 54,\n",
       "             'allah': 10,\n",
       "             'forgive': 6,\n",
       "             'forest': 67,\n",
       "             'fire': 360,\n",
       "             'near': 57,\n",
       "             'la': 26,\n",
       "             'ronge': 2,\n",
       "             'sask': 2,\n",
       "             'canada': 12,\n",
       "             'resident': 9,\n",
       "             'ask': 30,\n",
       "             'shelter': 8,\n",
       "             'place': 37,\n",
       "             'notify': 2,\n",
       "             'officer': 38,\n",
       "             'no': 255,\n",
       "             'evacuation': 55,\n",
       "             'order': 40,\n",
       "             'expect': 33,\n",
       "             'people': 200,\n",
       "             'receive': 5,\n",
       "             'wildfire': 80,\n",
       "             'california': 73,\n",
       "             'get': 213,\n",
       "             'send': 53,\n",
       "             'photo': 63,\n",
       "             'ruby': 2,\n",
       "             'alaska': 6,\n",
       "             'smoke': 53,\n",
       "             'pour': 5,\n",
       "             'school': 72,\n",
       "             'rockyfire': 5,\n",
       "             'update': 54,\n",
       "             ' ': 1553,\n",
       "             'hwy': 10,\n",
       "             'close': 36,\n",
       "             'direction': 16,\n",
       "             'lake': 17,\n",
       "             'county': 39,\n",
       "             'cafire': 3,\n",
       "             'flood': 143,\n",
       "             'disaster': 157,\n",
       "             'heavy': 22,\n",
       "             'rain': 60,\n",
       "             'cause': 57,\n",
       "             'flash': 22,\n",
       "             'flooding': 30,\n",
       "             'street': 35,\n",
       "             'manitou': 2,\n",
       "             'colorado': 17,\n",
       "             'spring': 16,\n",
       "             'area': 50,\n",
       "             'hill': 12,\n",
       "             'wood': 7,\n",
       "             'emergency': 159,\n",
       "             'happen': 54,\n",
       "             'building': 77,\n",
       "             'afraid': 6,\n",
       "             'tornado': 38,\n",
       "             'come': 161,\n",
       "             'die': 77,\n",
       "             'heat': 46,\n",
       "             'wave': 74,\n",
       "             'far': 29,\n",
       "             'haha': 21,\n",
       "             'south': 29,\n",
       "             'tampa': 7,\n",
       "             'hah': 4,\n",
       "             'wait': 39,\n",
       "             'second': 35,\n",
       "             'live': 89,\n",
       "             'go': 225,\n",
       "             'fvck': 2,\n",
       "             'florida': 7,\n",
       "             'tampabay': 2,\n",
       "             'day': 139,\n",
       "             'lose': 41,\n",
       "             'count': 3,\n",
       "             'bago': 4,\n",
       "             'myanmar': 19,\n",
       "             'arrive': 19,\n",
       "             'damage': 69,\n",
       "             'bus': 49,\n",
       "             'multi': 3,\n",
       "             'car': 114,\n",
       "             'crash': 163,\n",
       "             'breaking': 16,\n",
       "             'man': 138,\n",
       "             'love': 128,\n",
       "             'fruit': 9,\n",
       "             'summer': 44,\n",
       "             'lovely': 9,\n",
       "             'fast': 24,\n",
       "             'goooooooaaaaaal': 2,\n",
       "             'ridiculous': 5,\n",
       "             'london': 16,\n",
       "             'cool': 36,\n",
       "             'skiing': 2,\n",
       "             'wonderful': 6,\n",
       "             'looooool': 2,\n",
       "             'wayi': 2,\n",
       "             'not': 811,\n",
       "             'eat': 14,\n",
       "             'shit': 57,\n",
       "             'nyc': 13,\n",
       "             'week': 45,\n",
       "             'girlfriend': 7,\n",
       "             'cooool': 2,\n",
       "             'like': 338,\n",
       "             'pasta': 3,\n",
       "             'end': 58,\n",
       "             'wholesale': 5,\n",
       "             'market': 40,\n",
       "             'ablaze': 41,\n",
       "             'try': 65,\n",
       "             'bring': 40,\n",
       "             'metal': 14,\n",
       "             'rt': 109,\n",
       "             'africanbaze': 2,\n",
       "             'break': 61,\n",
       "             'newsnigeria': 2,\n",
       "             'flag': 26,\n",
       "             'set': 69,\n",
       "             'aba': 15,\n",
       "             'cry': 21,\n",
       "             'plus': 9,\n",
       "             'look': 90,\n",
       "             'sky': 19,\n",
       "             'night': 55,\n",
       "             'mufc': 3,\n",
       "             'build': 26,\n",
       "             'hype': 5,\n",
       "             'new': 231,\n",
       "             'acquisition': 3,\n",
       "             'doubt': 6,\n",
       "             'epl': 2,\n",
       "             'season': 18,\n",
       "             'inec': 3,\n",
       "             'office': 16,\n",
       "             'abia': 3,\n",
       "             'barbados': 2,\n",
       "             'bridgetown': 2,\n",
       "             'jamaica': 6,\n",
       "             'uo': 67,\n",
       "             'santa': 7,\n",
       "             'cruz': 11,\n",
       "             'head': 61,\n",
       "             'st': 33,\n",
       "             'elizabeth': 3,\n",
       "             'police': 141,\n",
       "             'superintende': 2,\n",
       "             'lord': 21,\n",
       "             'check': 52,\n",
       "             'nsfw': 5,\n",
       "             'outside': 26,\n",
       "             'alive': 12,\n",
       "             'dead': 97,\n",
       "             'inside': 30,\n",
       "             'awesome': 17,\n",
       "             'time': 137,\n",
       "             'visit': 14,\n",
       "             'cfc': 3,\n",
       "             'ancop': 2,\n",
       "             'site': 31,\n",
       "             'thank': 62,\n",
       "             'tita': 2,\n",
       "             'vida': 3,\n",
       "             'take': 69,\n",
       "             'care': 35,\n",
       "             'soooo': 3,\n",
       "             'pumped': 2,\n",
       "             'want': 155,\n",
       "             'chicago': 13,\n",
       "             'preaching': 2,\n",
       "             'hotel': 7,\n",
       "             'gain': 13,\n",
       "             'follower': 11,\n",
       "             'know': 137,\n",
       "             'stat': 5,\n",
       "             'grow': 27,\n",
       "             'west': 26,\n",
       "             'burn': 115,\n",
       "             'thousand': 20,\n",
       "             'perfect': 11,\n",
       "             'tracklist': 2,\n",
       "             'life': 113,\n",
       "             'leave': 71,\n",
       "             'retainer': 2,\n",
       "             'weird': 10,\n",
       "             'well': 24,\n",
       "             'wear': 12,\n",
       "             'single': 15,\n",
       "             'year': 149,\n",
       "             'deputy': 6,\n",
       "             'shot': 21,\n",
       "             'brighton': 2,\n",
       "             'home': 138,\n",
       "             'wife': 12,\n",
       "             'jail': 5,\n",
       "             'niece': 4,\n",
       "             'superintendent': 1,\n",
       "             'lanford': 1,\n",
       "             'salmon': 3,\n",
       "             'r': 19,\n",
       "             'arsonist': 19,\n",
       "             'deliberately': 1,\n",
       "             'black': 67,\n",
       "             'church': 8,\n",
       "             'north': 27,\n",
       "             'carolinaaeablaze': 1,\n",
       "             'noches': 1,\n",
       "             'elbestia': 1,\n",
       "             'happy': 23,\n",
       "             'teammate': 1,\n",
       "             'train': 117,\n",
       "             'hard': 21,\n",
       "             'goodnight': 1,\n",
       "             'gunner': 1,\n",
       "             'kurds': 1,\n",
       "             'trample': 1,\n",
       "             'turkmen': 2,\n",
       "             'later': 11,\n",
       "             'vandalize': 1,\n",
       "             'diyala': 1,\n",
       "             'truck': 54,\n",
       "             'r21': 1,\n",
       "             'voortrekker': 1,\n",
       "             'ave': 15,\n",
       "             'tambo': 1,\n",
       "             'intl': 1,\n",
       "             'cargo': 3,\n",
       "             'section': 5,\n",
       "             'heart': 39,\n",
       "             'city': 67,\n",
       "             'gift': 5,\n",
       "             'skyline': 2,\n",
       "             'kiss': 7,\n",
       "             'lip': 5,\n",
       "             'tonight': 39,\n",
       "             'los': 4,\n",
       "             'angeles': 4,\n",
       "             'ig': 4,\n",
       "             'fb': 3,\n",
       "             'fill': 9,\n",
       "             'sunset': 7,\n",
       "             'peep': 2,\n",
       "             'climate': 19,\n",
       "             'energy': 7,\n",
       "             'revel': 1,\n",
       "             'wmv': 2,\n",
       "             'video': 175,\n",
       "             'mean': 34,\n",
       "             'mac': 7,\n",
       "             'farewell': 1,\n",
       "             'en': 3,\n",
       "             'route': 12,\n",
       "             'dvd': 5,\n",
       "             'gtxrwm': 1,\n",
       "             'progressive': 2,\n",
       "             'greeting': 1,\n",
       "             'month': 24,\n",
       "             'student': 18,\n",
       "             'pen': 1,\n",
       "             'torch': 5,\n",
       "             'publication': 1,\n",
       "             'rene': 2,\n",
       "             'jacinta': 1,\n",
       "             'secret': 24,\n",
       "             '2k13': 1,\n",
       "             'fallen': 1,\n",
       "             'edit': 2,\n",
       "             'mar': 1,\n",
       "             'steve': 8,\n",
       "             'tinderbox': 1,\n",
       "             'clown': 2,\n",
       "             'hood': 2,\n",
       "             'nowplaying': 24,\n",
       "             'ian': 4,\n",
       "             'buff': 2,\n",
       "             'magnitude': 1,\n",
       "             'edm': 11,\n",
       "             'huge': 21,\n",
       "             'talk': 33,\n",
       "             'work': 102,\n",
       "             'kid': 39,\n",
       "             'cuz': 6,\n",
       "             'bicycle': 4,\n",
       "             'accident': 95,\n",
       "             'split': 3,\n",
       "             'testicle': 1,\n",
       "             'impossible': 4,\n",
       "             'michael': 12,\n",
       "             'father': 7,\n",
       "             'i24': 1,\n",
       "             'w': 45,\n",
       "             'nashvilletraffic': 1,\n",
       "             'traffic': 31,\n",
       "             'move': 24,\n",
       "             'm': 33,\n",
       "             'slow': 9,\n",
       "             'usual': 5,\n",
       "             'center': 25,\n",
       "             'lane': 13,\n",
       "             'block': 28,\n",
       "             'santaclara': 1,\n",
       "             'us101': 2,\n",
       "             'nb': 4,\n",
       "             'great': 71,\n",
       "             'america': 19,\n",
       "             'pkwy': 2,\n",
       "             'bayarea': 1,\n",
       "             'personalinjury': 1,\n",
       "             'read': 68,\n",
       "             'advice': 2,\n",
       "             'solicitor': 2,\n",
       "             'help': 87,\n",
       "             'otleyhour': 1,\n",
       "             'stlouis': 1,\n",
       "             'caraccidentlawyer': 1,\n",
       "             'speed': 4,\n",
       "             'teen': 15,\n",
       "             'teeu': 1,\n",
       "             'report': 77,\n",
       "             'motor': 6,\n",
       "             'vehicle': 24,\n",
       "             'curry': 1,\n",
       "             'herman': 1,\n",
       "             'rd': 20,\n",
       "             'stephenson': 1,\n",
       "             'involve': 18,\n",
       "             'overturn': 3,\n",
       "             'use': 37,\n",
       "             'bigrigradio': 1,\n",
       "             'awareness': 2,\n",
       "             'i77': 5,\n",
       "             'mile': 14,\n",
       "             'marker': 3,\n",
       "             'mooresville': 2,\n",
       "             'iredell': 2,\n",
       "             'ramp': 1,\n",
       "             'pm': 68,\n",
       "             'sleep': 18,\n",
       "             'pill': 3,\n",
       "             'double': 13,\n",
       "             'risk': 18,\n",
       "             'gon': 4,\n",
       "             'n': 24,\n",
       "             'cabrillo': 1,\n",
       "             'hwymagellan': 1,\n",
       "             'av': 5,\n",
       "             'mir': 1,\n",
       "             'congestion': 1,\n",
       "             'pastor': 1,\n",
       "             'scene': 17,\n",
       "             'accidentwho': 1,\n",
       "             'owner': 15,\n",
       "             'range': 4,\n",
       "             'rover': 2,\n",
       "             'mom': 23,\n",
       "             'wish': 14,\n",
       "             'spill': 7,\n",
       "             'mayonnaise': 1,\n",
       "             'horrible': 35,\n",
       "             'past': 35,\n",
       "             'sunday': 10,\n",
       "             'finally': 23,\n",
       "             'able': 10,\n",
       "             'god': 62,\n",
       "             'pissed': 1,\n",
       "             'donnie': 1,\n",
       "             'tell': 51,\n",
       "             'truckcrash': 1,\n",
       "             'fortworth': 1,\n",
       "             'interstate': 2,\n",
       "             'click': 7,\n",
       "             'ashville': 1,\n",
       "             'sb': 3,\n",
       "             'sr': 2,\n",
       "             'carolina': 1,\n",
       "             'motorcyclist': 9,\n",
       "             'i540': 1,\n",
       "             'cross': 17,\n",
       "             'median': 1,\n",
       "             'motorcycle': 6,\n",
       "             'rider': 7,\n",
       "             'travel': 18,\n",
       "             'fyi': 3,\n",
       "             'cadfyi': 2,\n",
       "             'property': 18,\n",
       "             'damagenhs999': 1,\n",
       "             'piner': 2,\n",
       "             'rdhorndale': 2,\n",
       "             'dr': 12,\n",
       "             'naayf': 1,\n",
       "             'turn': 40,\n",
       "             'chandanee': 1,\n",
       "             'magu': 1,\n",
       "             'mma': 6,\n",
       "             'taxi': 2,\n",
       "             'ram': 3,\n",
       "             'halfway': 1,\n",
       "             'confu': 1,\n",
       "             'manchester': 9,\n",
       "             'eddy': 1,\n",
       "             'stop': 66,\n",
       "             'nh3a': 1,\n",
       "             'delay': 11,\n",
       "             'min': 7,\n",
       "             'damagewpd1600': 1,\n",
       "             's': 55,\n",
       "             '17th': 1,\n",
       "             'injury': 81,\n",
       "             'willis': 1,\n",
       "             'foreman': 1,\n",
       "             'aashiqui': 1,\n",
       "             'actress': 2,\n",
       "             'anu': 2,\n",
       "             'aggarwal': 1,\n",
       "             'nearfatal': 1,\n",
       "             'suffield': 1,\n",
       "             'alberta': 4,\n",
       "             'backup': 3,\n",
       "             'southaccident': 1,\n",
       "             'right': 73,\n",
       "             'exit': 9,\n",
       "             'langtree': 1,\n",
       "             'rdconsider': 1,\n",
       "             'nc': 10,\n",
       "             'alternate': 1,\n",
       "             'change': 52,\n",
       "             'determine': 2,\n",
       "             'option': 9,\n",
       "             'financially': 1,\n",
       "             'support': 28,\n",
       "             'plan': 84,\n",
       "             'ongoing': 1,\n",
       "             'treatment': 7,\n",
       "             'deadly': 11,\n",
       "             'hagerstown': 1,\n",
       "             'today': 92,\n",
       "             'detail': 6,\n",
       "             'whag': 1,\n",
       "             'marinade': 1,\n",
       "             'fucking': 39,\n",
       "             'mfs': 3,\n",
       "             'fuck': 51,\n",
       "             'drive': 44,\n",
       "             'bahrain': 1,\n",
       "             'previously': 4,\n",
       "             'road': 46,\n",
       "             'kill': 173,\n",
       "             'explosion': 42,\n",
       "             'hear': 65,\n",
       "             'leader': 12,\n",
       "             'kenya': 4,\n",
       "             'forward': 4,\n",
       "             'comment': 19,\n",
       "             'issue': 59,\n",
       "             'disciplinary': 1,\n",
       "             'measuresarrestpastornganga': 1,\n",
       "             'scuf': 2,\n",
       "             'ps': 5,\n",
       "             'game': 49,\n",
       "             'cya': 1,\n",
       "             'effort': 11,\n",
       "             'painful': 1,\n",
       "             'win': 37,\n",
       "             'roger': 2,\n",
       "             'bannister': 1,\n",
       "             'ir': 7,\n",
       "             'icemoon': 7,\n",
       "             'aftershock': 20,\n",
       "             '   ': 12,\n",
       "             'dubstep': 8,\n",
       "             'trapmusic': 7,\n",
       "             'dnb': 8,\n",
       "             'dance': 17,\n",
       "             'icesu': 7,\n",
       "             'victory': 5,\n",
       "             'bargain': 5,\n",
       "             'basement': 3,\n",
       "             'price': 12,\n",
       "             'dwight': 1,\n",
       "             'david': 8,\n",
       "             'eisenhower': 1,\n",
       "             'remember': 40,\n",
       "             'charles': 4,\n",
       "             'schulz': 1,\n",
       "             'speak': 8,\n",
       "             'xb1': 2,\n",
       "             'conflict': 6,\n",
       "             'glorious': 3,\n",
       "             'triumph': 2,\n",
       "             'thomas': 7,\n",
       "             'paine': 1,\n",
       "             'growingupspoiled': 1,\n",
       "             'clay': 1,\n",
       "             'pigeon': 1,\n",
       "             'shoot': 39,\n",
       "             'guess': 16,\n",
       "             'actually': 28,\n",
       "             'free': 43,\n",
       "             'tc': 2,\n",
       "             'terrifying': 3,\n",
       "             'good': 157,\n",
       "             'roller': 3,\n",
       "             'coaster': 2,\n",
       "             'disclaimer': 1,\n",
       "             'see': 80,\n",
       "             'wisdomwed': 1,\n",
       "             'bonus': 1,\n",
       "             'minute': 41,\n",
       "             'daily': 21,\n",
       "             'habit': 3,\n",
       "             'improve': 3,\n",
       "             'lifehack': 1,\n",
       "             'protect': 10,\n",
       "             'profit': 4,\n",
       "             'global': 18,\n",
       "             'financial': 13,\n",
       "             'meltdown': 33,\n",
       "             'wiedemer': 1,\n",
       "             'http': 1,\n",
       "             'moment': 24,\n",
       "             'scary': 9,\n",
       "             'guy': 47,\n",
       "             'scream': 113,\n",
       "             'bloody': 44,\n",
       "             'murder': 17,\n",
       "             'silverwood': 1,\n",
       "             'ac': 6,\n",
       "             'fullac': 1,\n",
       "             'streaming': 2,\n",
       "             'youtube': 15,\n",
       "             'book': 32,\n",
       "             'face': 54,\n",
       "             'difficulty': 2,\n",
       "             'wrong': 21,\n",
       "             'joel': 3,\n",
       "             'osteen': 1,\n",
       "             'thing': 72,\n",
       "             'stand': 29,\n",
       "             'dream': 14,\n",
       "             'belief': 4,\n",
       "             'possible': 32,\n",
       "             'brown': 27,\n",
       "             'praise': 1,\n",
       "             'ministry': 1,\n",
       "             'wdyouth': 1,\n",
       "             'biblestudy': 1,\n",
       "             'way': 85,\n",
       "             'avoid': 21,\n",
       "             'trap': 46,\n",
       "             'think': 128,\n",
       "             'job': 45,\n",
       "             'orange': 4,\n",
       "             'bb': 1,\n",
       "             'kick': 16,\n",
       "             'make': 50,\n",
       "             'interrupt': 1,\n",
       "             'george': 6,\n",
       "             'bernard': 1,\n",
       "             'shaw': 2,\n",
       "             'oyster': 1,\n",
       "             'shell': 5,\n",
       "             'andrew': 2,\n",
       "             'carnegie': 1,\n",
       "             'need': 101,\n",
       "             'pyou': 1,\n",
       "             'play': 60,\n",
       "             'hybrid': 3,\n",
       "             'slayer': 3,\n",
       "             'ps4': 2,\n",
       "             'eu': 4,\n",
       "             'hmu': 2,\n",
       "             'expert': 22,\n",
       "             'france': 16,\n",
       "             'begin': 29,\n",
       "             'examine': 10,\n",
       "             'airplane': 36,\n",
       "             'debris': 50,\n",
       "             'find': 84,\n",
       "             'reunion': 33,\n",
       "             'island': 38,\n",
       "             'french': 10,\n",
       "             'air': 42,\n",
       "             'o': 10,\n",
       "             'news': 192,\n",
       "             'strict': 2,\n",
       "             'liability': 2,\n",
       "             'context': 3,\n",
       "             'pilot': 14,\n",
       "             'error': 5,\n",
       "             'common': 11,\n",
       "             'component': 2,\n",
       "             'aviation': 2,\n",
       "             'cr': 4,\n",
       "             'lifetime': 3,\n",
       "             'odd': 6,\n",
       "             'wedn': 2,\n",
       "             'awwww': 2,\n",
       "             'cutie': 1,\n",
       "             'family': 131,\n",
       "             'member': 19,\n",
       "             'osama': 1,\n",
       "             'bin': 10,\n",
       "             'laden': 9,\n",
       "             'ironic': 2,\n",
       "             'mhmmm': 1,\n",
       "             'gov': 9,\n",
       "             'suspect': 49,\n",
       "             'engine': 5,\n",
       "             'wing': 8,\n",
       "             'cessna': 1,\n",
       "             'ocampo': 1,\n",
       "             'coahuila': 2,\n",
       "             'mexico': 7,\n",
       "             'july': 7,\n",
       "             'include': 15,\n",
       "             'state': 52,\n",
       "             'government': 33,\n",
       "             'official': 51,\n",
       "             'watchthevideo': 1,\n",
       "             'wednesdayu': 1,\n",
       "             'wednesday': 14,\n",
       "             't': 24,\n",
       "             'kca': 4,\n",
       "             'votejkt48id': 4,\n",
       "             'mbataweel': 1,\n",
       "             'rip': 14,\n",
       "             'binladen': 1,\n",
       "             'coworker': 3,\n",
       "             'nude': 4,\n",
       "             'mode': 16,\n",
       "             'wreck': 99,\n",
       "             'politic': 3,\n",
       "             'mlb': 4,\n",
       "             'unbelievably': 1,\n",
       "             'insane': 6,\n",
       "             'airport': 31,\n",
       "             'aircraft': 25,\n",
       "             'aeroplane': 1,\n",
       "             'runway': 7,\n",
       "             'freakyu': 1,\n",
       "             'airplaneae29072015': 1,\n",
       "             'usama': 1,\n",
       "             'ladins': 1,\n",
       "             'naturally': 1,\n",
       "             'plane': 35,\n",
       "             'festival': 6,\n",
       "             'death': 111,\n",
       "             'carfest': 1,\n",
       "             'dtn': 4,\n",
       "             'brazil': 7,\n",
       "             'exp': 6,\n",
       "             'uiairplaneu': 1,\n",
       "             'wtf': 12,\n",
       "             'canuat': 6,\n",
       "             'believe': 31,\n",
       "             'eye': 41,\n",
       "             'nicole': 1,\n",
       "             'fletcher': 1,\n",
       "             'victim': 27,\n",
       "             'crashed': 1,\n",
       "             'ago': 26,\n",
       "             'little': 48,\n",
       "             'bit': 14,\n",
       "             'trauma': 39,\n",
       "             'omg': 20,\n",
       "             'bro': 9,\n",
       "             'jetengine': 1,\n",
       "             'turbojet': 1,\n",
       "             'boing': 1,\n",
       "             'g90': 1,\n",
       "             'phone': 41,\n",
       "             'look_like': 55,\n",
       "             'ship': 37,\n",
       "             'terrible': 7,\n",
       "             'statistically': 1,\n",
       "             'cop': 19,\n",
       "             'house': 65,\n",
       "             'colombia': 1,\n",
       "             'shooting': 15,\n",
       "             'drone': 11,\n",
       "             'worry': 19,\n",
       "             'esp': 3,\n",
       "             'vicinity': 2,\n",
       "             'early': 17,\n",
       "             'wake': 40,\n",
       "             'sister': 11,\n",
       "             'beg': 2,\n",
       "             'ride': 6,\n",
       "             'wher': 1,\n",
       "             'ambulance': 41,\n",
       "             'hospital': 12,\n",
       "             'rodkiai': 1,\n",
       "             'fear': 90,\n",
       "             'pakistani': 16,\n",
       "             'helicopter': 27,\n",
       "             'lorry': 6,\n",
       "             'emsneu': 1,\n",
       "             'reuters': 20,\n",
       "             'yugvani': 3,\n",
       "             'lead': 44,\n",
       "             'service': 67,\n",
       "             'boss': 5,\n",
       "             'welcome': 10,\n",
       "             'charity': 8,\n",
       "             'aberystwythshrewsbury': 1,\n",
       "             'incident': 11,\n",
       "             'halt': 2,\n",
       "             'shrew': 1,\n",
       "             'sprinter': 4,\n",
       "             'automatic': 7,\n",
       "             'frontline': 6,\n",
       "             'choice': 10,\n",
       "             'lez': 4,\n",
       "             'compliant': 4,\n",
       "             'ebay': 30,\n",
       "             'nanotech': 1,\n",
       "             'device': 3,\n",
       "             'target': 18,\n",
       "             'destroy': 95,\n",
       "             'blood': 42,\n",
       "             'clot': 1,\n",
       "             'hella': 2,\n",
       "             'crazy': 22,\n",
       "             'fight': 42,\n",
       "             'couple': 23,\n",
       "             'mosh': 1,\n",
       "             'pit': 4,\n",
       "             'run': 76,\n",
       "             'lucky': 11,\n",
       "             'justsaying': 1,\n",
       "             'randomthought': 1,\n",
       "             'tilnow': 3,\n",
       "             'dna': 1,\n",
       "             'ok': 36,\n",
       "             'hahahah': 2,\n",
       "             'pakistan': 21,\n",
       "             'nissan': 1,\n",
       "             'medical': 6,\n",
       "             'assistance': 4,\n",
       "             'ems1': 1,\n",
       "             'ny': 8,\n",
       "             'emts': 1,\n",
       "             'petition': 12,\n",
       "             'hour': 40,\n",
       "             'youminimum': 1,\n",
       "             'wageua': 1,\n",
       "             'ems': 6,\n",
       "             'paramedic': 4,\n",
       "             'parking': 4,\n",
       "             'lot': 43,\n",
       "             'say': 114,\n",
       "             'johns': 2,\n",
       "             'ui': 15,\n",
       "             'dog': 28,\n",
       "             'hatzolah': 1,\n",
       "             'respond': 17,\n",
       "             'dual': 3,\n",
       "             'siren': 51,\n",
       "             'andu': 2,\n",
       "             'worldnew': 6,\n",
       "             'number': 23,\n",
       "             'lesotho': 1,\n",
       "             'body': 58,\n",
       "             'surprised': 5,\n",
       "             'standardise': 1,\n",
       "             'clinical': 1,\n",
       "             'practice': 10,\n",
       "             'nhs': 2,\n",
       "             'trust': 21,\n",
       "             'jwalk': 1,\n",
       "             'pass': 23,\n",
       "             'hate': 28,\n",
       "             'episode': 13,\n",
       "             'trunk': 3,\n",
       "             'annihilate': 33,\n",
       "             'freiza': 1,\n",
       "             'clean': 7,\n",
       "             'show': 21,\n",
       "             'nigga': 13,\n",
       "             'mercy': 4,\n",
       "             'shall': 16,\n",
       "             'petebest': 1,\n",
       "             'dessicate': 1,\n",
       "             'lay': 4,\n",
       "             'bare': 4,\n",
       "             'kneel': 1,\n",
       "             'uribe': 4,\n",
       "             'baseball': 7,\n",
       "             'met': 2,\n",
       "             'heysundowns': 1,\n",
       "             'previous': 4,\n",
       "             'meeting': 7,\n",
       "             'celticindeed': 1,\n",
       "             'improvement': 2,\n",
       "             'mizzou': 1,\n",
       "             'muschamps': 1,\n",
       "             'career': 15,\n",
       "             'compete': 2,\n",
       "             'bama': 1,\n",
       "             'abs': 1,\n",
       "             'status': 5,\n",
       "             'education': 5,\n",
       "             'mba': 1,\n",
       "             'behalf': 3,\n",
       "             'easy': 9,\n",
       "             'careen': 1,\n",
       "             'eovm': 1,\n",
       "             'luka': 2,\n",
       "             'alois': 2,\n",
       "             'trancy': 2,\n",
       "             'un': 7,\n",
       "             'fella': 2,\n",
       "             'sorry': 20,\n",
       "             'pull': 16,\n",
       "             'drunk': 11,\n",
       "             'driver': 19,\n",
       "             'safety': 15,\n",
       "             'hit': 64,\n",
       "             'boom': 3,\n",
       "             'country': 27,\n",
       "             'entirely': 1,\n",
       "             'hu': 2,\n",
       "             'britain': 2,\n",
       "             'suregod': 1,\n",
       "             'promise': 11,\n",
       "             'israel': 17,\n",
       "             'butthe': 1,\n",
       "             'horror': 28,\n",
       "             'iran': 34,\n",
       "             'wnuke': 1,\n",
       "             'armenians': 1,\n",
       "             'spend': 17,\n",
       "             'history': 25,\n",
       "             'instantly': 2,\n",
       "             'aware': 5,\n",
       "             'ability': 4,\n",
       "             'humanity': 9,\n",
       "             'tryout': 4,\n",
       "             'minus': 1,\n",
       "             'fact': 15,\n",
       "             'quickly': 9,\n",
       "             'short': 13,\n",
       "             'ball': 23,\n",
       "             'toenail': 1,\n",
       "             '1960': 1,\n",
       "             'oryx': 1,\n",
       "             'symbol': 1,\n",
       "             'arabian': 4,\n",
       "             'peninsula': 1,\n",
       "             'hunter': 7,\n",
       "             'ready': 18,\n",
       "             'bucs': 1,\n",
       "             'weather': 51,\n",
       "             'philip': 1,\n",
       "             'forecast': 9,\n",
       "             'domain': 2,\n",
       "             'sophistication': 1,\n",
       "             'closely': 1,\n",
       "             'uptotheminute': 1,\n",
       "             'feat': 12,\n",
       "             'zrnf': 1,\n",
       "             'judas': 1,\n",
       "             'priest': 2,\n",
       "             'rob': 5,\n",
       "             'scorpion': 2,\n",
       "             'astonish': 1,\n",
       "             'gig': 2,\n",
       "             'officially': 6,\n",
       "             'skip': 3,\n",
       "             'fantasticfourfant4sticwhatever': 1,\n",
       "             'hashtag': 4,\n",
       "             'review': 13,\n",
       "             'bummer': 2,\n",
       "             'explain': 7,\n",
       "             'case': 32,\n",
       "             'survivor': 41,\n",
       "             'evolve': 3,\n",
       "             'godlike': 1,\n",
       "             'completely': 9,\n",
       "             'cech': 1,\n",
       "             'paul': 10,\n",
       "             'keegan': 1,\n",
       "             'legion': 3,\n",
       "             'imperfect': 2,\n",
       "             'project': 33,\n",
       "             'form': 7,\n",
       "             'cell': 4,\n",
       "             'exactly': 7,\n",
       "             'lesnarcena': 1,\n",
       "             'match': 16,\n",
       "             'summerslam': 1,\n",
       "             'brock': 2,\n",
       "             'damascus': 1,\n",
       "             'syrian': 11,\n",
       "             'army': 71,\n",
       "             'grind': 1,\n",
       "             'youalloosh': 1,\n",
       "             'gang': 6,\n",
       "             'manure': 1,\n",
       "             'pile': 17,\n",
       "             'forthright': 1,\n",
       "             'food': 47,\n",
       "             'coma': 1,\n",
       "             'bc': 27,\n",
       "             'kebabtahinipickle': 1,\n",
       "             'wfrie': 1,\n",
       "             'fun': 29,\n",
       "             'happyhour': 1,\n",
       "             'simmons': 1,\n",
       "             'bar': 28,\n",
       "             'camden': 1,\n",
       "             'handsome': 1,\n",
       "             'apart': 3,\n",
       "             'juanny': 1,\n",
       "             'beisbol': 1,\n",
       "             'lgm': 1,\n",
       "             'hell': 24,\n",
       "             'fraction': 3,\n",
       "             'total': 21,\n",
       "             'annihilation': 21,\n",
       "             'destruction': 38,\n",
       "             'usa': 20,\n",
       "             'maybe': 25,\n",
       "             'pres': 5,\n",
       "             'sell': 6,\n",
       "             'river': 32,\n",
       "             'evildead': 1,\n",
       "             'civilization': 2,\n",
       "             'national': 36,\n",
       "             'park': 19,\n",
       "             'services': 15,\n",
       "             'tonto': 8,\n",
       "             'salt': 14,\n",
       "             'wild': 46,\n",
       "             'horse': 18,\n",
       "             'quarterstaff': 1,\n",
       "             'world': 115,\n",
       "             'vs': 18,\n",
       "             'self': 21,\n",
       "             'transformation': 3,\n",
       "             'alien': 4,\n",
       "             'attack': 152,\n",
       "             'exterminate': 2,\n",
       "             'human': 22,\n",
       "             'starmade': 1,\n",
       "             'stardate': 1,\n",
       "             'planetary': 1,\n",
       "             'sign': 52,\n",
       "             'share': 29,\n",
       "             'save': 74,\n",
       "             'arizona': 3,\n",
       "             'signatureschange': 1,\n",
       "             'org': 3,\n",
       "             'thx': 8,\n",
       "             'soul': 19,\n",
       "             'punish': 3,\n",
       "             'withaeannihilation': 1,\n",
       "             'mention': 17,\n",
       "             'major': 22,\n",
       "             'contributor': 1,\n",
       "             'dieplease': 2,\n",
       "             'petitiontake': 2,\n",
       "             'voice': 9,\n",
       "             'reject': 6,\n",
       "             'law': 12,\n",
       "             'misguided': 1,\n",
       "             'false': 11,\n",
       "             'prophet': 7,\n",
       "             'imprison': 2,\n",
       "             'nation': 6,\n",
       "             'fuel': 12,\n",
       "             'jeb': 10,\n",
       "             'christie': 3,\n",
       "             'kasich': 1,\n",
       "             'away': 38,\n",
       "             'allow': 24,\n",
       "             'helphorses': 1,\n",
       "             'hey': 24,\n",
       "             'az': 1,\n",
       "             'wildhorses': 2,\n",
       "             'tantonationalforest': 1,\n",
       "             'singalong': 1,\n",
       "             'current': 4,\n",
       "             'nova': 1,\n",
       "             'bookslast': 1,\n",
       "             'checkedhe': 1,\n",
       "             'tie': 6,\n",
       "             'saltriverwildhorse': 1,\n",
       "             'dante': 2,\n",
       "             'join': 17,\n",
       "             'follow': 40,\n",
       "             'zone': 32,\n",
       "             'johnny': 2,\n",
       "             ...})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not', 'fire', 'like', 'no', 'new', 'go', 'get', 'people', 'news', 'video']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(window=2,\n",
    "                     alpha=0.01,\n",
    "                     workers=cores-1,\n",
    "                     iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:49:01: collecting all words and their counts\n",
      "INFO - 18:49:01: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:49:01: collected 12891 word types from a corpus of 61218 raw words and 7613 sentences\n",
      "INFO - 18:49:01: Loading a fresh vocabulary\n",
      "INFO - 18:49:01: effective_min_count=5 retains 2251 unique words (17% of original 12891, drops 10640)\n",
      "INFO - 18:49:01: effective_min_count=5 leaves 45879 word corpus (74% of original 61218, drops 15339)\n",
      "INFO - 18:49:01: deleting the raw counts dictionary of 12891 items\n",
      "INFO - 18:49:01: sample=0.001 downsamples 31 most-common words\n",
      "INFO - 18:49:01: downsampling leaves estimated 44111 word corpus (96.1% of prior 45879)\n",
      "INFO - 18:49:01: estimated required memory for 2251 words and 100 dimensions: 2926300 bytes\n",
      "INFO - 18:49:01: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:49:15: training model with 7 workers on 2251 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "INFO - 18:49:15: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:15: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:15: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:15: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:15: EPOCH - 1 : training on 61218 raw words (44132 effective words) took 0.5s, 92940 effective words/s\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:16: EPOCH - 2 : training on 61218 raw words (44044 effective words) took 0.2s, 219308 effective words/s\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:16: EPOCH - 3 : training on 61218 raw words (44125 effective words) took 0.2s, 227022 effective words/s\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:16: EPOCH - 4 : training on 61218 raw words (44101 effective words) took 0.2s, 253299 effective words/s\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:16: EPOCH - 5 : training on 61218 raw words (44090 effective words) took 0.2s, 201896 effective words/s\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:17: EPOCH - 6 : training on 61218 raw words (44109 effective words) took 0.2s, 219977 effective words/s\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:17: EPOCH - 7 : training on 61218 raw words (44166 effective words) took 0.2s, 192083 effective words/s\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:17: EPOCH - 8 : training on 61218 raw words (44097 effective words) took 0.2s, 203409 effective words/s\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:17: EPOCH - 9 : training on 61218 raw words (44099 effective words) took 0.2s, 197944 effective words/s\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:17: EPOCH - 10 : training on 61218 raw words (44123 effective words) took 0.3s, 170340 effective words/s\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:18: EPOCH - 11 : training on 61218 raw words (44086 effective words) took 0.2s, 177789 effective words/s\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:18: EPOCH - 12 : training on 61218 raw words (44096 effective words) took 0.2s, 205893 effective words/s\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:49:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:18: EPOCH - 13 : training on 61218 raw words (44183 effective words) took 0.2s, 206382 effective words/s\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:18: EPOCH - 14 : training on 61218 raw words (44069 effective words) took 0.2s, 201408 effective words/s\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:19: EPOCH - 15 : training on 61218 raw words (44093 effective words) took 0.2s, 194918 effective words/s\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:19: EPOCH - 16 : training on 61218 raw words (44110 effective words) took 0.2s, 216545 effective words/s\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:19: EPOCH - 17 : training on 61218 raw words (44096 effective words) took 0.2s, 236498 effective words/s\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:19: EPOCH - 18 : training on 61218 raw words (44048 effective words) took 0.2s, 181883 effective words/s\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:20: EPOCH - 19 : training on 61218 raw words (44082 effective words) took 0.2s, 188475 effective words/s\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:20: EPOCH - 20 : training on 61218 raw words (44095 effective words) took 0.3s, 172992 effective words/s\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:20: EPOCH - 21 : training on 61218 raw words (44149 effective words) took 0.2s, 218138 effective words/s\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:20: EPOCH - 22 : training on 61218 raw words (44097 effective words) took 0.3s, 157571 effective words/s\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:21: EPOCH - 23 : training on 61218 raw words (44086 effective words) took 0.2s, 176485 effective words/s\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:21: EPOCH - 24 : training on 61218 raw words (44097 effective words) took 0.2s, 177116 effective words/s\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:21: EPOCH - 25 : training on 61218 raw words (44162 effective words) took 0.3s, 164306 effective words/s\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:49:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:21: EPOCH - 26 : training on 61218 raw words (44156 effective words) took 0.2s, 188577 effective words/s\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:22: EPOCH - 27 : training on 61218 raw words (44083 effective words) took 0.2s, 192395 effective words/s\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:22: EPOCH - 28 : training on 61218 raw words (44163 effective words) took 0.3s, 173447 effective words/s\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:22: EPOCH - 29 : training on 61218 raw words (44128 effective words) took 0.2s, 186492 effective words/s\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:22: EPOCH - 30 : training on 61218 raw words (44089 effective words) took 0.3s, 173957 effective words/s\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:23: EPOCH - 31 : training on 61218 raw words (44097 effective words) took 0.2s, 212710 effective words/s\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:23: EPOCH - 32 : training on 61218 raw words (44125 effective words) took 0.3s, 165319 effective words/s\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:23: EPOCH - 33 : training on 61218 raw words (44117 effective words) took 0.3s, 169677 effective words/s\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:23: EPOCH - 34 : training on 61218 raw words (44070 effective words) took 0.2s, 205562 effective words/s\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:24: EPOCH - 35 : training on 61218 raw words (44103 effective words) took 0.3s, 155570 effective words/s\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:24: EPOCH - 36 : training on 61218 raw words (44039 effective words) took 0.3s, 162279 effective words/s\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:24: EPOCH - 37 : training on 61218 raw words (44121 effective words) took 0.2s, 200420 effective words/s\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:24: EPOCH - 38 : training on 61218 raw words (44083 effective words) took 0.2s, 190192 effective words/s\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:49:25: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:25: EPOCH - 39 : training on 61218 raw words (44081 effective words) took 0.2s, 178670 effective words/s\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:25: EPOCH - 40 : training on 61218 raw words (44146 effective words) took 0.2s, 203124 effective words/s\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:25: EPOCH - 41 : training on 61218 raw words (44113 effective words) took 0.2s, 182654 effective words/s\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:25: EPOCH - 42 : training on 61218 raw words (44095 effective words) took 0.2s, 179844 effective words/s\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:26: EPOCH - 43 : training on 61218 raw words (44092 effective words) took 0.2s, 179508 effective words/s\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:26: EPOCH - 44 : training on 61218 raw words (44142 effective words) took 0.2s, 178428 effective words/s\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:26: EPOCH - 45 : training on 61218 raw words (44144 effective words) took 0.2s, 181415 effective words/s\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:26: EPOCH - 46 : training on 61218 raw words (44076 effective words) took 0.3s, 164772 effective words/s\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:27: EPOCH - 47 : training on 61218 raw words (44151 effective words) took 0.3s, 170160 effective words/s\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:27: EPOCH - 48 : training on 61218 raw words (44134 effective words) took 0.2s, 180666 effective words/s\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:27: EPOCH - 49 : training on 61218 raw words (44160 effective words) took 0.2s, 187652 effective words/s\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 18:49:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 18:49:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 18:49:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 18:49:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 18:49:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:49:28: EPOCH - 50 : training on 61218 raw words (44093 effective words) took 0.2s, 186770 effective words/s\n",
      "INFO - 18:49:28: training on a 3060900 raw words (2205436 effective words) took 12.7s, 173871 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.21 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=50, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:21:56: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "word_vectors.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:51:19: saving Word2VecKeyedVectors object under word2vec.model, separately None\n",
      "INFO - 18:51:19: storing np array 'vectors' to word2vec.model.vectors.npy\n",
      "INFO - 18:51:20: not storing attribute vectors_norm\n",
      "INFO - 18:51:25: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('not', 0.9988853931427002),\n",
       " ('people', 0.998866081237793),\n",
       " ('news', 0.9987478256225586),\n",
       " ('death', 0.9985353350639343),\n",
       " ('come', 0.9985029697418213),\n",
       " ('flood', 0.9984305500984192),\n",
       " ('new', 0.9984046816825867),\n",
       " ('like', 0.9983848333358765),\n",
       " ('man', 0.9983129501342773),\n",
       " ('emergency', 0.9983068704605103)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.wv.most_similar(positive=[\"fire\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
